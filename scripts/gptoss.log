INFO 12-15 08:07:50 [__init__.py:239] Automatically detected platform rocm.
INFO 12-15 08:07:52 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-15 08:07:52 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-15 08:07:52 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-15 08:07:52 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-15 08:07:52 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-15 08:07:52 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-15 08:07:52 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-15 08:07:52 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-15 08:07:52 [__init__.py:44] plugin register_quantization_config loaded.
INFO 12-15 08:07:54 [api_server.py:981] vLLM API server version 0.8.2
INFO 12-15 08:07:54 [api_server.py:982] args: Namespace(subparser='serve', model_tag='/home/tester/data/openai/gpt-oss-20b', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/tester/data/openai/gpt-oss-20b', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=128, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization='moreh-mxfp4', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f24d8c88040>)
Deleted old log file: /home/tester/phucnguyen/hygon-test/dev/vllm_plugin_vllmm_0.8.2/logs/log.log
INFO 12-15 08:08:11 [config.py:590] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.
WARNING 12-15 08:08:11 [arg_utils.py:1860] Detected VLLM_USE_V1=1 with cuda. Usage should be considered experimental. Please report any issues on Github.
INFO 12-15 08:08:11 [config.py:1557] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.
INFO 12-15 08:08:11 [config.py:1702] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 12-15 08:08:11 [rocm.py:164] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-15 08:08:19 [__init__.py:239] Automatically detected platform rocm.
INFO 12-15 08:08:23 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/tester/data/openai/gpt-oss-20b', speculative_config=None, tokenizer='/home/tester/data/openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=moreh-mxfp4, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/tester/data/openai/gpt-oss-20b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
INFO 12-15 08:08:23 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-15 08:08:23 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-15 08:08:23 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-15 08:08:23 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-15 08:08:23 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-15 08:08:23 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-15 08:08:23 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-15 08:08:23 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-15 08:08:23 [__init__.py:44] plugin register_quantization_config loaded.
WARNING 12-15 08:08:23 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd2241ebf40>
INFO 12-15 08:08:23 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
current_platform.get_attn_backend_cls: selected_backend = None, head_size = 64, dtype = torch.bfloat16, kv_cache_dtype = torch.bfloat16
INFO 12-15 08:08:23 [rocm.py:126] Using Triton Attention backend on V1 engine.
INFO 12-15 08:08:24 [gpu_model_runner.py:1191] Starting to load model /home/tester/data/openai/gpt-oss-20b...
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
==================================================
enabled = True
==================================================
===> self.forward_hip
current_platform.get_attn_backend_cls: selected_backend = None, head_size = 64, dtype = torch.bfloat16, kv_cache_dtype = 'auto'
INFO 12-15 08:08:24 [rocm.py:126] Using Triton Attention backend on V1 engine.
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
config.rope_scaling = {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[INFO] Using MorehFusedMoE: MorehMxfp4Config
==================================================
enabled = True
==================================================
===> self.forward_hip
Using Mxfp4MoEMethod with config: moe = MorehFusedMoEConfig(num_experts=32, experts_per_token=4, hidden_dim=3072, num_local_experts=32, moe_parallel_config=MorehFusedMoEParallelConfig(tp_size=1, dp_size=1, ep_size=1, tp_rank=0, dp_rank=0, ep_rank=0, use_ep=False, all2all_backend='vllm_parallel_config.all2all_backend'), in_dtype=torch.bfloat16, max_num_tokens=256, has_bias=True)
hidden_size = 3072, num_experts = 32, intermediate_size_per_partition = 2880, extra_weight_attrs = {'weight_loader': <bound method CustomFusedMoE.weight_loader of MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)>, 'intermediate_size_full': 2880, 'intermediate_size_per_partition_before_pad': 2880}
==================================================
enabled = True
==================================================
===> self.forward_hip
name = 'input_layernorm', self.hidden_size = 2880, self.variance_epsilon = 1e-05, self.variance_size_override=None
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
Using GptOssModel with model_config = GptOssConfig {
  "architectures": [
    "MorehGptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.48.0",
  "use_cache": true,
  "vocab_size": 201088
}

Using GptOssModel with cache_config = <vllm.config.CacheConfig object at 0x7fd4a103ed10>
Using GptOssModel with parallel_config = ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_rank=0, data_parallel_master_ip='127.0.0.1', data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=True, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, world_size_across_dp=1, rank=0)
Using GptOssModel with quant_config = <vllm_plugin.quantization.mxfp4.MorehMxfp4Config object at 0x7fd22da367d0>
Using MorehGptOssForCausalLM with VLLM config = model='/home/tester/data/openai/gpt-oss-20b', speculative_config=None, tokenizer='/home/tester/data/openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=moreh-mxfp4, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/tester/data/openai/gpt-oss-20b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
--------------------------------------------------Call get_ep_group--------------------------------------------------
----------------------------------------------------------------------------------------------------
-> quant_method = 'mxfp4'
----------------------------------------------------------------------------------------------------
name                                                         | weight.shape         | w.dtype    | w.dev    | param.shape          | p.dtype    | p.dev   
------------------------------------------------------------------------------------------------------------------------------------------------------
embedding.weight                                             | (201088, 2880)       | torch.bfloat16 | cpu      | (201088, 2880)       | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.6.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.6.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.6.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.6.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.7.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.7.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.7.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.7.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.7.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.7.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.7.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.7.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.8.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.8.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.8.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.8.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.8.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.8.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.9.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.9.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.9.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.9.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.9.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.9.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
norm.weight                                                  | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.18.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.18.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.18.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.18.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.18.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.18.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.19.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.19.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.19.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.19.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.19.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.19.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.2.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.2.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.2.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.2.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.2.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.2.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.20.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.20.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.20.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.20.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.20.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.20.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.21.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.21.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.21.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.21.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.21.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.21.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.22.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.22.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.22.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.22.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.22.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.22.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.23.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.23.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.23.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.23.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.23.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.23.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.3.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.3.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.3.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.3.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.3.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.3.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.4.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.4.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.4.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.4.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.4.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.4.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.5.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.5.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.5.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.5.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.5.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.5.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.6.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.6.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.6.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.0.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.0.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.0.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.0.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.0.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.0.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.1.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.1.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.1.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.1.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.1.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.1.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.10.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.10.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.10.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.10.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.10.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.10.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.11.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.11.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.11.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.11.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.11.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.11.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.12.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.12.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.12.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.12.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.12.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.12.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.13.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.13.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.13.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.13.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.13.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.13.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.14.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.14.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.14.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.14.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.14.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.14.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.15.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.15.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.15.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.15.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.15.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.15.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.16.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.16.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.16.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.16.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.16.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.16.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.17.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.17.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.17.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.17.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.17.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.17.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.18.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
INFO 12-15 08:08:38 [loader.py:447] Loading weights took 13.77 seconds
INFO 12-15 08:08:39 [gpu_model_runner.py:1203] Model loading took 14.1511 GB and 14.835849 seconds
--------------------------------------------------
forward_ctx = {'model.layers.0.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.0.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.1.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.1.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.2.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.2.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.3.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.3.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.4.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.4.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.5.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.5.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.6.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.6.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.7.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.7.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.8.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.8.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.9.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.9.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.10.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.10.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.11.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.11.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.12.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.12.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.13.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.13.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.14.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.14.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.15.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.15.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.16.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.16.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.17.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.17.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.18.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.18.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.19.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.19.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.20.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.20.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.21.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.21.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.22.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.22.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
), 'model.layers.23.attn.attn': Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl), 'model.layers.23.mlp.experts': MorehFusedMoE(
  global_num_experts=32, local_num_experts=32, top_k=4, intermediate_size_per_partition=2880, tp_size=1,
  ep_size=1, reduce_results=True, renormalize=True, use_grouped_topk=False, scoring_func='softmax', activation='swigluoai'
)}
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
attn_module = Attention(head_size=64, num_heads=64, num_kv_heads=8, scale=0.125, backend=TritonAttentionImpl)
INFO 12-15 08:08:42 [kv_cache_utils.py:566] GPU KV cache size: 146,816 tokens
INFO 12-15 08:08:42 [kv_cache_utils.py:569] Maximum concurrency for 131,072 tokens per request: 1.12x
INFO 12-15 08:08:44 [core.py:151] init engine (profile, create kv cache, warmup model) took 4.63 seconds
INFO 12-15 08:08:44 [api_server.py:1028] Starting vLLM API server on http://0.0.0.0:8000
INFO 12-15 08:08:44 [launcher.py:26] Available routes are:
INFO 12-15 08:08:44 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /docs, Methods: HEAD, GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /redoc, Methods: HEAD, GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /health, Methods: GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /load, Methods: GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /version, Methods: GET
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /pooling, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /score, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /rerank, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 12-15 08:08:44 [launcher.py:34] Route: /invocations, Methods: POST
INFO 12-15 08:08:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:34 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:44 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:09:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:34 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:44 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:10:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:34 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:44 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:11:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:34 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:44 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:12:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:34 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:44 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:13:54 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:14:04 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:14:14 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-15 08:14:24 [launcher.py:74] Shutting down FastAPI HTTP server.
INFO 12-15 08:14:24 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
