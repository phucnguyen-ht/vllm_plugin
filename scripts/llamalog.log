INFO 12-16 02:13:06 [__init__.py:239] Automatically detected platform rocm.
Deleted old log file: /home/tester/phucnguyen/hygon-test/dev/vllm_plugin_vllmm_0.8.2/logs/log.log
INFO 12-16 02:13:08 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-16 02:13:08 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-16 02:13:08 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-16 02:13:08 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-16 02:13:08 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-16 02:13:08 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-16 02:13:08 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-16 02:13:08 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-16 02:13:08 [__init__.py:44] plugin register_quantization_config loaded.
INFO 12-16 02:13:10 [api_server.py:981] vLLM API server version 0.8.2
INFO 12-16 02:13:10 [api_server.py:982] args: Namespace(subparser='serve', model_tag='/home/tester/data/meta-llama/Meta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/tester/data/meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=1024, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=128, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f6bc60851b0>)
INFO 12-16 02:13:27 [config.py:590] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.
WARNING 12-16 02:13:27 [arg_utils.py:1860] Detected VLLM_USE_V1=1 with cuda. Usage should be considered experimental. Please report any issues on Github.
INFO 12-16 02:13:27 [config.py:1557] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.
INFO 12-16 02:13:27 [config.py:1702] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 12-16 02:13:27 [rocm.py:164] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-16 02:13:34 [__init__.py:239] Automatically detected platform rocm.
INFO 12-16 02:13:36 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/tester/data/meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='/home/tester/data/meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/tester/data/meta-llama/Meta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
INFO 12-16 02:13:36 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-16 02:13:36 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-16 02:13:36 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-16 02:13:36 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-16 02:13:36 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-16 02:13:36 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-16 02:13:36 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-16 02:13:36 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-16 02:13:36 [__init__.py:44] plugin register_quantization_config loaded.
WARNING 12-16 02:13:38 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7e79450eb0>
INFO 12-16 02:13:38 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
current_platform.get_attn_backend_cls: selected_backend = None, head_size = 128, dtype = torch.bfloat16, kv_cache_dtype = torch.bfloat16
INFO 12-16 02:13:38 [rocm.py:126] Using Triton Attention backend on V1 engine.
INFO 12-16 02:13:39 [gpu_model_runner.py:1190] Starting to load model /home/tester/data/meta-llama/Meta-Llama-3-8B...
==================================================
enabled = True
==================================================
===> self.forward_hip
current_platform.get_attn_backend_cls: selected_backend = None, head_size = 128, dtype = torch.bfloat16, kv_cache_dtype = 'auto'
INFO 12-16 02:13:39 [rocm.py:126] Using Triton Attention backend on V1 engine.
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
==================================================
enabled = True
==================================================
===> self.forward_hip
INFO 12-16 02:14:40 [loader.py:447] Loading weights took 61.10 seconds
INFO 12-16 02:14:40 [gpu_model_runner.py:1202] Model loading took 14.9596 GB and 61.407939 seconds
--------------------------------------------------
forward_ctx = {'model.layers.0.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.1.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.2.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.3.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.4.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.5.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.6.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.7.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.8.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.9.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.10.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.11.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.12.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.13.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.14.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.15.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.16.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.17.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.18.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.19.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.20.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.21.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.22.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.23.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.24.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.25.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.26.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.27.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.28.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.29.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.30.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl), 'model.layers.31.self_attn.attn': Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)}
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
attn_module = Attention(head_size=128, num_heads=32, num_kv_heads=8, scale=0.08838834764831845, backend=TritonAttentionImpl)
INFO 12-16 02:15:29 [kv_cache_utils.py:566] GPU KV cache size: 69,632 tokens
INFO 12-16 02:15:29 [kv_cache_utils.py:569] Maximum concurrency for 1,024 tokens per request: 68.00x
INFO 12-16 02:15:30 [core.py:151] init engine (profile, create kv cache, warmup model) took 49.79 seconds
WARNING 12-16 02:15:30 [config.py:1033] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 12-16 02:15:30 [serving_chat.py:114] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 12-16 02:15:30 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 12-16 02:15:30 [api_server.py:1028] Starting vLLM API server on http://0.0.0.0:8000
INFO 12-16 02:15:30 [launcher.py:26] Available routes are:
INFO 12-16 02:15:30 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /docs, Methods: HEAD, GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /redoc, Methods: HEAD, GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /health, Methods: GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /load, Methods: GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /ping, Methods: POST, GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /version, Methods: GET
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /pooling, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /score, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /rerank, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 12-16 02:15:30 [launcher.py:34] Route: /invocations, Methods: POST
INFO 12-16 02:15:40 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:15:50 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:00 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:10 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:20 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:55534 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55540 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55552 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55568 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55580 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55592 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55596 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55604 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55616 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55624 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55628 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55638 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55650 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55652 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55662 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:55674 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO 12-16 02:16:30 [logger.py:39] Received request cmpl-01f2ebb46bad461cb7dec6c937e9751f-0: prompt: 'Who won the world series in 2020?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 15546, 2834, 279, 1917, 4101, 304, 220, 2366, 15, 30], lora_request: None, prompt_adapter_request: None.
INFO 12-16 02:16:30 [async_llm.py:221] Added request cmpl-01f2ebb46bad461cb7dec6c937e9751f-0.
INFO 12-16 02:16:30 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:40 [loggers.py:80] Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:43 [async_llm.py:347] Aborted request cmpl-01f2ebb46bad461cb7dec6c937e9751f-0.
INFO 12-16 02:16:50 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-16 02:16:52 [launcher.py:74] Shutting down FastAPI HTTP server.
