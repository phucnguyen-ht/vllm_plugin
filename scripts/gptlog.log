INFO 12-17 08:17:26 [__init__.py:239] Automatically detected platform rocm.
INFO 12-17 08:17:29 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-17 08:17:29 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-17 08:17:29 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-17 08:17:29 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-17 08:17:29 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-17 08:17:29 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-17 08:17:29 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-17 08:17:29 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-17 08:17:29 [__init__.py:44] plugin register_quantization_config loaded.
INFO 12-17 08:17:31 [api_server.py:981] vLLM API server version 0.8.2
INFO 12-17 08:17:31 [api_server.py:982] args: Namespace(subparser='serve', model_tag='/home/tester/data/openai/gpt-oss-20b', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/tester/data/openai/gpt-oss-20b', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=128, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization='moreh-mxfp4', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f8022b0d480>)
INFO 12-17 08:17:48 [config.py:584] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.
WARNING 12-17 08:17:48 [arg_utils.py:1860] Detected VLLM_USE_V1=1 with cuda. Usage should be considered experimental. Please report any issues on Github.
INFO 12-17 08:17:48 [config.py:1551] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.
INFO 12-17 08:17:48 [config.py:1696] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 12-17 08:17:48 [rocm.py:164] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 12-17 08:17:57 [__init__.py:239] Automatically detected platform rocm.
INFO 12-17 08:18:01 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='/home/tester/data/openai/gpt-oss-20b', speculative_config=None, tokenizer='/home/tester/data/openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=moreh-mxfp4, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/tester/data/openai/gpt-oss-20b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
INFO 12-17 08:18:01 [__init__.py:30] Available plugins for group vllm.general_plugins:
INFO 12-17 08:18:01 [__init__.py:32] name=register_custom_model, value=vllm_plugin.models:register
INFO 12-17 08:18:01 [__init__.py:32] name=register_post_parsed_function, value=vllm_plugin.moreh_config:register_post_parsed_function
INFO 12-17 08:18:01 [__init__.py:32] name=register_quantization_config, value=vllm_plugin.quantization:register
INFO 12-17 08:18:01 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.
INFO 12-17 08:18:01 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.
INFO 12-17 08:18:01 [__init__.py:44] plugin register_custom_model loaded.
INFO 12-17 08:18:01 [__init__.py:44] plugin register_post_parsed_function loaded.
INFO 12-17 08:18:01 [__init__.py:44] plugin register_quantization_config loaded.
WARNING 12-17 08:18:01 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f828476af20>
INFO 12-17 08:18:01 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 12-17 08:18:01 [rocm.py:126] Using Triton Attention backend on V1 engine.
INFO 12-17 08:18:02 [gpu_model_runner.py:1175] Starting to load model /home/tester/data/openai/gpt-oss-20b...
INFO 12-17 08:18:02 [rocm.py:126] Using Triton Attention backend on V1 engine.
Using MorehGptOssForCausalLM with VLLM config = model='/home/tester/data/openai/gpt-oss-20b', speculative_config=None, tokenizer='/home/tester/data/openai/gpt-oss-20b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=moreh-mxfp4, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/tester/data/openai/gpt-oss-20b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
--------------------------------------------------Call get_ep_group--------------------------------------------------
----------------------------------------------------------------------------------------------------
-> quant_method = 'mxfp4'
----------------------------------------------------------------------------------------------------
name                                                         | weight.shape         | w.dtype    | w.dev    | param.shape          | p.dtype    | p.dev   
------------------------------------------------------------------------------------------------------------------------------------------------------
embedding.weight                                             | (201088, 2880)       | torch.bfloat16 | cpu      | (201088, 2880)       | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.6.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.6.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.6.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.6.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.7.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.7.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.7.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.7.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.7.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.7.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.7.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.7.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.7.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.7.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.7.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.8.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.8.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.8.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.8.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.8.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.8.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.8.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.8.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.8.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.8.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.9.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.9.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.9.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.9.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.9.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.9.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.9.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.9.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.9.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.9.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
norm.weight                                                  | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.18.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.18.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.18.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.18.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.18.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.18.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.18.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.19.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.19.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.19.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.19.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.19.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.19.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.19.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.19.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.19.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.19.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.2.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.2.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.2.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.2.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.2.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.2.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.2.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.2.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.2.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.2.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.20.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.20.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.20.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.20.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.20.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.20.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.20.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.20.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.20.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.20.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.21.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.21.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.21.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.21.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.21.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.21.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.21.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.21.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.21.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.21.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.22.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.22.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.22.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.22.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.22.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.22.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.22.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.22.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.22.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.22.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.23.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.23.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.23.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.23.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.23.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.23.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.23.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.23.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.23.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.23.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.3.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.3.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.3.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.3.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.3.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.3.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.3.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.3.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.3.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.3.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.4.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.4.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.4.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.4.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.4.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.4.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.4.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.4.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.4.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.4.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.5.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.5.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.5.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.5.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.5.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.5.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.5.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.5.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.5.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.5.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.6.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.6.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.6.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.6.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.6.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.6.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.0.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.0.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.0.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.0.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.0.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.0.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.0.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.0.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.0.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.0.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.input_layernorm.weight                              | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w2_bias                                 | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w2_weight                               | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.1.mlp.experts.w2_weight_scale                         | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.1.mlp.experts.w13_bias                                | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.1.mlp.experts.w13_weight                              | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.1.mlp.experts.w13_weight_scale                        | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.1.mlp.router.bias                                     | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.1.mlp.router.weight                                   | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.1.post_attention_layernorm.weight                     | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.attn.o_proj.bias                                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.1.attn.o_proj.weight                                  | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.1.attn.sinks                                          | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.bias                                       | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.1.attn.qkv.weight                                     | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.10.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.10.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.10.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.10.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.10.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.10.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.10.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.10.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.10.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.10.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.11.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.11.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.11.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.11.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.11.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.11.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.11.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.11.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.11.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.11.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.12.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.12.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.12.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.12.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.12.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.12.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.12.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.12.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.12.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.12.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.13.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.13.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.13.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.13.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.13.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.13.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.13.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.13.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.13.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.13.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.14.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.14.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.14.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.14.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.14.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.14.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.14.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.14.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.14.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.14.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.15.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.15.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.15.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.15.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.15.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.15.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.15.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.15.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.15.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.15.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.16.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.16.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.16.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.16.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.16.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.16.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.16.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.16.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.16.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.16.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w2_bias                                | (32, 2880)           | torch.bfloat16 | cpu      | (32, 3072)           | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w2_weight                              | (32, 2880, 90, 16)   | torch.uint8 | cpu      | (32, 3072, 1536)     | torch.uint8 | cuda:0  
layers.17.mlp.experts.w2_weight_scale                        | (32, 2880, 90)       | torch.uint8 | cpu      | (32, 3072, 96)       | torch.uint8 | cuda:0  
layers.17.mlp.experts.w13_bias                               | (32, 5760)           | torch.bfloat16 | cpu      | (32, 6144)           | torch.bfloat16 | cuda:0  
layers.17.mlp.experts.w13_weight                             | (32, 5760, 90, 16)   | torch.uint8 | cpu      | (32, 6144, 1536)     | torch.uint8 | cuda:0  
layers.17.mlp.experts.w13_weight_scale                       | (32, 5760, 90)       | torch.uint8 | cpu      | (32, 6144, 96)       | torch.uint8 | cuda:0  
layers.17.mlp.router.bias                                    | (32,)                | torch.bfloat16 | cpu      | (32,)                | torch.bfloat16 | cuda:0  
layers.17.mlp.router.weight                                  | (32, 2880)           | torch.bfloat16 | cpu      | (32, 2880)           | torch.bfloat16 | cuda:0  
layers.17.post_attention_layernorm.weight                    | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.17.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (4096, 2880)         | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.17.attn.sinks                                         | (64,)                | torch.bfloat16 | cpu      | (64,)                | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.17.attn.qkv.weight                                    | (512, 2880)          | torch.bfloat16 | cpu      | (5120, 2880)         | torch.bfloat16 | cuda:0  
layers.18.input_layernorm.weight                             | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.18.attn.o_proj.bias                                   | (2880,)              | torch.bfloat16 | cpu      | (2880,)              | torch.bfloat16 | cuda:0  
layers.18.attn.o_proj.weight                                 | (2880, 4096)         | torch.bfloat16 | cpu      | (2880, 4096)         | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (4096,)              | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
layers.18.attn.qkv.bias                                      | (512,)               | torch.bfloat16 | cpu      | (5120,)              | torch.bfloat16 | cuda:0  
INFO 12-17 08:18:15 [loader.py:447] Loading weights took 12.96 seconds
INFO 12-17 08:18:16 [gpu_model_runner.py:1187] Model loading took 14.1511 GB and 13.834311 seconds
INFO 12-17 08:18:21 [kv_cache_utils.py:566] GPU KV cache size: 146,560 tokens
INFO 12-17 08:18:21 [kv_cache_utils.py:569] Maximum concurrency for 131,072 tokens per request: 1.12x
INFO 12-17 08:18:22 [core.py:151] init engine (profile, create kv cache, warmup model) took 6.22 seconds
INFO 12-17 08:18:22 [api_server.py:1028] Starting vLLM API server on http://0.0.0.0:8000
INFO 12-17 08:18:22 [launcher.py:26] Available routes are:
INFO 12-17 08:18:22 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 12-17 08:18:22 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 12-17 08:18:22 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 12-17 08:18:22 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 12-17 08:18:22 [launcher.py:34] Route: /health, Methods: GET
INFO 12-17 08:18:22 [launcher.py:34] Route: /load, Methods: GET
INFO 12-17 08:18:22 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 12-17 08:18:22 [launcher.py:34] Route: /version, Methods: GET
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /pooling, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /score, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /rerank, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 12-17 08:18:22 [launcher.py:34] Route: /invocations, Methods: POST
INFO 12-17 08:18:24 [logger.py:39] Received request cmpl-fb4b5898c8fb438fad090f30dd99957a-0: prompt: 'Who won the world series in 2020?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [20600, 3313, 290, 2375, 5594, 306, 220, 1323, 15, 30], lora_request: None, prompt_adapter_request: None.
INFO 12-17 08:18:24 [async_llm.py:221] Added request cmpl-fb4b5898c8fb438fad090f30dd99957a-0.
INFO 12-17 08:18:33 [loggers.py:80] Avg prompt throughput: 0.2 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:18:43 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:18:53 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:19:03 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:19:13 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:19:23 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:37706 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 12-17 08:19:27 [logger.py:39] Received request cmpl-8a8d4b61e7f14d7885fcfe3bfb36db3e-0: prompt: 'What are the main causes of climate change?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4827, 553, 290, 2758, 16431, 328, 16721, 3343, 30], lora_request: None, prompt_adapter_request: None.
INFO 12-17 08:19:27 [async_llm.py:221] Added request cmpl-8a8d4b61e7f14d7885fcfe3bfb36db3e-0.
INFO 12-17 08:19:33 [loggers.py:80] Avg prompt throughput: 0.9 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:19:43 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:19:53 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:20:03 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:20:13 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:20:23 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:38090 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 12-17 08:20:30 [logger.py:39] Received request cmpl-d57dea88fb35409081590534c768dbfc-0: prompt: 'Can you summarize the plot of Pride and Prejudice?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8475, 481, 104833, 290, 12857, 328, 71004, 326, 4659, 184888, 30], lora_request: None, prompt_adapter_request: None.
INFO 12-17 08:20:30 [async_llm.py:221] Added request cmpl-d57dea88fb35409081590534c768dbfc-0.
INFO 12-17 08:20:33 [loggers.py:80] Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:20:43 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:20:53 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:21:03 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:21:13 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:21:23 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:58066 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 12-17 08:21:32 [logger.py:39] Received request cmpl-f7e3dade468840e48283688b1e99e6cd-0: prompt: 'What are the health benefits of regular exercise?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4827, 553, 290, 3230, 8808, 328, 6953, 13526, 30], lora_request: None, prompt_adapter_request: None.
INFO 12-17 08:21:32 [async_llm.py:221] Added request cmpl-f7e3dade468840e48283688b1e99e6cd-0.
INFO 12-17 08:21:33 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 12-17 08:21:43 [loggers.py:80] Avg prompt throughput: 0.9 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:21:53 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:22:03 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:22:13 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO 12-17 08:22:14 [async_llm.py:347] Aborted request cmpl-f7e3dade468840e48283688b1e99e6cd-0.
INFO 12-17 08:22:15 [launcher.py:74] Shutting down FastAPI HTTP server.
