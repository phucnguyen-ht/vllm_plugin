==> Create WEIGHT(LLM HEAD): sum(output_partition_sizes) = 201088, input_size_per_partition = 2880, output_partition_sizes = [201088]
=======================

==> Create WEIGHT(LLM HEAD): sum(output_partition_sizes) = 201088, input_size_per_partition = 2880, output_partition_sizes = [201088]
=======================

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
---------------------------------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
---------------------------------------------------------------------------
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
name='input_ids'               | dtype=torch.int32     | shape=(10,)                | 
name='input_embeds'            | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.2305,      0.2969,     -0.5195,     -0.0237,      0.3320,     -1.5078,      0.2324,     -0.3184,     -1.7656,      3.0938,      0.0674,     -1.0312,     -1.5859,     12.5625,     -0.3164,     -0.0264],
        [     1.4766,     -2.0625,      0.3281,      0.1162,     -2.2656,     -4.3438,     -0.0249,      0.4004,     -0.7305,     -0.2656,      0.0703,     -0.0737,      0.8906,     -1.1172,      0.1338,     -0.7461],
        [     0.5586,      1.0625,      0.5625,     -0.1377,      0.7031,      0.1060,      0.0649,     -0.0250,     -2.0938,      2.1562,      0.0520,      0.0272,      2.2500,    -22.2500,      0.3125,     -0.3203],
        [    -0.4297,      0.4941,      1.6406,     -0.0505,      0.3809,      0.2314,      0.3359,      0.1177,      3.2812,      0.8320,     -0.1226,     -0.5078,      2.3906,     10.9375,     -0.0596,     -0.1719],
        [    -0.6875,      0.0154,      2.0938,      0.2275,      1.3125,      1.1484,      0.1494,     -0.4980,      0.8438,      0.0515,      0.1348,     -0.0129,     -2.6094,      9.8125,     -0.0679,     -0.0425],
        [    -0.5195,      0.3496,      1.1641,     -0.0762,      2.0312,      3.7188,     -0.2754,     -0.1611,      2.0781,      1.5234,      0.0996,     -0.4004,      2.2656,     -8.3750,      0.1260,      0.0210],
        [    -0.4727,      0.6562,      0.7070,      0.1260,      0.4531,     -2.0469,      0.0566,      0.0869,     -1.7266,      0.2578,      0.0098,      0.2500,     -1.0625,     -6.3750,      0.0398,     -0.2100],
        [     0.5273,     -1.2656,      1.3516,      0.1050,     -1.1641,      1.4453,      0.0405,     -0.1611,      1.2812,     -0.2129,     -0.0216,      0.3418,      1.3125,    -40.7500,      0.1260,      0.2871],
        [    -0.9531,      0.6094,      1.1641,      0.1074,     -0.0085,     -2.3906,     -0.0918,      0.0498,      0.3496,      0.2832,      0.0277,      0.2227,      2.5156,     -8.8125,      0.1484,     -0.0339],
        [     0.0977,     -0.3125,      0.3574,      0.0947,     -0.2490,     -2.2812,     -0.0317,     -0.1133,     -4.2812,      1.0078,      0.1338,      0.1943,      1.5547,      0.4688,     -0.0781,     -0.2695]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1250,      0.1445,     -0.2178,     -0.0342,      0.1494,     -0.3984,      0.1807,     -0.2539,     -0.5625,      1.2344,      0.1138,     -0.5820,     -0.6055,      3.4531,     -0.4609,     -0.0244],
        [     0.8203,     -1.0391,      0.1426,      0.1738,     -1.0469,     -1.1875,     -0.0200,      0.3281,     -0.2393,     -0.1089,      0.1226,     -0.0430,      0.3496,     -0.3164,      0.2012,     -0.7109],
        [     0.2471,      0.4258,      0.1924,     -0.1621,      0.2578,      0.0229,      0.0413,     -0.0162,     -0.5430,      0.7031,      0.0718,      0.0126,      0.6992,     -5.0000,      0.3730,     -0.2422],
        [    -0.2236,      0.2324,      0.6602,     -0.0703,      0.1650,      0.0588,      0.2520,      0.0898,      1.0000,      0.3184,     -0.1992,     -0.2773,      0.8789,      2.8906,     -0.0840,     -0.1533],
        [    -0.3555,      0.0072,      0.8398,      0.3145,      0.5625,      0.2891,      0.1108,     -0.3770,      0.2559,      0.0197,      0.2178,     -0.0070,     -0.9492,      2.5781,     -0.0942,     -0.0374],
        [    -0.2598,      0.1582,      0.4512,     -0.1016,      0.8438,      0.9062,     -0.1982,     -0.1187,      0.6094,      0.5586,      0.1562,     -0.2090,      0.8008,     -2.1250,      0.1699,      0.0179],
        [    -0.2637,      0.3320,      0.3047,      0.1875,      0.2100,     -0.5586,      0.0454,      0.0713,     -0.5664,      0.1060,      0.0171,      0.1455,     -0.4180,     -1.7969,      0.0601,     -0.1992],
        [     0.2812,     -0.6094,      0.5586,      0.1494,     -0.5156,      0.3770,      0.0310,     -0.1260,      0.4004,     -0.0835,     -0.0361,      0.1904,      0.4941,    -11.0625,      0.1816,      0.2617],
        [    -0.4707,      0.2715,      0.4434,      0.1416,     -0.0035,     -0.5742,     -0.0649,      0.0359,      0.1011,      0.1025,      0.0425,      0.1143,      0.8750,     -2.2031,      0.1973,     -0.0286],
        [     0.0520,     -0.1504,      0.1475,      0.1348,     -0.1099,     -0.5938,     -0.0243,     -0.0884,     -1.3359,      0.3945,      0.2217,      0.1079,      0.5820,      0.1260,     -0.1123,     -0.2451]]
[after   INPUT_LAYER_NORM] residual is None
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.0106, -0.1147,  0.0845,  0.0498, -0.0864, -0.0649, -0.0240, -0.0532,  0.6602, -0.2100, -0.0238, -0.0708, -0.5820,  0.8320,  0.0864, -0.1885],
        [-0.0583, -0.2344, -0.0579, -0.0182,  0.1572,  1.0156, -0.0349, -0.2080,  0.3359, -0.2070, -0.0078, -0.2051,  1.6172,  0.6953, -0.1729,  0.1011],
        [-0.0305, -0.4473, -0.1035,  0.1924, -0.3750, -0.6758, -0.0408,  0.1113,  0.5898,  0.3711,  0.0649, -0.3984,  0.4531,  0.7773, -0.5078,  0.4062],
        [-0.0923,  0.1118,  0.0933,  0.2256, -0.2334,  0.1836,  0.0369,  0.0840,  1.2891,  0.2598,  0.0378, -0.2773, -0.1816,  0.0674, -0.2402,  0.1387],
        [-0.1953,  0.0405,  0.0898, -0.0031, -0.4785,  1.0859, -0.0322, -0.1289,  0.8789, -0.4316,  0.1006, -0.0245,  1.4297,  0.3301, -0.0366, -0.0747],
        [ 0.0444,  0.3145, -0.0100,  0.0791, -0.0327, -0.1416, -0.0320, -0.0262,  1.4375,  0.2988,  0.0559,  0.0166,  0.8047, -0.1436,  0.0289, -0.5430],
        [-0.0928, -0.0845,  0.0159, -0.0369,  0.0693, -0.7070, -0.0432, -0.0131,  0.5781, -0.0109,  0.2139, -0.1523,  0.1201,  0.1001,  0.0356, -0.0294],
        [-0.2129,  0.0596,  0.0962,  0.1060,  0.2324,  0.0205, -0.0518,  0.0186,  1.3125,  0.1260,  0.1895, -0.3418, -2.6406, -0.4844, -0.1621, -0.0659],
        [-0.1582,  0.2305, -0.2871, -0.0302, -0.0090,  0.6523,  0.0674,  0.1152,  3.0000, -0.3516,  0.3711, -0.2852,  0.0197, -1.8672, -0.2002, -0.1309],
        [-0.0176, -0.0330, -0.1338, -0.0077,  0.3672,  0.5781,  0.0305, -0.0674,  0.6797, -0.5000,  0.1348, -0.2305, -0.4785,  0.2129,  0.0645, -0.0742]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1611,      0.0928,     -0.2031,      0.0532,      0.0996,     -0.2539,      0.2500,     -0.4316,     -0.1680,      0.9766,      0.0957,     -0.7969,     -0.3789,      0.7617,     -0.4219,     -0.2119],
        [     0.9453,     -1.1641,      0.1260,      0.2002,     -0.8516,     -0.5352,     -0.0718,      0.2236,     -0.0601,     -0.1592,      0.1367,     -0.2012,      0.4375,     -0.0240,     -0.0713,     -0.6328],
        [     0.3145,      0.2793,      0.1914,      0.0991,      0.1187,     -0.0820,      0.0258,      0.0894,     -0.2041,      0.7578,      0.2285,     -0.2393,      0.4219,     -1.0938,     -0.3203,      0.0752],
        [    -0.3320,      0.2949,      0.7695,      0.3398,      0.0569,      0.0635,      0.4238,      0.2236,      0.6641,      0.3496,     -0.1777,     -0.5391,      0.3672,      0.5977,     -0.5234,     -0.0311],
        [    -0.5586,      0.0271,      0.9648,      0.4336,      0.3184,      0.3418,      0.1338,     -0.6914,      0.2480,     -0.1221,      0.4883,     -0.0256,     -0.1953,      0.5469,     -0.1816,     -0.1094],
        [    -0.3105,      0.3301,      0.5273,      0.0058,      0.7891,      0.5625,     -0.3594,     -0.2129,      0.5234,      0.6016,      0.3340,     -0.2695,      0.5234,     -0.4727,      0.2773,     -0.5000],
        [    -0.4180,      0.3223,      0.3730,      0.2012,      0.2334,     -0.4922,      0.0178,      0.0957,     -0.1934,      0.0923,      0.5430,      0.0781,     -0.1826,     -0.3965,      0.1533,     -0.2598],
        [     0.2002,     -0.5859,      0.6445,      0.4102,     -0.3594,      0.2246,     -0.0128,     -0.1572,      0.3770,     -0.0280,      0.3496,      0.0000,     -0.2217,     -2.2344,     -0.0630,      0.2080],
        [    -0.6914,      0.4004,      0.3809,      0.1465,     -0.0066,     -0.2617,     -0.0272,      0.1787,      0.4746,     -0.0215,      0.8164,     -0.0420,      0.4121,     -0.5664,     -0.0879,     -0.1504],
        [     0.0549,     -0.1807,      0.1069,      0.1816,      0.0488,     -0.2812,     -0.0015,     -0.2148,     -0.5625,      0.1748,      0.6016,     -0.0266,      0.1924,      0.0398,     -0.0256,     -0.3457]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.2412,      0.1816,     -0.4355,      0.0261,      0.2461,     -1.5703,      0.2080,     -0.3711,     -1.1094,      2.8906,      0.0435,     -1.1016,     -2.1719,     13.3750,     -0.2305,     -0.2148],
        [     1.4219,     -2.2969,      0.2695,      0.0981,     -2.1094,     -3.3281,     -0.0598,      0.1924,     -0.3945,     -0.4727,      0.0625,     -0.2793,      2.5000,     -0.4219,     -0.0391,     -0.6445],
        [     0.5273,      0.6172,      0.4590,      0.0547,      0.3281,     -0.5703,      0.0242,      0.0864,     -1.5000,      2.5312,      0.1172,     -0.3711,      2.7031,    -21.5000,     -0.1953,      0.0859],
        [    -0.5234,      0.6055,      1.7344,      0.1748,      0.1475,      0.4141,      0.3730,      0.2012,      4.5625,      1.0938,     -0.0850,     -0.7852,      2.2031,     11.0000,     -0.3008,     -0.0332],
        [    -0.8828,      0.0559,      2.1875,      0.2246,      0.8359,      2.2344,      0.1172,     -0.6250,      1.7188,     -0.3809,      0.2354,     -0.0374,     -1.1797,     10.1250,     -0.1045,     -0.1172],
        [    -0.4746,      0.6641,      1.1562,      0.0029,      2.0000,      3.5781,     -0.3066,     -0.1875,      3.5156,      1.8203,      0.1553,     -0.3828,      3.0625,     -8.5000,      0.1553,     -0.5234],
        [    -0.5664,      0.5703,      0.7227,      0.0889,      0.5234,     -2.7500,      0.0134,      0.0737,     -1.1484,      0.2471,      0.2236,      0.0977,     -0.9414,     -6.2812,      0.0752,     -0.2393],
        [     0.3145,     -1.2031,      1.4453,      0.2109,     -0.9297,      1.4688,     -0.0112,     -0.1426,      2.5938,     -0.0869,      0.1680,      0.0000,     -1.3281,    -41.2500,     -0.0361,      0.2207],
        [    -1.1094,      0.8398,      0.8750,      0.0771,     -0.0176,     -1.7344,     -0.0244,      0.1650,      3.3438,     -0.0684,      0.3984,     -0.0625,      2.5312,    -10.6875,     -0.0518,     -0.1650],
        [     0.0801,     -0.3457,      0.2236,      0.0869,      0.1182,     -1.7031,     -0.0012,     -0.1807,     -3.5938,      0.5078,      0.2695,     -0.0361,      1.0781,      0.6797,     -0.0137,     -0.3438]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0040,     -0.2461,      0.2246,     -0.0349,      0.1221,      0.0879,     -0.1816,      0.0913,      1.5703,      0.3848,      0.0014,      0.3184,      0.3164,      2.3594,     -0.1748,      0.7031],
        [    -0.1660,      0.0732,      0.1221,     -0.0791,     -0.0427,     -0.8125,     -0.0300,      0.1318,      0.4609,     -0.3789,      0.1196,     -0.2432,      1.4531,      0.3066,     -0.0344,      0.4551],
        [    -0.0586,     -0.5898,      0.2773,     -0.0869,      0.1689,     -0.6172,      0.0903,     -0.0150,      2.1094,     -0.2256,      0.0608,     -0.2051,      0.7188,     -0.4629,     -0.0942,     -0.3711],
        [    -0.0588,     -0.2812,      0.0510,      0.0488,     -0.4102,     -0.1338,     -0.2461,      0.0430,     -1.5547,      0.0240,      0.0420,     -0.0972,      2.5000,     -0.3945,     -0.0151,     -0.2832],
        [    -0.1816,      0.0299,      0.2930,     -0.0366,      1.0547,     -0.8867,     -0.2852,     -0.1006,      0.4805,     -0.1992,     -0.0525,     -0.2910,      3.7031,     -0.4941,      0.1855,     -0.4707],
        [     0.0762,      0.1172,      0.1855,      0.1885,     -0.9805,      0.1455,     -0.0233,     -0.0479,      0.6562,     -0.2490,     -0.0786,      0.0374,     -1.2109,     -1.3750,     -0.0311,     -0.0325],
        [     0.1787,      0.0540,      0.1260,     -0.0752,     -1.1484,      1.4922,     -0.1719,      0.1387,      0.4648,      0.0952,      0.0889,     -0.1729,     -0.1060,      4.8750,      0.0928,     -0.1553],
        [    -0.3125,      0.3223,     -0.3164,     -0.0322,      0.3516,      0.7695,     -0.0908,      0.0208,      1.1953,      0.1924,      0.0742,     -0.4609,      0.2373,     -1.3672,      0.0332,      0.1299],
        [     0.3691,      0.0664,     -0.6211,     -0.0713,     -1.0078,      2.7500,      0.0209,      0.1445,      1.8047,     -0.5000,      0.0684,     -0.0981,     -1.3750,     -4.3125,     -0.3047,      0.7344],
        [    -0.0449,      0.4219,      0.1670,      0.0693,      0.6992,     -0.7617,     -0.1396,     -0.0559,     -0.0786,     -0.4395,      0.0364,      0.3633,     -2.5156,     -0.2773,      0.0801,     -0.0515]]
-------------------------
name='positions layer 0'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 0'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0040,     -0.2461,      0.2246,     -0.0349,      0.1221,      0.0879,     -0.1816,      0.0913,      1.5703,      0.3848,      0.0014,      0.3184,      0.3164,      2.3594,     -0.1748,      0.7031],
        [    -0.1660,      0.0732,      0.1221,     -0.0791,     -0.0427,     -0.8125,     -0.0300,      0.1318,      0.4609,     -0.3789,      0.1196,     -0.2432,      1.4531,      0.3066,     -0.0344,      0.4551],
        [    -0.0586,     -0.5898,      0.2773,     -0.0869,      0.1689,     -0.6172,      0.0903,     -0.0150,      2.1094,     -0.2256,      0.0608,     -0.2051,      0.7188,     -0.4629,     -0.0942,     -0.3711],
        [    -0.0588,     -0.2812,      0.0510,      0.0488,     -0.4102,     -0.1338,     -0.2461,      0.0430,     -1.5547,      0.0240,      0.0420,     -0.0972,      2.5000,     -0.3945,     -0.0151,     -0.2832],
        [    -0.1816,      0.0299,      0.2930,     -0.0366,      1.0547,     -0.8867,     -0.2852,     -0.1006,      0.4805,     -0.1992,     -0.0525,     -0.2910,      3.7031,     -0.4941,      0.1855,     -0.4707],
        [     0.0762,      0.1172,      0.1855,      0.1885,     -0.9805,      0.1455,     -0.0233,     -0.0479,      0.6562,     -0.2490,     -0.0786,      0.0374,     -1.2109,     -1.3750,     -0.0311,     -0.0325],
        [     0.1787,      0.0540,      0.1260,     -0.0752,     -1.1484,      1.4922,     -0.1719,      0.1387,      0.4648,      0.0952,      0.0889,     -0.1729,     -0.1060,      4.8750,      0.0928,     -0.1553],
        [    -0.3125,      0.3223,     -0.3164,     -0.0322,      0.3516,      0.7695,     -0.0908,      0.0208,      1.1953,      0.1924,      0.0742,     -0.4609,      0.2373,     -1.3672,      0.0332,      0.1299],
        [     0.3691,      0.0664,     -0.6211,     -0.0713,     -1.0078,      2.7500,      0.0209,      0.1445,      1.8047,     -0.5000,      0.0684,     -0.0981,     -1.3750,     -4.3125,     -0.3047,      0.7344],
        [    -0.0449,      0.4219,      0.1670,      0.0693,      0.6992,     -0.7617,     -0.1396,     -0.0559,     -0.0786,     -0.4395,      0.0364,      0.3633,     -2.5156,     -0.2773,      0.0801,     -0.0515]]
name='residual layer 0'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.2412,      0.1816,     -0.4355,      0.0261,      0.2461,     -1.5703,      0.2080,     -0.3711,     -1.1094,      2.8906,      0.0435,     -1.1016,     -2.1719,     13.3750,     -0.2305,     -0.2148],
        [     1.4219,     -2.2969,      0.2695,      0.0981,     -2.1094,     -3.3281,     -0.0598,      0.1924,     -0.3945,     -0.4727,      0.0625,     -0.2793,      2.5000,     -0.4219,     -0.0391,     -0.6445],
        [     0.5273,      0.6172,      0.4590,      0.0547,      0.3281,     -0.5703,      0.0242,      0.0864,     -1.5000,      2.5312,      0.1172,     -0.3711,      2.7031,    -21.5000,     -0.1953,      0.0859],
        [    -0.5234,      0.6055,      1.7344,      0.1748,      0.1475,      0.4141,      0.3730,      0.2012,      4.5625,      1.0938,     -0.0850,     -0.7852,      2.2031,     11.0000,     -0.3008,     -0.0332],
        [    -0.8828,      0.0559,      2.1875,      0.2246,      0.8359,      2.2344,      0.1172,     -0.6250,      1.7188,     -0.3809,      0.2354,     -0.0374,     -1.1797,     10.1250,     -0.1045,     -0.1172],
        [    -0.4746,      0.6641,      1.1562,      0.0029,      2.0000,      3.5781,     -0.3066,     -0.1875,      3.5156,      1.8203,      0.1553,     -0.3828,      3.0625,     -8.5000,      0.1553,     -0.5234],
        [    -0.5664,      0.5703,      0.7227,      0.0889,      0.5234,     -2.7500,      0.0134,      0.0737,     -1.1484,      0.2471,      0.2236,      0.0977,     -0.9414,     -6.2812,      0.0752,     -0.2393],
        [     0.3145,     -1.2031,      1.4453,      0.2109,     -0.9297,      1.4688,     -0.0112,     -0.1426,      2.5938,     -0.0869,      0.1680,      0.0000,     -1.3281,    -41.2500,     -0.0361,      0.2207],
        [    -1.1094,      0.8398,      0.8750,      0.0771,     -0.0176,     -1.7344,     -0.0244,      0.1650,      3.3438,     -0.0684,      0.3984,     -0.0625,      2.5312,    -10.6875,     -0.0518,     -0.1650],
        [     0.0801,     -0.3457,      0.2236,      0.0869,      0.1182,     -1.7031,     -0.0012,     -0.1807,     -3.5938,      0.5078,      0.2695,     -0.0361,      1.0781,      0.6797,     -0.0137,     -0.3438]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1348,     -0.0308,     -0.0845,     -0.0082,      0.1289,     -0.3301,      0.0184,     -0.1934,      0.1416,      1.1406,      0.0444,     -0.4258,     -0.6289,      4.5000,     -0.3418,      0.2754],
        [     0.5820,     -0.8945,      0.1318,      0.0150,     -0.6367,     -0.7734,     -0.0530,      0.1875,      0.0173,     -0.2500,      0.1523,     -0.2393,      1.1328,     -0.0278,     -0.0522,     -0.0903],
        [     0.2373,      0.0120,      0.2715,     -0.0277,      0.1602,     -0.2422,      0.0732,      0.0452,      0.1729,      0.7422,      0.1621,     -0.2891,      1.0703,     -5.8125,     -0.2246,     -0.1484],
        [    -0.2891,      0.1396,      0.6445,      0.1885,     -0.0830,      0.0562,      0.0806,      0.1514,      0.8398,      0.3535,     -0.0386,     -0.4355,      1.4453,      2.7500,     -0.2412,     -0.1611],
        [    -0.4863,      0.0342,      0.8242,      0.1465,      0.5469,      0.2480,     -0.0977,     -0.4160,      0.5625,     -0.1689,      0.1514,     -0.1484,      0.7148,      2.2969,      0.0569,     -0.2754],
        [    -0.2324,      0.3965,      0.5703,      0.1895,      0.3770,      0.8750,     -0.2451,     -0.1719,      1.3672,      0.5820,      0.0811,     -0.1992,      0.6680,     -3.0000,      0.1108,     -0.3340],
        [    -0.2432,      0.3418,      0.3887,      0.0146,     -0.2500,     -0.3184,     -0.1270,      0.1670,     -0.2412,      0.1367,      0.3555,     -0.0469,     -0.4062,     -0.4629,      0.1611,     -0.2539],
        [     0.0010,     -0.3965,      0.4258,      0.1572,     -0.1904,      0.4688,     -0.0674,     -0.0791,      1.1016,      0.0347,      0.2266,     -0.2363,     -0.3496,    -11.5625,     -0.0023,      0.1855],
        [    -0.3828,      0.4102,      0.0962,      0.0052,     -0.3379,      0.2129,     -0.0023,      0.2012,      1.5000,     -0.1875,      0.4375,     -0.0825,      0.3711,     -4.0625,     -0.2832,      0.3027],
        [     0.0171,      0.0320,      0.1377,      0.1289,      0.2520,     -0.4805,     -0.0869,     -0.1436,     -1.0000,      0.0211,      0.2676,      0.1572,     -0.4297,      0.1016,      0.0493,     -0.1963]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.2451,     -0.0645,     -0.2109,     -0.0088,      0.3672,     -1.4844,      0.0264,     -0.2793,      0.4609,      3.2812,      0.0449,     -0.7812,     -1.8594,     15.7500,     -0.4062,      0.4883],
        [     1.2578,     -2.2188,      0.3906,      0.0190,     -2.1562,     -4.1250,     -0.0898,      0.3242,      0.0664,     -0.8516,      0.1816,     -0.5234,      3.9531,     -0.1152,     -0.0732,     -0.1895],
        [     0.4688,      0.0273,      0.7344,     -0.0322,      0.4961,     -1.1875,      0.1143,      0.0713,      0.6094,      2.3125,      0.1777,     -0.5781,      3.4219,    -22.0000,     -0.2891,     -0.2852],
        [    -0.5820,      0.3242,      1.7891,      0.2236,     -0.2617,      0.2812,      0.1270,      0.2441,      3.0000,      1.1172,     -0.0430,     -0.8828,      4.6875,     10.6250,     -0.3164,     -0.3164],
        [    -1.0625,      0.0859,      2.4844,      0.1875,      1.8906,      1.3438,     -0.1680,     -0.7266,      2.2031,     -0.5781,      0.1826,     -0.3281,      2.5312,      9.6250,      0.0811,     -0.5859],
        [    -0.3984,      0.7812,      1.3438,      0.1914,      1.0156,      3.7188,     -0.3301,     -0.2354,      4.1875,      1.5703,      0.0767,     -0.3457,      1.8516,     -9.8750,      0.1240,     -0.5547],
        [    -0.3867,      0.6250,      0.8477,      0.0137,     -0.6250,     -1.2578,     -0.1582,      0.2129,     -0.6836,      0.3418,      0.3125,     -0.0752,     -1.0469,     -1.4062,      0.1680,     -0.3945],
        [     0.0020,     -0.8828,      1.1250,      0.1787,     -0.5781,      2.2344,     -0.1021,     -0.1221,      3.7812,      0.1055,      0.2422,     -0.4609,     -1.0938,    -42.5000,     -0.0029,      0.3516],
        [    -0.7422,      0.9062,      0.2539,      0.0059,     -1.0234,      1.0156,     -0.0035,      0.3086,      5.1562,     -0.5703,      0.4668,     -0.1602,      1.1562,    -15.0000,     -0.3555,      0.5703],
        [     0.0352,      0.0762,      0.3906,      0.1562,      0.8164,     -2.4688,     -0.1406,     -0.2363,     -3.6719,      0.0684,      0.3066,      0.3281,     -1.4375,      0.4023,      0.0664,     -0.3945]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1279,      0.1465,      0.1904,      0.2637,      0.2852,      0.8242,     -0.0177,      0.2363,      3.6094,     -0.3887,     -0.0011,      0.1084,     -0.6836,      0.6367,     -0.2363,      0.9805],
        [     0.0903,      0.5820,      0.2334,      0.0327,      0.1060,      2.1562,      0.0520,     -0.1592,      1.6328,     -0.3516,     -0.0991,      0.1465,      6.0312,      1.2891,      0.2695,     -0.3242],
        [     0.0142,      0.2246,      0.1221,      0.2832,      0.0134,      0.7812,     -0.0811,     -0.0286,     -0.2227,     -0.9961,      0.0083,      0.0248,      3.9688,     -0.4961,     -0.0986,      0.3633],
        [    -0.0293,      0.2314,     -0.0635,     -0.0903,      0.0216,     -1.3438,     -0.0884,      0.3262,      0.3867,      0.0147,      0.2910,      0.0874,      5.0625,      0.1250,     -0.0256,      0.9766],
        [    -0.0452,     -0.0952,     -0.3691,     -0.2734,     -0.1147,     -2.6562,      0.0344,      0.2158,      0.1592,     -0.6875,     -0.1113,      0.1021,      4.0312,     -0.6719,      0.1006,     -0.1504],
        [    -0.0265,      0.1914,     -0.0742,     -0.1260,     -0.7148,     -0.9297,      0.2500,     -0.1523,      0.2871,     -0.6953,     -0.0449,      0.2139,      6.5312,      0.1777,      0.2363,     -0.0325],
        [    -0.0408,      0.8906,     -0.2676,     -0.0074,     -0.5430,     -1.0859,      0.2988,     -0.1406,      0.1309,      0.3066,      0.1406,      0.0549,      6.2500,      0.7461,      0.0869,     -0.5469],
        [     0.0018,      0.7773,     -0.1836,     -0.1826,     -0.3848,     -0.1719,      0.2295,      0.0977,      0.9570,     -0.0184,      0.0349,     -0.1045,      5.5000,      0.3223,      0.1787,     -0.4414],
        [    -0.0996,      0.2793,     -0.2715,     -0.2832,      0.0151,      0.7422,      0.2793,     -0.0962,     -0.3281,     -0.3047,      0.1553,     -0.1426,      1.4766,      0.1465,      0.3281,      0.5469],
        [     0.0347,      0.1211,      0.1816,      0.0581,     -0.3965,      0.4824,      0.0491,     -0.0079,      0.6094,      0.1631,      0.0723,      0.0659,     -0.5391,      0.4082,     -0.0593,      0.3086]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.2393,  0.0378, -0.0092,  0.3262,  0.2393, -0.0767,  0.0085, -0.0383,  0.4980,  0.8008,  0.0557, -0.4160, -0.2715,  0.7383, -0.6719,  0.7344],
        [ 0.8125, -0.7109,  0.2617,  0.0620, -0.7109, -0.2148, -0.0352,  0.1387,  0.1953, -0.3105,  0.0977, -0.2188,  1.0000,  0.0496,  0.1924, -0.2402],
        [ 0.2988,  0.1128,  0.3691,  0.3086,  0.1807, -0.0454,  0.0317,  0.0369,  0.0459,  0.3516,  0.2275, -0.3301,  0.7617, -0.9805, -0.3906,  0.0376],
        [-0.4121,  0.2695,  0.8047,  0.1777, -0.0923, -0.1289,  0.0396,  0.5352,  0.4336,  0.3281,  0.3301, -0.5156,  1.0859,  0.5078, -0.3750,  0.3438],
        [-0.6719, -0.0041,  0.8945, -0.1035,  0.6172, -0.1445, -0.1250, -0.4336,  0.2734, -0.3340,  0.0854, -0.1328,  0.6641,  0.3828,  0.1807, -0.3496],
        [-0.2871,  0.4727,  0.5938,  0.0879,  0.1162,  0.3398, -0.0830, -0.3652,  0.5742,  0.2539,  0.0422, -0.0854,  0.9375, -0.4609,  0.3965, -0.3066],
        [-0.3203,  0.8164,  0.3008,  0.0093, -0.5000, -0.3164,  0.1621,  0.0752, -0.0791,  0.2090,  0.6680, -0.0146,  0.6484, -0.0349,  0.3105, -0.5508],
        [ 0.0025, -0.0493,  0.4258, -0.0051, -0.3594,  0.2432,  0.1270, -0.0221,  0.5859,  0.0244,  0.3555, -0.3555,  0.4785, -1.9297,  0.1865, -0.0454],
        [-0.4844,  0.4902, -0.0070, -0.3184, -0.3320,  0.1826,  0.2432,  0.1709,  0.5312, -0.2168,  0.7031, -0.1680,  0.2520, -0.6016, -0.0256,  0.5000],
        [ 0.0496,  0.1006,  0.2812,  0.3027,  0.1709, -0.2559, -0.0996, -0.2422, -0.4141,  0.0708,  0.5312,  0.2695, -0.2334,  0.0405,  0.0082, -0.0476]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3730,      0.0820,     -0.0205,      0.2539,      0.6523,     -0.6602,      0.0087,     -0.0430,      4.0625,      2.8906,      0.0439,     -0.6719,     -2.5469,     16.3750,     -0.6406,      1.4688],
        [     1.3516,     -1.6406,      0.6250,      0.0518,     -2.0469,     -1.9688,     -0.0378,      0.1650,      1.7031,     -1.2031,      0.0825,     -0.3770,     10.0000,      1.1719,      0.1963,     -0.5156],
        [     0.4824,      0.2520,      0.8555,      0.2500,      0.5078,     -0.4062,      0.0332,      0.0427,      0.3867,      1.3125,      0.1855,     -0.5547,      7.3750,    -22.5000,     -0.3867,      0.0781],
        [    -0.6094,      0.5547,      1.7266,      0.1328,     -0.2402,     -1.0625,      0.0386,      0.5703,      3.3906,      1.1328,      0.2480,     -0.7969,      9.7500,     10.7500,     -0.3418,      0.6602],
        [    -1.1094,     -0.0093,      2.1094,     -0.0859,      1.7734,     -1.3125,     -0.1338,     -0.5117,      2.3594,     -1.2656,      0.0713,     -0.2266,      6.5625,      8.9375,      0.1816,     -0.7344],
        [    -0.4258,      0.9727,      1.2656,      0.0654,      0.3008,      2.7812,     -0.0801,     -0.3867,      4.4688,      0.8750,      0.0317,     -0.1318,      8.3750,     -9.6875,      0.3594,     -0.5859],
        [    -0.4277,      1.5156,      0.5781,      0.0063,     -1.1719,     -2.3438,      0.1406,      0.0723,     -0.5547,      0.6484,      0.4531,     -0.0203,      5.1875,     -0.6602,      0.2539,     -0.9414],
        [     0.0038,     -0.1055,      0.9414,     -0.0039,     -0.9609,      2.0625,      0.1270,     -0.0244,      4.7500,      0.0869,      0.2773,     -0.5664,      4.4062,    -42.2500,      0.1758,     -0.0898],
        [    -0.8438,      1.1875,     -0.0176,     -0.2773,     -1.0078,      1.7578,      0.2754,      0.2129,      4.8125,     -0.8750,      0.6211,     -0.3027,      2.6250,    -14.8750,     -0.0273,      1.1172],
        [     0.0698,      0.1973,      0.5703,      0.2148,      0.4199,     -1.9844,     -0.0918,     -0.2441,     -3.0625,      0.2314,      0.3789,      0.3945,     -1.9766,      0.8125,      0.0071,     -0.0859]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0388,      0.1455,     -0.1846,     -0.0030,      0.3262,     -1.4688,     -0.0413,      0.0771,      0.5742,     -0.4258,     -0.2021,     -0.0134,      1.8281,      0.6250,     -0.2207,     -0.1084],
        [     0.3438,     -0.0786,      0.0522,     -0.0908,      0.0088,     -0.4219,      0.0825,     -0.1699,      1.0625,      0.5117,      0.2246,     -0.1128,     -1.0156,     -0.1797,      0.3164,     -1.9297],
        [     0.2109,     -0.1943,      0.6875,      0.0327,      0.5469,     -2.1875,     -0.1060,     -0.1797,     -0.0684,      0.2383,     -0.5703,      0.3477,      0.4961,      1.0859,      0.4688,     -0.6875],
        [     0.1797,      0.1797,      0.6133,      0.5703,      0.5117,     -2.3594,      0.0364,     -0.0605,      1.0234,      0.8828,     -0.4160,      0.0203,     -0.9219,     -1.3828,      1.1094,      0.1494],
        [    -0.3887,     -1.2266,     -0.0082,     -0.2812,     -0.1670,     -3.1719,      0.2373,     -0.0442,     -2.6875,     -0.4473,      0.1445,     -0.4043,     -0.0942,      3.3750,     -0.5000,      1.1250],
        [     0.1895,     -0.0275,     -0.3477,      0.2295,     -0.7266,     -3.0469,      0.2949,     -0.6211,     -4.0938,      0.3574,     -0.0012,     -0.1011,      0.5117,     -2.3125,      0.0427,     -0.4238],
        [    -0.1318,     -0.1436,      0.4570,     -0.0093,      0.3633,      2.9844,      0.0330,     -0.0013,     -1.9219,     -0.4922,     -0.1816,     -0.2715,      0.1357,      2.3281,     -0.3633,     -0.3750],
        [    -0.1436,     -0.4453,     -0.3984,     -0.1318,      0.5977,      2.2500,     -0.0425,      0.0796,      0.5859,      0.3906,      0.0082,     -0.2021,     -2.2656,     -0.4590,      0.1973,     -0.1377],
        [     0.0117,     -0.3027,     -0.3574,     -0.4141,     -0.6719,      5.8438,      0.0114,     -0.1777,      2.0469,     -0.1211,      0.0552,     -0.0442,     -2.7188,     -0.5820,      0.0018,      0.5391],
        [     0.0231,     -0.2314,      0.4180,      0.1445,      0.1748,     -0.6562,     -0.1719,      0.1641,     -1.9375,     -0.3223,      0.0645,      0.4668,     -0.7500,     -3.1250,      0.1260,     -0.0315]]
-------------------------
name='positions layer 1'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 1'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0388,      0.1455,     -0.1846,     -0.0030,      0.3262,     -1.4688,     -0.0413,      0.0771,      0.5742,     -0.4258,     -0.2021,     -0.0134,      1.8281,      0.6250,     -0.2207,     -0.1084],
        [     0.3438,     -0.0786,      0.0522,     -0.0908,      0.0088,     -0.4219,      0.0825,     -0.1699,      1.0625,      0.5117,      0.2246,     -0.1128,     -1.0156,     -0.1797,      0.3164,     -1.9297],
        [     0.2109,     -0.1943,      0.6875,      0.0327,      0.5469,     -2.1875,     -0.1060,     -0.1797,     -0.0684,      0.2383,     -0.5703,      0.3477,      0.4961,      1.0859,      0.4688,     -0.6875],
        [     0.1797,      0.1797,      0.6133,      0.5703,      0.5117,     -2.3594,      0.0364,     -0.0605,      1.0234,      0.8828,     -0.4160,      0.0203,     -0.9219,     -1.3828,      1.1094,      0.1494],
        [    -0.3887,     -1.2266,     -0.0082,     -0.2812,     -0.1670,     -3.1719,      0.2373,     -0.0442,     -2.6875,     -0.4473,      0.1445,     -0.4043,     -0.0942,      3.3750,     -0.5000,      1.1250],
        [     0.1895,     -0.0275,     -0.3477,      0.2295,     -0.7266,     -3.0469,      0.2949,     -0.6211,     -4.0938,      0.3574,     -0.0012,     -0.1011,      0.5117,     -2.3125,      0.0427,     -0.4238],
        [    -0.1318,     -0.1436,      0.4570,     -0.0093,      0.3633,      2.9844,      0.0330,     -0.0013,     -1.9219,     -0.4922,     -0.1816,     -0.2715,      0.1357,      2.3281,     -0.3633,     -0.3750],
        [    -0.1436,     -0.4453,     -0.3984,     -0.1318,      0.5977,      2.2500,     -0.0425,      0.0796,      0.5859,      0.3906,      0.0082,     -0.2021,     -2.2656,     -0.4590,      0.1973,     -0.1377],
        [     0.0117,     -0.3027,     -0.3574,     -0.4141,     -0.6719,      5.8438,      0.0114,     -0.1777,      2.0469,     -0.1211,      0.0552,     -0.0442,     -2.7188,     -0.5820,      0.0018,      0.5391],
        [     0.0231,     -0.2314,      0.4180,      0.1445,      0.1748,     -0.6562,     -0.1719,      0.1641,     -1.9375,     -0.3223,      0.0645,      0.4668,     -0.7500,     -3.1250,      0.1260,     -0.0315]]
name='residual layer 1'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3730,      0.0820,     -0.0205,      0.2539,      0.6523,     -0.6602,      0.0087,     -0.0430,      4.0625,      2.8906,      0.0439,     -0.6719,     -2.5469,     16.3750,     -0.6406,      1.4688],
        [     1.3516,     -1.6406,      0.6250,      0.0518,     -2.0469,     -1.9688,     -0.0378,      0.1650,      1.7031,     -1.2031,      0.0825,     -0.3770,     10.0000,      1.1719,      0.1963,     -0.5156],
        [     0.4824,      0.2520,      0.8555,      0.2500,      0.5078,     -0.4062,      0.0332,      0.0427,      0.3867,      1.3125,      0.1855,     -0.5547,      7.3750,    -22.5000,     -0.3867,      0.0781],
        [    -0.6094,      0.5547,      1.7266,      0.1328,     -0.2402,     -1.0625,      0.0386,      0.5703,      3.3906,      1.1328,      0.2480,     -0.7969,      9.7500,     10.7500,     -0.3418,      0.6602],
        [    -1.1094,     -0.0093,      2.1094,     -0.0859,      1.7734,     -1.3125,     -0.1338,     -0.5117,      2.3594,     -1.2656,      0.0713,     -0.2266,      6.5625,      8.9375,      0.1816,     -0.7344],
        [    -0.4258,      0.9727,      1.2656,      0.0654,      0.3008,      2.7812,     -0.0801,     -0.3867,      4.4688,      0.8750,      0.0317,     -0.1318,      8.3750,     -9.6875,      0.3594,     -0.5859],
        [    -0.4277,      1.5156,      0.5781,      0.0063,     -1.1719,     -2.3438,      0.1406,      0.0723,     -0.5547,      0.6484,      0.4531,     -0.0203,      5.1875,     -0.6602,      0.2539,     -0.9414],
        [     0.0038,     -0.1055,      0.9414,     -0.0039,     -0.9609,      2.0625,      0.1270,     -0.0244,      4.7500,      0.0869,      0.2773,     -0.5664,      4.4062,    -42.2500,      0.1758,     -0.0898],
        [    -0.8438,      1.1875,     -0.0176,     -0.2773,     -1.0078,      1.7578,      0.2754,      0.2129,      4.8125,     -0.8750,      0.6211,     -0.3027,      2.6250,    -14.8750,     -0.0273,      1.1172],
        [     0.0698,      0.1973,      0.5703,      0.2148,      0.4199,     -1.9844,     -0.0918,     -0.2441,     -3.0625,      0.2314,      0.3789,      0.3945,     -1.9766,      0.8125,      0.0071,     -0.0859]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1670,      0.0933,     -0.0771,      0.1602,      0.3125,     -0.4473,     -0.0200,      0.0184,      1.1250,      0.7500,     -0.1089,     -0.3223,     -0.2031,      3.6875,     -0.5273,      0.5078],
        [     0.7461,     -0.6211,      0.2256,     -0.0221,     -0.5703,     -0.4453,      0.0242,     -0.0023,      0.5938,     -0.1855,      0.1865,     -0.2031,      2.2500,      0.1895,      0.2773,     -0.8047],
        [     0.2852,      0.0194,      0.4805,      0.1494,      0.2754,     -0.4512,     -0.0369,     -0.0608,      0.0640,      0.3887,     -0.2178,     -0.0806,      1.8438,     -3.8438,      0.0415,     -0.1865],
        [    -0.1934,      0.2715,      0.7969,      0.4062,      0.0781,     -0.6484,      0.0415,      0.2471,      0.9688,      0.5547,     -0.1045,     -0.3301,      2.2500,      1.8359,      0.4258,      0.2734],
        [    -0.6523,     -0.4453,      0.6914,     -0.2061,      0.4473,     -0.8242,      0.0557,     -0.2617,     -0.0703,     -0.4570,      0.1299,     -0.2598,      1.6016,      2.3438,     -0.1709,      0.1270],
        [    -0.1279,      0.4219,      0.3770,      0.2051,     -0.1475,     -0.0608,      0.1436,     -0.5898,      0.0996,      0.4082,      0.0229,     -0.1196,      2.7344,     -2.8281,      0.2695,     -0.4102],
        [    -0.3066,      0.6211,      0.4297,     -0.0021,     -0.2832,      0.1484,      0.1172,      0.0420,     -0.6680,      0.0522,      0.2061,     -0.1504,      1.6562,      0.3984,     -0.0737,     -0.5391],
        [    -0.0747,     -0.2412,      0.2188,     -0.0933,     -0.1235,      0.9727,      0.0554,      0.0317,      1.3906,      0.1562,      0.2100,     -0.3867,      0.6484,     -9.9375,      0.2451,     -0.0908],
        [    -0.3887,      0.3379,     -0.1318,     -0.4121,     -0.5000,      1.4922,      0.1650,      0.0177,      1.5625,     -0.2832,      0.4336,     -0.1523,     -0.0248,     -3.1406,     -0.0146,      0.5781],
        [     0.0474,     -0.0143,      0.3809,      0.2354,      0.1924,     -0.5703,     -0.1660,     -0.0439,     -1.2422,     -0.0283,      0.3105,      0.4141,     -0.7891,     -0.5117,      0.0835,     -0.0447]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3340,      0.2275,     -0.2051,      0.2500,      0.9766,     -2.1250,     -0.0327,      0.0342,      4.6250,      2.4688,     -0.1582,     -0.6836,     -0.7188,     17.0000,     -0.8594,      1.3594],
        [     1.6953,     -1.7188,      0.6758,     -0.0391,     -2.0312,     -2.3906,      0.0447,     -0.0049,      2.7656,     -0.6914,      0.3066,     -0.4902,      9.0000,      0.9922,      0.5117,     -2.4375],
        [     0.6953,      0.0576,      1.5469,      0.2832,      1.0547,     -2.5938,     -0.0728,     -0.1367,      0.3184,      1.5469,     -0.3848,     -0.2070,      7.8750,    -21.3750,      0.0820,     -0.6094],
        [    -0.4297,      0.7344,      2.3438,      0.7031,      0.2715,     -3.4219,      0.0752,      0.5078,      4.4062,      2.0156,     -0.1680,     -0.7773,      8.8125,      9.3750,      0.7656,      0.8086],
        [    -1.5000,     -1.2344,      2.0938,     -0.3672,      1.6094,     -4.5000,      0.1035,     -0.5547,     -0.3281,     -1.7109,      0.2158,     -0.6328,      6.4688,     12.3125,     -0.3184,      0.3906],
        [    -0.2363,      0.9453,      0.9180,      0.2949,     -0.4258,     -0.2656,      0.2148,     -1.0078,      0.3750,      1.2344,      0.0305,     -0.2324,      8.8750,    -12.0000,      0.4023,     -1.0078],
        [    -0.5586,      1.3750,      1.0312,     -0.0030,     -0.8086,      0.6406,      0.1738,      0.0708,     -2.4688,      0.1562,      0.2715,     -0.2910,      5.3125,      1.6719,     -0.1094,     -1.3125],
        [    -0.1396,     -0.5508,      0.5430,     -0.1357,     -0.3633,      4.3125,      0.0845,      0.0552,      5.3438,      0.4766,      0.2852,     -0.7695,      2.1406,    -42.7500,      0.3730,     -0.2275],
        [    -0.8320,      0.8828,     -0.3750,     -0.6914,     -1.6797,      7.5938,      0.2871,      0.0352,      6.8750,     -0.9961,      0.6758,     -0.3477,     -0.0938,    -15.4375,     -0.0255,      1.6562],
        [     0.0928,     -0.0342,      0.9883,      0.3594,      0.5938,     -2.6406,     -0.2637,     -0.0801,     -5.0000,     -0.0908,      0.4434,      0.8594,     -2.7188,     -2.3125,      0.1328,     -0.1172]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0259,      0.1299,      0.0018,     -0.0791,     -0.2617,     -1.9844,     -0.0233,     -0.0243,      0.9180,     -0.2480,      0.0108,      0.1084,     -1.7266,      0.5430,     -0.1030,     -0.1611],
        [     0.0942,      0.4336,      0.2158,     -0.4160,     -0.9805,     -2.7188,     -0.0079,     -0.0020,     -0.0435,      0.2061,     -0.1992,      0.0138,     -0.5430,      0.1357,     -0.1816,     -0.2539],
        [     0.3633,      0.7383,      0.3711,     -0.1846,     -1.2891,     -2.1094,     -0.0039,     -0.0017,     -1.2109,     -0.5039,     -0.2354,      0.2207,      3.6406,      0.4961,     -0.0408,     -0.2305],
        [     0.0220,      0.1846,      0.0903,      0.2734,     -0.4238,     -1.9922,      0.2617,      0.2305,     -0.6094,     -0.2930,      0.3477,      0.1689,     -0.4902,     -0.3438,     -0.3223,      0.1011],
        [    -0.0649,      0.7969,      0.0889,      0.4922,     -1.3203,     -0.2207,      0.4512,      0.2324,     -1.8672,     -0.2715,      0.5352,      0.1367,      1.9062,     -0.8633,     -0.1533,     -0.0115],
        [     0.2139,      0.4648,      0.5625,      0.1484,     -1.3750,     -5.4062,      0.1602,      0.2734,     -0.8711,      0.0122,      0.2432,      0.2236,      6.5312,     -1.4297,     -0.3633,      0.3984],
        [     0.0874,      0.1572,      0.2090,      0.2246,     -0.2617,     -1.3750,      0.0520,     -0.2080,     -3.2969,      0.6484,     -0.1206,     -0.1011,      3.2031,      0.7148,      0.0996,     -0.5078],
        [    -0.0688,      0.1377,      0.0669,     -0.0796,     -0.1992,     -1.2812,      0.1572,     -0.0210,     -2.1719,      0.2070,      0.1455,     -0.1689,      3.2969,     -0.5352,     -0.2236,     -0.1001],
        [    -0.2715,     -0.0281,     -0.0991,      0.1338,     -0.4414,     -2.2500,      0.1543,      0.0014,     -2.0312,      0.3672,      0.0742,     -0.1245,     -0.1543,     -1.2891,     -0.4629,      0.8438],
        [     0.0942,      0.6914,      0.2383,     -0.1260,     -0.0703,     -1.0938,     -0.1826,     -0.2754,     -1.4375,     -0.7734,     -0.3438,     -0.0596,      0.3867,     -0.0894,     -0.3008,     -0.6328]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.2285,  0.1650, -0.1089,  0.1699,  0.2617, -0.4590, -0.0547,  0.0079,  0.7266,  0.6445, -0.1436, -0.3750, -0.2012,  0.8242, -0.7930,  0.4375],
        [ 1.1797, -0.5273,  0.4258, -0.4023, -0.9766, -0.5078,  0.0320, -0.0049,  0.3164, -0.1250,  0.0933, -0.2773,  0.6211,  0.0476,  0.2412, -0.8828],
        [ 0.6406,  0.2988,  0.8359,  0.0801, -0.0693, -0.4258, -0.0610, -0.0903, -0.0952,  0.2471, -0.4883,  0.0073,  0.7695, -0.8047,  0.0276, -0.2500],
        [-0.2715,  0.3828,  1.1719,  0.8750, -0.0503, -0.5430,  0.2969,  0.5352,  0.4492,  0.4512,  0.1572, -0.3574,  0.6133,  0.3828,  0.3281,  0.3008],
        [-0.9922, -0.1729,  1.0000,  0.1064,  0.0903, -0.4512,  0.4629, -0.2207, -0.2471, -0.4941,  0.6250, -0.2773,  0.5898,  0.4609, -0.3340,  0.1196],
        [-0.0165,  0.6406,  0.7852,  0.4336, -0.6445, -0.6211,  0.3594, -0.5781, -0.0640,  0.3574,  0.2617, -0.0056,  1.2500, -0.6250,  0.0317, -0.2207],
        [-0.3398,  0.6875,  0.6445,  0.2129, -0.3809, -0.0791,  0.2139, -0.1069, -0.7344,  0.2266,  0.1426, -0.2490,  0.6797,  0.1094, -0.0078, -0.6484],
        [-0.1641, -0.2021,  0.3477, -0.2266, -0.2178,  0.3574,  0.2490,  0.0291,  0.4395,  0.2100,  0.4434, -0.6484,  0.4727, -2.1562,  0.1299, -0.1270],
        [-0.7695,  0.3711, -0.2383, -0.5195, -0.7227,  0.5586,  0.4043,  0.0273,  0.5938, -0.1719,  0.6836, -0.2891, -0.0190, -0.7383, -0.3770,  0.8594],
        [ 0.1475,  0.3223,  0.6992,  0.2461,  0.2031, -0.4414, -0.4629, -0.3027, -0.8984, -0.2676,  0.1030,  0.5586, -0.2041, -0.1211, -0.1475, -0.2930]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3086,      0.3574,     -0.2031,      0.1709,      0.7148,     -4.1250,     -0.0562,      0.0099,      5.5312,      2.2188,     -0.1475,     -0.5742,     -2.4375,     17.5000,     -0.9609,      1.1953],
        [     1.7891,     -1.2812,      0.8906,     -0.4551,     -3.0156,     -5.1250,      0.0369,     -0.0069,      2.7188,     -0.4844,      0.1074,     -0.4766,      8.4375,      1.1250,      0.3301,     -2.6875],
        [     1.0625,      0.7969,      1.9219,      0.0986,     -0.2344,     -4.6875,     -0.0767,     -0.1387,     -0.8906,      1.0469,     -0.6211,      0.0137,     11.5000,    -20.8750,      0.0413,     -0.8398],
        [    -0.4082,      0.9180,      2.4375,      0.9766,     -0.1523,     -5.4062,      0.3359,      0.7383,      3.7969,      1.7188,      0.1797,     -0.6094,      8.3125,      9.0000,      0.4434,      0.9102],
        [    -1.5625,     -0.4375,      2.1875,      0.1250,      0.2891,     -4.7188,      0.5547,     -0.3223,     -2.1875,     -1.9844,      0.7500,     -0.4961,      8.3750,     11.4375,     -0.4727,      0.3789],
        [    -0.0225,      1.4062,      1.4844,      0.4434,     -1.7969,     -5.6875,      0.3750,     -0.7344,     -0.4961,      1.2500,      0.2734,     -0.0088,     15.3750,    -13.4375,      0.0391,     -0.6094],
        [    -0.4707,      1.5312,      1.2422,      0.2217,     -1.0703,     -0.7344,      0.2256,     -0.1367,     -5.7500,      0.8047,      0.1504,     -0.3926,      8.5000,      2.3906,     -0.0098,     -1.8203],
        [    -0.2090,     -0.4141,      0.6094,     -0.2148,     -0.5625,      3.0312,      0.2422,      0.0342,      3.1719,      0.6836,      0.4297,     -0.9375,      5.4375,    -43.2500,      0.1494,     -0.3281],
        [    -1.1016,      0.8555,     -0.4746,     -0.5586,     -2.1250,      5.3438,      0.4414,      0.0366,      4.8438,     -0.6289,      0.7500,     -0.4727,     -0.2480,    -16.7500,     -0.4883,      2.5000],
        [     0.1875,      0.6562,      1.2266,      0.2334,      0.5234,     -3.7344,     -0.4453,     -0.3555,     -6.4375,     -0.8633,      0.0996,      0.8008,     -2.3281,     -2.4062,     -0.1680,     -0.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.1650,     -0.4785,      0.2754,      0.4102,      0.3359,     -2.3125,     -0.1670,      0.1367,     -0.4277,      0.6758,     -0.0161,      0.0603,      3.1875,     -0.4844,      0.0957,      0.8477],
        [    -0.0118,      0.3301,      0.2832,      0.2188,     -2.0781,      1.0234,      0.0016,     -0.3242,     -0.7891,      1.4688,     -0.2578,      0.2656,      1.6719,     -0.0972,      0.6094,     -2.4375],
        [     0.4180,     -0.0266,      0.0081,     -0.0889,     -0.0493,      2.2031,      0.2871,     -0.2852,      0.0649,     -1.3750,     -0.2197,      0.2490,      2.6250,     -0.5273,      0.6094,      0.8633],
        [    -0.0022,      0.3867,     -0.0120,     -0.0889,      0.0728,      1.1484,      0.3203,     -0.5938,     -0.6562,      0.0618,     -0.3281,     -0.3750,      2.1406,      1.2266,      0.3086,     -0.5820],
        [    -0.5234,     -0.0508,      0.5312,     -0.3105,      0.6211,      4.6562,      0.1836,      0.6523,     -1.4062,      1.3750,     -0.5469,     -0.0635,      1.9688,      1.2734,     -0.2217,     -1.0469],
        [    -0.4648,      0.2402,     -0.1641,     -0.4863,     -0.7461,      0.4219,      0.2715,     -0.0747,     -1.0859,     -0.7383,     -0.3516,     -0.3242,     -2.6250,     -0.6484,      0.1504,      0.2334],
        [     0.0310,     -0.1631,      0.3379,     -0.2988,     -0.4141,      5.0312,      0.0713,     -0.1162,      0.4492,      0.1445,     -0.4648,      0.2930,      0.4102,     -0.0098,     -0.1914,     -1.0156],
        [     0.2148,     -0.6641,      0.0104,     -0.3047,      0.0410,      3.1406,      0.1807,      0.3906,      1.4062,      0.1914,      0.0938,     -0.1934,     -0.5469,     -0.8555,     -0.4707,     -0.2227],
        [     0.4043,     -0.7578,     -0.2598,     -0.2637,     -0.7773,      1.7500,      0.1230,      0.3770,     -0.4023,     -0.7148,     -0.2178,      0.1191,      1.5156,     -2.9531,     -0.8242,      3.0781],
        [     0.8555,     -0.5469,      0.0991,      0.2891,     -0.6328,      2.4531,     -0.1543,      0.1748,     -0.3867,     -2.0469,      0.0459,     -0.2256,     -1.1719,      1.5469,      0.0986,      1.3750]]
-------------------------
name='positions layer 2'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 2'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.1650,     -0.4785,      0.2754,      0.4102,      0.3359,     -2.3125,     -0.1670,      0.1367,     -0.4277,      0.6758,     -0.0161,      0.0603,      3.1875,     -0.4844,      0.0957,      0.8477],
        [    -0.0118,      0.3301,      0.2832,      0.2188,     -2.0781,      1.0234,      0.0016,     -0.3242,     -0.7891,      1.4688,     -0.2578,      0.2656,      1.6719,     -0.0972,      0.6094,     -2.4375],
        [     0.4180,     -0.0266,      0.0081,     -0.0889,     -0.0493,      2.2031,      0.2871,     -0.2852,      0.0649,     -1.3750,     -0.2197,      0.2490,      2.6250,     -0.5273,      0.6094,      0.8633],
        [    -0.0022,      0.3867,     -0.0120,     -0.0889,      0.0728,      1.1484,      0.3203,     -0.5938,     -0.6562,      0.0618,     -0.3281,     -0.3750,      2.1406,      1.2266,      0.3086,     -0.5820],
        [    -0.5234,     -0.0508,      0.5312,     -0.3105,      0.6211,      4.6562,      0.1836,      0.6523,     -1.4062,      1.3750,     -0.5469,     -0.0635,      1.9688,      1.2734,     -0.2217,     -1.0469],
        [    -0.4648,      0.2402,     -0.1641,     -0.4863,     -0.7461,      0.4219,      0.2715,     -0.0747,     -1.0859,     -0.7383,     -0.3516,     -0.3242,     -2.6250,     -0.6484,      0.1504,      0.2334],
        [     0.0310,     -0.1631,      0.3379,     -0.2988,     -0.4141,      5.0312,      0.0713,     -0.1162,      0.4492,      0.1445,     -0.4648,      0.2930,      0.4102,     -0.0098,     -0.1914,     -1.0156],
        [     0.2148,     -0.6641,      0.0104,     -0.3047,      0.0410,      3.1406,      0.1807,      0.3906,      1.4062,      0.1914,      0.0938,     -0.1934,     -0.5469,     -0.8555,     -0.4707,     -0.2227],
        [     0.4043,     -0.7578,     -0.2598,     -0.2637,     -0.7773,      1.7500,      0.1230,      0.3770,     -0.4023,     -0.7148,     -0.2178,      0.1191,      1.5156,     -2.9531,     -0.8242,      3.0781],
        [     0.8555,     -0.5469,      0.0991,      0.2891,     -0.6328,      2.4531,     -0.1543,      0.1748,     -0.3867,     -2.0469,      0.0459,     -0.2256,     -1.1719,      1.5469,      0.0986,      1.3750]]
name='residual layer 2'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3086,      0.3574,     -0.2031,      0.1709,      0.7148,     -4.1250,     -0.0562,      0.0099,      5.5312,      2.2188,     -0.1475,     -0.5742,     -2.4375,     17.5000,     -0.9609,      1.1953],
        [     1.7891,     -1.2812,      0.8906,     -0.4551,     -3.0156,     -5.1250,      0.0369,     -0.0069,      2.7188,     -0.4844,      0.1074,     -0.4766,      8.4375,      1.1250,      0.3301,     -2.6875],
        [     1.0625,      0.7969,      1.9219,      0.0986,     -0.2344,     -4.6875,     -0.0767,     -0.1387,     -0.8906,      1.0469,     -0.6211,      0.0137,     11.5000,    -20.8750,      0.0413,     -0.8398],
        [    -0.4082,      0.9180,      2.4375,      0.9766,     -0.1523,     -5.4062,      0.3359,      0.7383,      3.7969,      1.7188,      0.1797,     -0.6094,      8.3125,      9.0000,      0.4434,      0.9102],
        [    -1.5625,     -0.4375,      2.1875,      0.1250,      0.2891,     -4.7188,      0.5547,     -0.3223,     -2.1875,     -1.9844,      0.7500,     -0.4961,      8.3750,     11.4375,     -0.4727,      0.3789],
        [    -0.0225,      1.4062,      1.4844,      0.4434,     -1.7969,     -5.6875,      0.3750,     -0.7344,     -0.4961,      1.2500,      0.2734,     -0.0088,     15.3750,    -13.4375,      0.0391,     -0.6094],
        [    -0.4707,      1.5312,      1.2422,      0.2217,     -1.0703,     -0.7344,      0.2256,     -0.1367,     -5.7500,      0.8047,      0.1504,     -0.3926,      8.5000,      2.3906,     -0.0098,     -1.8203],
        [    -0.2090,     -0.4141,      0.6094,     -0.2148,     -0.5625,      3.0312,      0.2422,      0.0342,      3.1719,      0.6836,      0.4297,     -0.9375,      5.4375,    -43.2500,      0.1494,     -0.3281],
        [    -1.1016,      0.8555,     -0.4746,     -0.5586,     -2.1250,      5.3438,      0.4414,      0.0366,      4.8438,     -0.6289,      0.7500,     -0.4727,     -0.2480,    -16.7500,     -0.4883,      2.5000],
        [     0.1875,      0.6562,      1.2266,      0.2334,      0.5234,     -3.7344,     -0.4453,     -0.3555,     -6.4375,     -0.8633,      0.0996,      0.8008,     -2.3281,     -2.4062,     -0.1680,     -0.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0708,     -0.0457,      0.0300,      0.3242,      0.3008,     -1.2109,     -0.1357,      0.0723,      1.0781,      0.7891,     -0.0898,     -0.2256,      0.1543,      3.1719,     -0.4160,      0.5625],
        [     0.6328,     -0.2578,      0.3496,     -0.0947,     -1.0469,     -0.5547,      0.0168,     -0.1177,      0.2910,      0.1934,     -0.0593,     -0.0664,      1.4844,      0.1377,      0.3262,     -1.0156],
        [     0.5273,      0.2100,      0.5820,      0.0039,     -0.0588,     -0.3379,      0.0923,     -0.1514,     -0.1250,     -0.0645,     -0.3340,      0.0835,      2.0938,     -2.8906,      0.2266,      0.0047],
        [    -0.1445,      0.3516,      0.7188,      0.3516,     -0.0162,     -0.5703,      0.2832,      0.0508,      0.4707,      0.3438,     -0.0581,     -0.3086,      1.5312,      1.3594,      0.2578,      0.0645],
        [    -0.6992,     -0.1250,      0.7656,     -0.0698,      0.1777,     -0.0080,      0.3047,      0.1108,     -0.5117,     -0.1128,      0.0757,     -0.1670,      1.4375,      1.6094,     -0.2266,     -0.1250],
        [    -0.2119,      0.5469,      0.4805,     -0.0210,     -0.6406,     -0.8672,      0.3457,     -0.3496,     -0.2910,      0.1221,     -0.0376,     -0.1289,      2.2812,     -2.3125,      0.0801,     -0.0908],
        [    -0.1924,      0.4570,      0.5820,     -0.0381,     -0.3750,      0.7109,      0.1592,     -0.1104,     -0.9844,      0.2285,     -0.1523,     -0.0386,      1.6094,      0.3926,     -0.0854,     -0.6914],
        [     0.0025,     -0.3457,      0.2178,     -0.2451,     -0.1270,      0.9844,      0.2178,      0.1777,      0.8125,      0.2021,      0.2432,     -0.4180,      0.8477,     -6.9688,     -0.1309,     -0.1289],
        [    -0.2578,      0.0277,     -0.2285,     -0.3418,     -0.6211,      0.9922,      0.2559,      0.1523,      0.6992,     -0.2734,      0.2178,     -0.1157,      0.1943,     -2.7500,     -0.4727,      1.1484],
        [     0.4043,      0.0325,      0.4336,      0.2285,     -0.0247,     -0.1885,     -0.2871,     -0.0703,     -1.1328,     -0.6211,      0.0625,      0.1982,     -0.5625,     -0.1260,     -0.0262,      0.1357]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1436,     -0.1211,      0.0723,      0.5820,      1.0469,     -6.4375,     -0.2227,      0.1465,      5.0938,      2.8906,     -0.1641,     -0.5156,      0.7500,     17.0000,     -0.8672,      2.0469],
        [     1.7734,     -0.9531,      1.1719,     -0.2363,     -5.0938,     -4.0938,      0.0386,     -0.3320,      1.9297,      0.9844,     -0.1504,     -0.2109,     10.1250,      1.0312,      0.9375,     -5.1250],
        [     1.4844,      0.7695,      1.9297,      0.0098,     -0.2832,     -2.4844,      0.2109,     -0.4238,     -0.8242,     -0.3281,     -0.8398,      0.2617,     14.1250,    -21.3750,      0.6523,      0.0234],
        [    -0.4102,      1.3047,      2.4219,      0.8867,     -0.0796,     -4.2500,      0.6562,      0.1445,      3.1406,      1.7812,     -0.1484,     -0.9844,     10.4375,     10.2500,      0.7500,      0.3281],
        [    -2.0938,     -0.4883,      2.7188,     -0.1855,      0.9102,     -0.0625,      0.7383,      0.3301,     -3.5938,     -0.6094,      0.2031,     -0.5586,     10.3750,     12.6875,     -0.6953,     -0.6680],
        [    -0.4883,      1.6484,      1.3203,     -0.0430,     -2.5469,     -5.2500,      0.6484,     -0.8086,     -1.5781,      0.5117,     -0.0781,     -0.3320,     12.7500,    -14.0625,      0.1895,     -0.3750],
        [    -0.4395,      1.3672,      1.5781,     -0.0771,     -1.4844,      4.3125,      0.2969,     -0.2539,     -5.3125,      0.9492,     -0.3145,     -0.0996,      8.9375,      2.3750,     -0.2012,     -2.8438],
        [     0.0059,     -1.0781,      0.6211,     -0.5195,     -0.5234,      6.1875,      0.4219,      0.4258,      4.5625,      0.8750,      0.5234,     -1.1328,      4.8750,    -44.0000,     -0.3203,     -0.5508],
        [    -0.6953,      0.0977,     -0.7344,     -0.8203,     -2.9062,      7.0938,      0.5625,      0.4141,      4.4375,     -1.3438,      0.5312,     -0.3535,      1.2656,    -19.7500,     -1.3125,      5.5625],
        [     1.0469,      0.1094,      1.3281,      0.5234,     -0.1094,     -1.2812,     -0.6016,     -0.1807,     -6.8125,     -2.9062,      0.1455,      0.5742,     -3.5000,     -0.8594,     -0.0693,      0.6250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.1001,      0.6484,      0.0245,      0.4746,      0.6016,     -0.4824,     -0.2539,      0.1143,      0.4727,      0.2314,      0.3496,      0.0942,      0.7422,     -0.4277,      0.0894,      0.3027],
        [     0.2812,      0.8008,     -0.3926,     -0.2168,      1.3359,     -2.3750,     -0.5547,      0.6523,     -0.5664,     -0.0947,     -0.1943,      0.0718,      7.0000,      0.4512,     -0.2227,      0.9141],
        [     0.2910,      0.9609,     -0.3457,     -0.0625,      1.6641,      0.4902,     -0.6016,      0.6328,      0.2227,     -1.1562,     -0.8047,     -0.2676,      5.2500,      0.6250,      0.3789,     -0.6016],
        [     0.3906,      0.5859,     -0.1670,      0.0679,      1.3984,      0.0376,      0.0378,      0.5195,     -0.3477,     -0.4648,     -0.4688,     -0.1021,      3.3594,      0.1128,      0.6289,     -1.1562],
        [     0.0137,      0.6133,      0.4180,      0.2461,     -0.2178,     -0.5469,     -0.1826,      0.3867,     -0.7852,      1.1328,     -0.5312,     -0.0039,      3.4219,     -0.2197,      0.1035,     -1.1562],
        [    -0.7070,      0.3652,      0.0469,     -0.0332,     -1.1875,     -1.9062,     -0.2021,      0.0537,     -1.5000,      1.1172,     -0.6719,      0.2305,      5.1562,      0.3867,      0.3652,     -1.3516],
        [    -0.4355,      0.3965,     -0.2793,      0.0001,     -0.8906,      0.9375,     -0.0767,      0.2676,      0.1201,      1.0156,     -0.5469,      0.1797,      3.2031,      0.2021,      0.5508,     -0.7930],
        [    -0.7852,      0.0068,      0.5117,      0.0356,     -1.1250,     -2.2812,      0.0282,     -0.0815,     -1.0234,      1.6250,     -1.4141,      0.3301,      4.7188,      0.1621,      0.6523,     -0.3457],
        [    -0.6133,      0.1504,      0.0615,     -0.3359,     -0.1436,     -1.0781,      0.1260,      0.1338,     -0.0294,      0.2617,     -1.3125,      0.1582,      2.6719,     -0.0518,      0.3750,     -1.5938],
        [     0.0593,      0.4746,     -0.1699,     -0.3809,      0.3262,      0.3809,     -0.0282,     -0.0938,     -0.4941,     -0.0820,     -0.9961,     -0.0918,      4.1250,     -0.0771,      0.2324,     -0.3906]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.0298,  0.2285,  0.0540,  0.8242,  0.5391, -0.6406, -0.4043,  0.1689,  0.6133,  0.8164,  0.1357, -0.2559,  0.0879,  0.8750, -0.4746,  0.6641],
        [ 1.0391, -0.0488,  0.3203, -0.2617, -0.9141, -0.4453, -0.3242,  0.1533,  0.1108,  0.1729, -0.1865, -0.0625,  0.7461,  0.0581,  0.3242, -0.8789],
        [ 0.9414,  0.5820,  0.6836, -0.0320,  0.3496, -0.1436, -0.2559,  0.1045, -0.0513, -0.3008, -0.9336, -0.0027,  0.8867, -0.8477,  0.4883, -0.1260],
        [-0.0118,  0.7305,  1.1172,  0.6641,  0.3848, -0.3477,  0.5234,  0.3828,  0.2734,  0.3047, -0.4023, -0.5859,  0.7227,  0.4863,  0.7500, -0.2080],
        [-1.2344,  0.0471,  1.5156,  0.0408,  0.1963, -0.0491,  0.4082,  0.4023, -0.4160,  0.1182, -0.2070, -0.2949,  0.7031,  0.5703, -0.3125, -0.4434],
        [-0.8320,  0.8945,  0.7734, -0.0605, -1.2500, -0.6797,  0.3867, -0.5000, -0.3438,  0.4336, -0.5625, -0.0630,  1.0781, -0.7383,  0.3457, -0.4961],
        [-0.6914,  0.8828,  0.8359, -0.0693, -0.8945,  0.5625,  0.2158,  0.0102, -0.6602,  0.5938, -0.7305,  0.0562,  0.8281,  0.1572,  0.2471, -1.1875],
        [-0.4531, -0.3945,  0.5352, -0.3203, -0.4590,  0.3086,  0.3262,  0.1895,  0.3301,  0.5547, -0.5547, -0.4160,  0.4824, -1.9766,  0.1719, -0.2148],
        [-0.7539,  0.0903, -0.3164, -0.7617, -0.8398,  0.4707,  0.4922,  0.2988,  0.4082, -0.2393, -0.4824, -0.1001,  0.1963, -0.8828, -0.4824,  0.9414],
        [ 0.7148,  0.2383,  0.6094,  0.1050,  0.0669, -0.0791, -0.5078, -0.1680, -0.7578, -0.7383, -0.5898,  0.2754,  0.0349, -0.0466,  0.0942,  0.0623]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0435,      0.5273,      0.0967,      1.0547,      1.6484,     -6.9062,     -0.4766,      0.2617,      5.5625,      3.1250,      0.1855,     -0.4219,      1.4922,     16.6250,     -0.7773,      2.3438],
        [     2.0625,     -0.1523,      0.7812,     -0.4531,     -3.7500,     -6.4688,     -0.5156,      0.3203,      1.3594,      0.8906,     -0.3438,     -0.1387,     17.1250,      1.4844,      0.7148,     -4.2188],
        [     1.7734,      1.7344,      1.5859,     -0.0527,      1.3828,     -1.9922,     -0.3906,      0.2090,     -0.6016,     -1.4844,     -1.6406,     -0.0059,     19.3750,    -20.7500,      1.0312,     -0.5781],
        [    -0.0195,      1.8906,      2.2500,      0.9531,      1.3203,     -4.2188,      0.6953,      0.6641,      2.7969,      1.3125,     -0.6172,     -1.0859,     13.8125,     10.3750,      1.3750,     -0.8281],
        [    -2.0781,      0.1250,      3.1406,      0.0605,      0.6914,     -0.6094,      0.5547,      0.7188,     -4.3750,      0.5234,     -0.3281,     -0.5625,     13.8125,     12.4375,     -0.5938,     -1.8281],
        [    -1.1953,      2.0156,      1.3672,     -0.0762,     -3.7344,     -7.1562,      0.4453,     -0.7539,     -3.0781,      1.6250,     -0.7500,     -0.1016,     17.8750,    -13.6875,      0.5547,     -1.7266],
        [    -0.8750,      1.7656,      1.2969,     -0.0771,     -2.3750,      5.2500,      0.2207,      0.0137,     -5.1875,      1.9688,     -0.8594,      0.0801,     12.1250,      2.5781,      0.3496,     -3.6406],
        [    -0.7812,     -1.0703,      1.1328,     -0.4844,     -1.6484,      3.9062,      0.4492,      0.3438,      3.5312,      2.5000,     -0.8906,     -0.8047,      9.6250,    -43.7500,      0.3320,     -0.8984],
        [    -1.3125,      0.2480,     -0.6719,     -1.1562,     -3.0469,      6.0000,      0.6875,      0.5469,      4.4062,     -1.0781,     -0.7812,     -0.1953,      3.9375,    -19.7500,     -0.9375,      3.9688],
        [     1.1094,      0.5859,      1.1562,      0.1426,      0.2168,     -0.8984,     -0.6289,     -0.2734,     -7.3125,     -2.9844,     -0.8516,      0.4824,      0.6250,     -0.9375,      0.1631,      0.2344]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1367,     -0.8086,      0.1187,      0.7695,     -0.6445,     -2.3281,      0.0383,      0.1377,      0.0430,      0.0374,      0.0012,      0.0088,      0.8164,      1.2578,      0.3477,      0.1924],
        [     0.8867,     -1.3906,      0.6016,      0.8086,     -0.8086,     -0.9062,      0.3379,      0.7656,     -3.0156,      0.5820,     -0.5859,      0.0040,      4.6562,     -0.0452,      0.4766,      1.9141],
        [     0.2520,      0.1895,      0.2100,      0.3613,      0.5312,     -3.3750,      0.0869,     -0.1523,      2.2500,      0.1016,      0.1128,      0.4277,      1.0234,     -0.0952,      0.7812,     -1.4062],
        [     0.1494,     -0.4375,      0.8672,      0.1187,      1.2344,      2.3750,      0.0383,     -0.0659,     -2.5000,      0.1250,     -0.0986,     -0.2266,     -0.5352,     -0.5117,      0.3652,      0.6523],
        [     0.1621,      0.5977,      0.8555,     -0.1089,      1.5938,      3.3750,     -0.2236,     -0.0674,      1.1484,     -5.2188,      0.8242,     -0.7656,     -1.9531,      1.3047,     -1.4609,      2.6094],
        [    -0.7422,     -1.3281,      0.2500,      0.0894,     -0.4883,      2.6875,     -0.3809,     -0.0305,     -0.0649,      0.8789,      0.9883,     -0.1064,     -0.1108,      0.3926,     -1.5938,     -0.9727],
        [     0.0248,     -1.5000,      0.4258,      0.2139,     -1.0312,      2.5469,      0.0315,     -0.5898,      1.4141,      1.0938,     -0.0042,      0.2217,     -2.5000,      0.1621,     -0.8047,     -1.3516],
        [    -0.1846,      0.0728,      0.1797,     -0.5781,      0.8828,     -0.3867,      0.0608,      0.1758,      0.0991,     -3.2031,      0.1436,     -0.2832,     -1.1406,      0.0148,     -0.9805,      0.7422],
        [     0.3242,      0.0347,      0.2754,      0.3281,      1.0391,      3.2656,      0.5352,      0.8359,      1.7422,     -1.7344,     -0.7109,      0.0952,     -3.2969,     -0.8047,     -0.7031,      1.1562],
        [     0.2451,     -0.2715,     -0.3398,      0.2734,     -1.4141,     -1.0938,      0.2480,     -0.0067,      1.2969,     -0.0317,     -0.2715,      0.5391,      0.5664,      1.2109,      0.1621,     -1.9062]]
-------------------------
name='positions layer 3'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 3'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1367,     -0.8086,      0.1187,      0.7695,     -0.6445,     -2.3281,      0.0383,      0.1377,      0.0430,      0.0374,      0.0012,      0.0088,      0.8164,      1.2578,      0.3477,      0.1924],
        [     0.8867,     -1.3906,      0.6016,      0.8086,     -0.8086,     -0.9062,      0.3379,      0.7656,     -3.0156,      0.5820,     -0.5859,      0.0040,      4.6562,     -0.0452,      0.4766,      1.9141],
        [     0.2520,      0.1895,      0.2100,      0.3613,      0.5312,     -3.3750,      0.0869,     -0.1523,      2.2500,      0.1016,      0.1128,      0.4277,      1.0234,     -0.0952,      0.7812,     -1.4062],
        [     0.1494,     -0.4375,      0.8672,      0.1187,      1.2344,      2.3750,      0.0383,     -0.0659,     -2.5000,      0.1250,     -0.0986,     -0.2266,     -0.5352,     -0.5117,      0.3652,      0.6523],
        [     0.1621,      0.5977,      0.8555,     -0.1089,      1.5938,      3.3750,     -0.2236,     -0.0674,      1.1484,     -5.2188,      0.8242,     -0.7656,     -1.9531,      1.3047,     -1.4609,      2.6094],
        [    -0.7422,     -1.3281,      0.2500,      0.0894,     -0.4883,      2.6875,     -0.3809,     -0.0305,     -0.0649,      0.8789,      0.9883,     -0.1064,     -0.1108,      0.3926,     -1.5938,     -0.9727],
        [     0.0248,     -1.5000,      0.4258,      0.2139,     -1.0312,      2.5469,      0.0315,     -0.5898,      1.4141,      1.0938,     -0.0042,      0.2217,     -2.5000,      0.1621,     -0.8047,     -1.3516],
        [    -0.1846,      0.0728,      0.1797,     -0.5781,      0.8828,     -0.3867,      0.0608,      0.1758,      0.0991,     -3.2031,      0.1436,     -0.2832,     -1.1406,      0.0148,     -0.9805,      0.7422],
        [     0.3242,      0.0347,      0.2754,      0.3281,      1.0391,      3.2656,      0.5352,      0.8359,      1.7422,     -1.7344,     -0.7109,      0.0952,     -3.2969,     -0.8047,     -0.7031,      1.1562],
        [     0.2451,     -0.2715,     -0.3398,      0.2734,     -1.4141,     -1.0938,      0.2480,     -0.0067,      1.2969,     -0.0317,     -0.2715,      0.5391,      0.5664,      1.2109,      0.1621,     -1.9062]]
name='residual layer 3'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0435,      0.5273,      0.0967,      1.0547,      1.6484,     -6.9062,     -0.4766,      0.2617,      5.5625,      3.1250,      0.1855,     -0.4219,      1.4922,     16.6250,     -0.7773,      2.3438],
        [     2.0625,     -0.1523,      0.7812,     -0.4531,     -3.7500,     -6.4688,     -0.5156,      0.3203,      1.3594,      0.8906,     -0.3438,     -0.1387,     17.1250,      1.4844,      0.7148,     -4.2188],
        [     1.7734,      1.7344,      1.5859,     -0.0527,      1.3828,     -1.9922,     -0.3906,      0.2090,     -0.6016,     -1.4844,     -1.6406,     -0.0059,     19.3750,    -20.7500,      1.0312,     -0.5781],
        [    -0.0195,      1.8906,      2.2500,      0.9531,      1.3203,     -4.2188,      0.6953,      0.6641,      2.7969,      1.3125,     -0.6172,     -1.0859,     13.8125,     10.3750,      1.3750,     -0.8281],
        [    -2.0781,      0.1250,      3.1406,      0.0605,      0.6914,     -0.6094,      0.5547,      0.7188,     -4.3750,      0.5234,     -0.3281,     -0.5625,     13.8125,     12.4375,     -0.5938,     -1.8281],
        [    -1.1953,      2.0156,      1.3672,     -0.0762,     -3.7344,     -7.1562,      0.4453,     -0.7539,     -3.0781,      1.6250,     -0.7500,     -0.1016,     17.8750,    -13.6875,      0.5547,     -1.7266],
        [    -0.8750,      1.7656,      1.2969,     -0.0771,     -2.3750,      5.2500,      0.2207,      0.0137,     -5.1875,      1.9688,     -0.8594,      0.0801,     12.1250,      2.5781,      0.3496,     -3.6406],
        [    -0.7812,     -1.0703,      1.1328,     -0.4844,     -1.6484,      3.9062,      0.4492,      0.3438,      3.5312,      2.5000,     -0.8906,     -0.8047,      9.6250,    -43.7500,      0.3320,     -0.8984],
        [    -1.3125,      0.2480,     -0.6719,     -1.1562,     -3.0469,      6.0000,      0.6875,      0.5469,      4.4062,     -1.0781,     -0.7812,     -0.1953,      3.9375,    -19.7500,     -0.9375,      3.9688],
        [     1.1094,      0.5859,      1.1562,      0.1426,      0.2168,     -0.8984,     -0.6289,     -0.2734,     -7.3125,     -2.9844,     -0.8516,      0.4824,      0.6250,     -0.9375,      0.1631,      0.2344]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0752,     -0.0952,      0.0693,      0.7500,      0.2441,     -1.4766,     -0.1973,      0.1602,      1.2109,      0.7422,      0.0786,     -0.1504,      0.5625,      4.0000,     -0.1611,      0.6055],
        [     0.9023,     -0.3828,      0.3242,      0.1064,     -0.8086,     -0.8594,     -0.0586,      0.3184,     -0.2617,      0.2539,     -0.2852,     -0.0359,      3.8750,      0.2344,      0.3242,     -0.4043],
        [     0.7109,      0.5430,      0.4844,      0.1050,      0.3887,     -0.7148,     -0.1138,      0.0188,      0.2969,     -0.2715,     -0.5352,      0.1279,      4.1562,     -3.8750,      0.5664,     -0.3965],
        [     0.0454,      0.4102,      0.8359,      0.3652,      0.5195,     -0.2451,      0.2734,      0.1992,      0.0535,      0.2812,     -0.2500,     -0.3965,      2.6875,      1.8359,      0.5430,     -0.0352],
        [    -0.5898,      0.1807,      0.9492,     -0.0145,      0.4102,      0.3242,      0.1094,      0.1914,     -0.5117,     -0.8125,      0.1543,     -0.3555,      2.1250,      2.2500,     -0.5664,      0.1377],
        [    -0.7188,      0.2051,      0.4609,      0.0048,     -0.9062,     -0.6289,      0.0255,     -0.2754,     -0.5977,      0.5195,      0.0884,     -0.0664,      3.8125,     -2.6094,     -0.3438,     -0.5703],
        [    -0.3535,      0.0889,      0.5469,      0.0554,     -0.8203,      1.2266,      0.1118,     -0.2266,     -0.8086,      0.7109,     -0.3594,      0.1084,      2.3125,      0.6055,     -0.1680,     -1.1875],
        [    -0.3594,     -0.2988,      0.3730,     -0.3848,     -0.1650,      0.4961,      0.2021,      0.1836,      0.6953,     -0.1465,     -0.2773,     -0.3516,      1.8203,     -8.6250,     -0.2148,     -0.0332],
        [    -0.3301,      0.0762,     -0.1016,     -0.2695,     -0.3887,      1.1797,      0.4375,      0.4395,      1.0625,     -0.5273,     -0.5000,     -0.0289,      0.1240,     -3.6562,     -0.4902,      0.9766],
        [     0.5117,      0.0962,      0.2373,      0.1533,     -0.2637,     -0.2871,     -0.1543,     -0.1006,     -1.1719,     -0.6406,     -0.4277,      0.3359,      0.2617,      0.0549,      0.1099,     -0.3613]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1797,     -0.2812,      0.2148,      1.8281,      1.0000,     -9.2500,     -0.4375,      0.3984,      5.5938,      3.1562,      0.1865,     -0.4141,      2.3125,     17.8750,     -0.4297,      2.5312],
        [     2.9531,     -1.5469,      1.3828,      0.3555,     -4.5625,     -7.3750,     -0.1777,      1.0859,     -1.6562,      1.4688,     -0.9297,     -0.1348,     21.7500,      1.4375,      1.1875,     -2.3125],
        [     2.0312,      1.9219,      1.7969,      0.3086,      1.9141,     -5.3750,     -0.3047,      0.0566,      1.6484,     -1.3828,     -1.5312,      0.4219,     20.3750,    -20.8750,      1.8125,     -1.9844],
        [     0.1299,      1.4531,      3.1250,      1.0703,      2.5625,     -1.8438,      0.7344,      0.5977,      0.2969,      1.4375,     -0.7148,     -1.3125,     13.2500,      9.8750,      1.7422,     -0.1758],
        [    -1.9141,      0.7227,      4.0000,     -0.0483,      2.2812,      2.7656,      0.3320,      0.6523,     -3.2188,     -4.6875,      0.4961,     -1.3281,     11.8750,     13.7500,     -2.0625,      0.7812],
        [    -1.9375,      0.6875,      1.6172,      0.0132,     -4.2188,     -4.4688,      0.0645,     -0.7852,     -3.1406,      2.5000,      0.2383,     -0.2080,     17.7500,    -13.3125,     -1.0391,     -2.7031],
        [    -0.8516,      0.2656,      1.7188,      0.1367,     -3.4062,      7.8125,      0.2520,     -0.5781,     -3.7812,      3.0625,     -0.8633,      0.3008,      9.6250,      2.7344,     -0.4551,     -5.0000],
        [    -0.9648,     -0.9961,      1.3125,     -1.0625,     -0.7656,      3.5156,      0.5117,      0.5195,      3.6250,     -0.7031,     -0.7461,     -1.0859,      8.5000,    -43.7500,     -0.6484,     -0.1562],
        [    -0.9883,      0.2832,     -0.3965,     -0.8281,     -2.0000,      9.2500,      1.2188,      1.3828,      6.1562,     -2.8125,     -1.4922,     -0.1001,      0.6406,    -20.5000,     -1.6406,      5.1250],
        [     1.3516,      0.3145,      0.8164,      0.4160,     -1.1953,     -1.9922,     -0.3809,     -0.2793,     -6.0000,     -3.0156,     -1.1250,      1.0234,      1.1875,      0.2734,      0.3242,     -1.6719]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0181,     -0.3770,      0.1240,      0.1709,      1.3203,     -3.4219,     -0.2383,      0.1177,      1.6016,      0.1885,      0.3379,      0.2422,     -1.9297,      0.6523,     -0.1904,      1.2969],
        [    -0.1992,      0.6484,      0.4004,     -0.6758,      0.7148,     -1.2500,     -0.0972,     -0.6406,     -2.9375,     -2.2188,     -0.1992,     -0.2715,      1.1953,      0.8711,     -0.0378,      1.1797],
        [     0.4434,     -0.0129,      0.4219,     -0.0160,     -1.4297,      1.7344,      0.1030,     -0.1826,     -0.9023,     -1.0469,      0.2207,     -0.2871,      1.3594,      1.6172,     -0.4746,      1.7109],
        [     0.2314,     -0.0383,      0.0830,     -0.3438,     -0.5195,     -3.3594,     -0.0262,     -0.1768,     -0.4141,     -0.6797,      0.2520,      0.1367,      1.1094,      0.5898,     -0.1270,      0.8945],
        [     0.1631,      1.5469,     -0.3535,      0.3379,     -0.2715,     -0.4668,      0.2500,     -0.6406,     -0.8555,     -2.3750,      0.1113,      1.0234,     -0.3477,     -0.1465,     -0.0933,      2.6562],
        [     0.7930,      0.8008,      0.4961,     -0.0093,     -1.5000,     -0.5859,     -0.1885,     -0.1045,      0.9844,     -3.5312,      1.0391,      0.1836,      0.6367,      0.5469,      0.6758,      1.3750],
        [     0.4395,      0.3047,      0.4629,      0.4902,     -1.8984,     -2.7188,     -0.1963,      0.1001,     -0.2324,     -2.0625,      0.4922,     -0.4414,      1.2578,      0.8477,      0.4414,     -1.3984],
        [     0.2852,      0.0173,      0.0894,      0.6758,     -1.0000,      1.5391,     -0.0859,      0.0199,     -0.8398,     -0.4688,     -0.1328,      0.1069,     -0.3320,     -2.0000,      0.0535,     -0.3887],
        [     0.0771,      0.7539,      0.1992,      0.5469,     -2.1875,      1.2266,      0.1543,      0.0898,     -0.9492,     -1.6094,      0.0247,     -0.0586,      2.2031,     -1.3672,      0.1055,      0.7930],
        [     0.1436,     -0.1172,      0.2334,     -0.3281,     -0.7461,     -1.6016,     -0.2432,      0.0002,      0.6602,     -0.7031,      0.1167,     -0.3125,      0.2988,      0.6484,      0.6562,     -0.8789]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0898,     -0.2100,      0.1611,      1.0859,      0.5625,     -0.8828,     -0.4355,      0.2432,      0.7500,      0.6719,      0.2734,     -0.0806,      0.0205,      0.8750,     -0.2734,      0.7188],
        [     1.3438,     -0.2520,      0.7383,     -0.1533,     -0.8125,     -0.5273,     -0.1553,      0.1836,     -0.4180,     -0.1318,     -0.5156,     -0.1670,      1.0703,      0.0957,      0.4434,     -0.1855],
        [     1.3438,      0.5977,      1.0312,      0.1562,      0.1147,     -0.2480,     -0.1270,     -0.0583,      0.0757,     -0.4766,     -0.6719,      0.0620,      1.1328,     -0.8906,      0.5781,     -0.0503],
        [     0.1904,      0.4297,      1.4453,      0.3750,      0.4668,     -0.3418,      0.4336,      0.1885,     -0.0115,      0.1445,     -0.2305,     -0.5234,      0.7266,      0.4707,      0.6758,      0.1279],
        [    -0.7617,      0.5703,      1.3516,      0.1235,      0.3809,      0.1260,      0.2949,      0.0043,     -0.3320,     -1.1094,      0.2480,     -0.1123,      0.4824,      0.5039,     -0.7461,      0.5078],
        [    -0.6719,      0.5039,      1.0625,      0.0022,     -1.4609,     -0.3730,     -0.0850,     -0.4434,     -0.2363,     -0.2188,      0.7070,     -0.0121,      1.0391,     -0.6367,     -0.1689,     -0.2637],
        [    -0.2490,      0.1982,      1.1250,      0.3730,     -1.3984,      0.3848,      0.0388,     -0.2461,     -0.4531,      0.2178,     -0.2100,     -0.0718,      0.6328,      0.1836,     -0.0065,     -1.3047],
        [    -0.3965,     -0.3281,      0.6953,     -0.2207,     -0.4453,      0.3691,      0.2871,      0.2656,      0.3027,     -0.2461,     -0.4805,     -0.4805,      0.4570,     -2.2812,     -0.2754,     -0.1069],
        [    -0.4785,      0.3125,     -0.0884,     -0.1445,     -0.9531,      0.6875,      0.8359,      0.6562,      0.5078,     -0.8359,     -0.7227,     -0.0703,      0.1426,     -0.9766,     -0.6367,      1.0469],
        [     0.9375,      0.0713,      0.5625,      0.0542,     -0.5312,     -0.2832,     -0.4531,     -0.1484,     -0.6250,     -0.8398,     -0.5938,      0.3770,      0.0894,      0.0493,      0.4883,     -0.5391]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1621,     -0.6562,      0.3398,      2.0000,      2.3125,    -12.6875,     -0.6758,      0.5156,      7.1875,      3.3438,      0.5234,     -0.1719,      0.3828,     18.5000,     -0.6211,      3.8281],
        [     2.7500,     -0.8984,      1.7812,     -0.3203,     -3.8438,     -8.6250,     -0.2754,      0.4453,     -4.5938,     -0.7500,     -1.1250,     -0.4062,     23.0000,      2.3125,      1.1484,     -1.1328],
        [     2.4688,      1.9062,      2.2188,      0.2930,      0.4844,     -3.6406,     -0.2012,     -0.1260,      0.7461,     -2.4375,     -1.3125,      0.1348,     21.7500,    -19.2500,      1.3359,     -0.2734],
        [     0.3613,      1.4141,      3.2031,      0.7266,      2.0469,     -5.1875,      0.7070,      0.4219,     -0.1172,      0.7578,     -0.4629,     -1.1719,     14.3750,     10.4375,      1.6172,      0.7188],
        [    -1.7500,      2.2656,      3.6406,      0.2891,      2.0156,      2.2969,      0.5820,      0.0117,     -4.0625,     -7.0625,      0.6094,     -0.3047,     11.5000,     13.6250,     -2.1562,      3.4375],
        [    -1.1406,      1.4844,      2.1094,      0.0038,     -5.7188,     -5.0625,     -0.1240,     -0.8906,     -2.1562,     -1.0312,      1.2812,     -0.0244,     18.3750,    -12.7500,     -0.3633,     -1.3281],
        [    -0.4121,      0.5703,      2.1875,      0.6250,     -5.3125,      5.0938,      0.0557,     -0.4785,     -4.0000,      1.0000,     -0.3711,     -0.1406,     10.8750,      3.5781,     -0.0137,     -6.4062],
        [    -0.6797,     -0.9805,      1.3984,     -0.3867,     -1.7656,      5.0625,      0.4258,      0.5391,      2.7812,     -1.1719,     -0.8789,     -0.9805,      8.1875,    -45.7500,     -0.5938,     -0.5469],
        [    -0.9102,      1.0391,     -0.1973,     -0.2812,     -4.1875,     10.5000,      1.3750,      1.4688,      5.2188,     -4.4375,     -1.4688,     -0.1582,      2.8438,    -21.8750,     -1.5312,      5.9062],
        [     1.4922,      0.1973,      1.0469,      0.0879,     -1.9375,     -3.5938,     -0.6250,     -0.2793,     -5.3438,     -3.7188,     -1.0078,      0.7109,      1.4844,      0.9219,      0.9805,     -2.5469]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3184,      0.1641,      0.2139,     -0.3320,     -0.1445,     -2.9219,     -0.0552,      0.1973,     -1.4766,      0.0654,     -0.4160,     -0.0698,     -0.6758,     -0.5273,     -0.2676,      0.9922],
        [     1.0781,     -1.5078,     -0.3086,     -0.1157,     -2.5469,     -3.7500,      0.8242,      1.3438,     -1.0234,     -1.0156,     -0.3242,      1.8594,      0.3379,     -0.1162,      0.6836,     -4.0000],
        [     0.5508,     -1.0859,      0.5430,      0.0439,     -1.8906,     -1.4609,      0.8750,      0.8281,     -0.7070,     -0.6445,     -0.2119,      0.1670,     -1.9141,     -0.5000,     -0.3516,     -2.5156],
        [    -1.1484,     -0.3027,      0.1025,     -0.6602,     -1.0391,     -1.7031,      0.2969,      1.1406,     -1.1250,      0.8320,      1.1016,     -0.2988,      0.2021,     -0.6758,      0.8281,     -3.2656],
        [    -0.6445,     -0.9336,     -0.0747,     -0.5195,     -5.7188,     -2.4375,     -0.4395,      1.5703,      1.0234,     -0.1357,      0.6367,      0.0928,      1.0625,     -0.3770,      0.7891,      2.4844],
        [    -0.3066,     -1.0156,     -0.0962,     -0.3848,     -5.5625,     -4.2500,     -0.3965,      0.5781,     -0.0123,      0.7422,      0.4980,     -0.1875,     -2.9844,     -0.1475,      0.8164,     -3.7031],
        [    -0.2070,      0.2246,      0.4902,      0.1094,     -4.4375,     -0.5234,     -0.5273,      0.7188,      2.1406,     -0.6953,      0.0894,      0.7266,     -0.7891,      1.0625,      0.4590,     -2.4531],
        [    -0.0889,     -0.7617,      0.0557,      0.2070,     -3.5156,     -3.8906,     -0.3125,      0.2734,     -1.1562,     -1.1797,     -0.5586,     -0.3008,     -0.7422,      0.2617,      0.1162,      1.6094],
        [    -0.7461,     -0.7773,      0.4727,      0.3418,      0.4473,     -2.0625,     -0.0167,      1.1953,      1.3984,      0.4434,      0.0256,     -1.0703,     -0.8125,     -0.0811,     -0.4199,      1.0312],
        [     0.7148,     -0.0006,      0.4473,      0.5664,      0.0108,     -1.0156,      0.5508,     -0.4902,     -0.5820,     -0.3164,     -0.9062,     -0.4355,     -3.5625,     -0.3320,     -0.0219,     -0.6758]]
-------------------------
name='positions layer 4'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 4'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3184,      0.1641,      0.2139,     -0.3320,     -0.1445,     -2.9219,     -0.0552,      0.1973,     -1.4766,      0.0654,     -0.4160,     -0.0698,     -0.6758,     -0.5273,     -0.2676,      0.9922],
        [     1.0781,     -1.5078,     -0.3086,     -0.1157,     -2.5469,     -3.7500,      0.8242,      1.3438,     -1.0234,     -1.0156,     -0.3242,      1.8594,      0.3379,     -0.1162,      0.6836,     -4.0000],
        [     0.5508,     -1.0859,      0.5430,      0.0439,     -1.8906,     -1.4609,      0.8750,      0.8281,     -0.7070,     -0.6445,     -0.2119,      0.1670,     -1.9141,     -0.5000,     -0.3516,     -2.5156],
        [    -1.1484,     -0.3027,      0.1025,     -0.6602,     -1.0391,     -1.7031,      0.2969,      1.1406,     -1.1250,      0.8320,      1.1016,     -0.2988,      0.2021,     -0.6758,      0.8281,     -3.2656],
        [    -0.6445,     -0.9336,     -0.0747,     -0.5195,     -5.7188,     -2.4375,     -0.4395,      1.5703,      1.0234,     -0.1357,      0.6367,      0.0928,      1.0625,     -0.3770,      0.7891,      2.4844],
        [    -0.3066,     -1.0156,     -0.0962,     -0.3848,     -5.5625,     -4.2500,     -0.3965,      0.5781,     -0.0123,      0.7422,      0.4980,     -0.1875,     -2.9844,     -0.1475,      0.8164,     -3.7031],
        [    -0.2070,      0.2246,      0.4902,      0.1094,     -4.4375,     -0.5234,     -0.5273,      0.7188,      2.1406,     -0.6953,      0.0894,      0.7266,     -0.7891,      1.0625,      0.4590,     -2.4531],
        [    -0.0889,     -0.7617,      0.0557,      0.2070,     -3.5156,     -3.8906,     -0.3125,      0.2734,     -1.1562,     -1.1797,     -0.5586,     -0.3008,     -0.7422,      0.2617,      0.1162,      1.6094],
        [    -0.7461,     -0.7773,      0.4727,      0.3418,      0.4473,     -2.0625,     -0.0167,      1.1953,      1.3984,      0.4434,      0.0256,     -1.0703,     -0.8125,     -0.0811,     -0.4199,      1.0312],
        [     0.7148,     -0.0006,      0.4473,      0.5664,      0.0108,     -1.0156,      0.5508,     -0.4902,     -0.5820,     -0.3164,     -0.9062,     -0.4355,     -3.5625,     -0.3320,     -0.0219,     -0.6758]]
name='residual layer 4'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1621,     -0.6562,      0.3398,      2.0000,      2.3125,    -12.6875,     -0.6758,      0.5156,      7.1875,      3.3438,      0.5234,     -0.1719,      0.3828,     18.5000,     -0.6211,      3.8281],
        [     2.7500,     -0.8984,      1.7812,     -0.3203,     -3.8438,     -8.6250,     -0.2754,      0.4453,     -4.5938,     -0.7500,     -1.1250,     -0.4062,     23.0000,      2.3125,      1.1484,     -1.1328],
        [     2.4688,      1.9062,      2.2188,      0.2930,      0.4844,     -3.6406,     -0.2012,     -0.1260,      0.7461,     -2.4375,     -1.3125,      0.1348,     21.7500,    -19.2500,      1.3359,     -0.2734],
        [     0.3613,      1.4141,      3.2031,      0.7266,      2.0469,     -5.1875,      0.7070,      0.4219,     -0.1172,      0.7578,     -0.4629,     -1.1719,     14.3750,     10.4375,      1.6172,      0.7188],
        [    -1.7500,      2.2656,      3.6406,      0.2891,      2.0156,      2.2969,      0.5820,      0.0117,     -4.0625,     -7.0625,      0.6094,     -0.3047,     11.5000,     13.6250,     -2.1562,      3.4375],
        [    -1.1406,      1.4844,      2.1094,      0.0038,     -5.7188,     -5.0625,     -0.1240,     -0.8906,     -2.1562,     -1.0312,      1.2812,     -0.0244,     18.3750,    -12.7500,     -0.3633,     -1.3281],
        [    -0.4121,      0.5703,      2.1875,      0.6250,     -5.3125,      5.0938,      0.0557,     -0.4785,     -4.0000,      1.0000,     -0.3711,     -0.1406,     10.8750,      3.5781,     -0.0137,     -6.4062],
        [    -0.6797,     -0.9805,      1.3984,     -0.3867,     -1.7656,      5.0625,      0.4258,      0.5391,      2.7812,     -1.1719,     -0.8789,     -0.9805,      8.1875,    -45.7500,     -0.5938,     -0.5469],
        [    -0.9102,      1.0391,     -0.1973,     -0.2812,     -4.1875,     10.5000,      1.3750,      1.4688,      5.2188,     -4.4375,     -1.4688,     -0.1582,      2.8438,    -21.8750,     -1.5312,      5.9062],
        [     1.4922,      0.1973,      1.0469,      0.0879,     -1.9375,     -3.5938,     -0.6250,     -0.2793,     -5.3438,     -3.7188,     -1.0078,      0.7109,      1.4844,      0.9219,      0.9805,     -2.5469]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1533,     -0.1240,      0.1494,      0.4844,      0.4043,     -2.2500,     -0.2520,      0.1924,      0.9297,      0.6367,      0.0315,     -0.0664,     -0.0488,      2.2969,     -0.2285,      0.8320],
        [     0.8750,     -0.4336,      0.2871,     -0.0908,     -0.8594,     -1.2891,      0.1357,      0.3477,     -0.6602,     -0.2373,     -0.3066,      0.2871,      2.7969,      0.2012,      0.3398,     -0.6367],
        [     0.7773,      0.1660,      0.6016,      0.0791,     -0.2119,     -0.5977,      0.1875,      0.1523,      0.0052,     -0.4668,     -0.3613,      0.0669,      2.6719,     -2.0312,      0.2051,     -0.3887],
        [    -0.1943,      0.2168,      0.6953,      0.0150,      0.1465,     -0.7773,      0.2695,      0.3281,     -0.1582,      0.2305,      0.1455,     -0.3145,      1.8828,      0.9727,      0.4902,     -0.3418],
        [    -0.5039,      0.2217,      0.6367,     -0.0439,     -0.4570,     -0.0134,      0.0325,      0.2812,     -0.3281,     -0.8867,      0.2412,     -0.0386,      1.3828,      1.1172,     -0.2334,      0.6758],
        [    -0.3809,      0.0967,      0.4473,     -0.0908,     -1.7344,     -1.1094,     -0.1475,     -0.0698,     -0.2910,     -0.0444,      0.4297,     -0.0481,      2.1094,     -1.3594,      0.0967,     -0.7148],
        [    -0.1875,      0.1895,      0.6836,      0.2012,     -1.7266,      0.6289,     -0.1543,      0.0613,     -0.2871,      0.0540,     -0.0781,      0.1533,      1.5938,      0.5625,      0.1094,     -1.4453],
        [    -0.2266,     -0.4043,      0.3633,     -0.0481,     -0.9141,      0.1572,      0.0361,      0.2031,      0.2461,     -0.4082,     -0.3887,     -0.3262,      1.1484,     -5.3750,     -0.1138,      0.1699],
        [    -0.4062,      0.0505,      0.0571,      0.0135,     -0.5391,      0.9414,      0.3594,      0.5547,      0.8320,     -0.5781,     -0.3262,     -0.2598,      0.2598,     -2.1562,     -0.3887,      0.9180],
        [     0.5977,      0.0422,      0.3438,      0.1621,     -0.3086,     -0.5703,     -0.0219,     -0.1777,     -0.8242,     -0.6445,     -0.4805,      0.0645,     -0.2949,      0.0645,      0.2109,     -0.4746]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.4805,     -0.4922,      0.5547,      1.6719,      2.1719,    -15.6250,     -0.7305,      0.7109,      5.7188,      3.4062,      0.1074,     -0.2422,     -0.2930,     18.0000,     -0.8906,      4.8125],
        [     3.8281,     -2.4062,      1.4688,     -0.4355,     -6.3750,    -12.3750,      0.5469,      1.7891,     -5.6250,     -1.7656,     -1.4531,      1.4531,     23.3750,      2.2031,      1.8281,     -5.1250],
        [     3.0156,      0.8203,      2.7656,      0.3359,     -1.4062,     -5.0938,      0.6719,      0.7031,      0.0391,     -3.0781,     -1.5234,      0.3008,     19.8750,    -19.7500,      0.9844,     -2.7812],
        [    -0.7891,      1.1094,      3.3125,      0.0664,      1.0078,     -6.8750,      1.0000,      1.5625,     -1.2422,      1.5938,      0.6406,     -1.4688,     14.5625,      9.7500,      2.4375,     -2.5469],
        [    -2.3906,      1.3281,      3.5625,     -0.2305,     -3.7031,     -0.1406,      0.1426,      1.5781,     -3.0312,     -7.1875,      1.2500,     -0.2119,     12.5625,     13.2500,     -1.3672,      5.9375],
        [    -1.4453,      0.4688,      2.0156,     -0.3809,    -11.2500,     -9.3125,     -0.5195,     -0.3125,     -2.1719,     -0.2891,      1.7812,     -0.2119,     15.3750,    -12.8750,      0.4531,     -5.0312],
        [    -0.6172,      0.7969,      2.6719,      0.7344,     -9.7500,      4.5625,     -0.4727,      0.2402,     -1.8594,      0.3047,     -0.2812,      0.5859,     10.0625,      4.6250,      0.4453,     -8.8750],
        [    -0.7695,     -1.7422,      1.4531,     -0.1797,     -5.2812,      1.1719,      0.1133,      0.8125,      1.6250,     -2.3438,     -1.4375,     -1.2812,      7.4375,    -45.5000,     -0.4766,      1.0625],
        [    -1.6562,      0.2617,      0.2754,      0.0605,     -3.7344,      8.4375,      1.3594,      2.6562,      6.6250,     -4.0000,     -1.4453,     -1.2266,      2.0312,    -22.0000,     -1.9531,      6.9375],
        [     2.2031,      0.1963,      1.4922,      0.6562,     -1.9297,     -4.6250,     -0.0742,     -0.7695,     -5.9375,     -4.0312,     -1.9141,      0.2754,     -2.0781,      0.5898,      0.9570,     -3.2188]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.1846,  0.5430, -0.7734,  0.1631,  1.8047,  0.4121, -0.1182,  0.4414, -0.2061,  0.0562,  0.1235,  0.2871, -7.1250,  0.2910,  0.3379, -0.0410],
        [-0.0138, -0.0703, -0.1128,  0.3457,  1.8125, -0.9844,  0.0400,  0.3965, -0.8281, -0.1650,  0.3652, -0.5078,  1.1719, -0.1147,  0.7422,  0.8008],
        [ 0.5469, -0.5117, -0.2812,  0.2988,  3.0469, -3.0469, -0.0559,  0.0762, -2.3438, -0.5156,  0.5508,  0.2441,  2.3281,  0.4023,  0.9414, -1.0234],
        [-0.1816, -0.1836, -0.2891,  0.3770,  2.0469, -7.9375,  0.2227,  0.0508, -2.2969, -0.3691,  0.6289,  0.4980,  0.3867,  0.1562,  0.6523, -1.2656],
        [-0.6211, -0.1963, -0.1797, -0.2520,  2.2656,  0.4746,  0.5586,  0.7109,  0.6797, -0.5469,  0.0732,  0.1885, -0.8281, -0.2871,  0.4883,  0.7422],
        [-0.3730, -0.1445, -0.5039, -0.3398,  0.4570, -6.5000,  0.2637,  0.3906, -3.0000,  1.1016, -0.1230,  0.3066,  0.0850,  0.3184,  0.8711,  0.9297],
        [-0.1846, -0.6758, -0.7031, -0.7227, -0.3887,  1.0156, -0.1177,  0.5273, -2.2500, -1.1484, -0.3457,  0.4375, -1.4375, -0.1001,  0.5664,  1.2578],
        [-0.2832,  0.1592, -0.1182, -0.4395,  0.2432, -1.0312,  0.1162,  0.2539,  0.5039, -1.3828,  0.2275, -0.2812, -0.5664, -0.2715, -0.1465,  1.7266],
        [-0.5273,  0.1162, -0.4473, -0.7930, -0.5977,  0.8164, -0.2051,  0.7578, -0.7305, -2.2969,  0.7344,  0.3633, -1.6172, -0.3691,  0.0099,  1.5469],
        [ 0.1826, -0.0339, -0.0396, -0.0981, -1.2109, -2.8438,  0.1670, -0.2988,  0.2080,  0.7266, -0.1270, -0.1270, -2.3125,  0.4043,  0.3086, -0.1328]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.2891,  0.0128, -0.0825,  0.7305,  0.6992, -0.8008, -0.4199,  0.4023,  0.4492,  0.5391,  0.0874,  0.0162, -0.3223,  1.0000, -0.1797,  0.6523],
        [ 1.3828, -0.5234,  0.4297, -0.0299, -0.6719, -0.5898,  0.2432,  0.6406, -0.4395, -0.2520, -0.3457,  0.2871,  0.8945,  0.0952,  0.6953, -0.4941],
        [ 1.4609,  0.0732,  0.8867,  0.2383,  0.2734, -0.4062,  0.2871,  0.2578, -0.1768, -0.5273, -0.3477,  0.1855,  0.9062, -0.9961,  0.5859, -0.4902],
        [-0.3750,  0.2080,  1.0234,  0.1572,  0.4805, -0.6992,  0.5391,  0.5039, -0.2578,  0.1709,  0.4297, -0.3125,  0.5781,  0.4824,  0.8906, -0.4668],
        [-1.0625,  0.2324,  1.0469, -0.1562, -0.2061,  0.0143,  0.2852,  0.6562, -0.1562, -0.9844,  0.4102, -0.0069,  0.4160,  0.5781, -0.2314,  0.7461],
        [-0.7461,  0.0771,  0.5391, -0.2715, -1.7969, -0.7891, -0.1201,  0.0259, -0.3984,  0.1196,  0.5938,  0.0325,  0.6328, -0.6484,  0.4062, -0.5273],
        [-0.3789,  0.0330,  0.8086,  0.0051, -1.9375,  0.3203, -0.3184,  0.2910, -0.3633, -0.1426, -0.2578,  0.4004,  0.4062,  0.2676,  0.3535, -1.1250],
        [-0.4922, -0.4297,  0.5430, -0.2656, -0.9570,  0.0080,  0.1226,  0.4004,  0.1865, -0.6211, -0.4922, -0.6094,  0.3203, -2.6719, -0.2158,  0.4082],
        [-0.8555,  0.0859, -0.0588, -0.2617, -0.6875,  0.4414,  0.5156,  1.0781,  0.4336, -0.8828, -0.2422, -0.2812,  0.0162, -1.1016, -0.5703,  1.0469],
        [ 0.9883,  0.0388,  0.5234,  0.2119, -0.5273, -0.3750,  0.0439, -0.3555, -0.4434, -0.4902, -0.7383,  0.0513, -0.1816,  0.0518,  0.3887, -0.4375]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.6641,      0.0508,     -0.2188,      1.8359,      3.9688,    -15.1875,     -0.8477,      1.1562,      5.5000,      3.4688,      0.2305,      0.0449,     -7.4062,     18.2500,     -0.5547,      4.7812],
        [     3.8125,     -2.4688,      1.3594,     -0.0898,     -4.5625,    -13.3750,      0.5859,      2.1875,     -6.4375,     -1.9297,     -1.0859,      0.9453,     24.5000,      2.0938,      2.5625,     -4.3125],
        [     3.5625,      0.3086,      2.4844,      0.6328,      1.6406,     -8.1250,      0.6172,      0.7812,     -2.3125,     -3.5938,     -0.9727,      0.5469,     22.2500,    -19.3750,      1.9219,     -3.8125],
        [    -0.9688,      0.9258,      3.0312,      0.4434,      3.0625,    -14.8125,      1.2188,      1.6094,     -3.5312,      1.2266,      1.2656,     -0.9688,     14.9375,      9.8750,      3.0938,     -3.8125],
        [    -3.0156,      1.1328,      3.3750,     -0.4824,     -1.4375,      0.3340,      0.7031,      2.2812,     -2.3438,     -7.7500,      1.3203,     -0.0234,     11.7500,     12.9375,     -0.8789,      6.6875],
        [    -1.8203,      0.3242,      1.5156,     -0.7188,    -10.8125,    -15.8125,     -0.2559,      0.0781,     -5.1875,      0.8125,      1.6562,      0.0947,     15.4375,    -12.5625,      1.3281,     -4.0938],
        [    -0.8008,      0.1211,      1.9688,      0.0117,    -10.1250,      5.5625,     -0.5898,      0.7656,     -4.1250,     -0.8438,     -0.6250,      1.0234,      8.6250,      4.5312,      1.0156,     -7.6250],
        [    -1.0547,     -1.5859,      1.3359,     -0.6172,     -5.0312,      0.1406,      0.2295,      1.0625,      2.1250,     -3.7188,     -1.2109,     -1.5625,      6.8750,    -45.7500,     -0.6250,      2.7812],
        [    -2.1875,      0.3789,     -0.1719,     -0.7344,     -4.3438,      9.2500,      1.1562,      3.4062,      5.9062,     -6.3125,     -0.7109,     -0.8633,      0.4141,    -22.3750,     -1.9453,      8.5000],
        [     2.3906,      0.1621,      1.4531,      0.5586,     -3.1406,     -7.4688,      0.0928,     -1.0703,     -5.7188,     -3.3125,     -2.0469,      0.1484,     -4.3750,      0.9922,      1.2656,     -3.3438]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.3574, -1.6328,  0.6055,  3.0469,  1.0547, -4.2188, -0.3340,  0.2236,  0.2324,  0.2090,  1.0547, -0.9062,  0.8945,  0.4316, -1.1953, -3.8750],
        [ 1.3203,  1.8203,  1.2188, -0.0757, -2.4375, -6.9062, -0.2754, -0.7930,  2.6875, -5.6562, -0.1196,  1.2812, 14.0625,  0.0630,  0.2275, -9.9375],
        [ 1.0859,  0.5664,  0.8906, -0.2578, -4.0938, -8.2500, -1.4844, -1.3438, -1.1953,  0.2422, -1.6875,  0.8828,  5.5625,  0.3730, -2.6406, -5.1250],
        [-0.1099,  0.3945,  0.8398,  1.5781, -1.6016, -2.0625, -1.8672,  0.7617,  3.2656, -1.0547, -0.1914,  0.5547,  1.7578,  0.1816, -1.4531,  0.9141],
        [-0.5469,  0.2695,  0.6250,  0.0806, -4.2812, -7.1875, -0.1641,  2.8438,  4.5000, -0.4980,  0.8984, -0.7148, -2.6562, -0.0703, -2.2031, -1.9141],
        [-2.3125, -1.5703,  0.4512,  0.4141, -1.2812,  0.3633, -0.1709, -0.3887, -2.6875, -0.6094,  1.6250, -0.5352,  2.9531, -1.0859, -1.3828, -5.0938],
        [-0.1855, -1.2266, -0.9922, -0.1113, -4.4062,  0.8516, -0.4219, -1.3203,  1.9688,  0.9805,  0.8516,  0.0574, -0.2852, -0.2168, -1.9219, -6.3750],
        [-0.7500, -1.3984, -0.3926,  0.1465, -2.1250, -3.3906, -0.2285, -0.7188, -0.0464, -0.3965,  0.5312, -0.1338, -0.1348, -0.4531, -1.3281, -1.4531],
        [ 0.5273, -2.4531, -1.0859, -0.2832, -1.1172, -5.8125, -1.1562, -0.4629,  1.9141,  0.9648, -0.1377, -1.2578,  0.9297, -0.7695, -3.9062, -1.6172],
        [ 0.4062, -0.3184, -0.4492,  0.6367,  0.1660, -8.6875, -0.6680,  0.3691,  2.9062,  1.1797, -0.8359, -0.6094,  5.1875,  0.6719,  0.3594,  1.1719]]
-------------------------
name='positions layer 5'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 5'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.3574, -1.6328,  0.6055,  3.0469,  1.0547, -4.2188, -0.3340,  0.2236,  0.2324,  0.2090,  1.0547, -0.9062,  0.8945,  0.4316, -1.1953, -3.8750],
        [ 1.3203,  1.8203,  1.2188, -0.0757, -2.4375, -6.9062, -0.2754, -0.7930,  2.6875, -5.6562, -0.1196,  1.2812, 14.0625,  0.0630,  0.2275, -9.9375],
        [ 1.0859,  0.5664,  0.8906, -0.2578, -4.0938, -8.2500, -1.4844, -1.3438, -1.1953,  0.2422, -1.6875,  0.8828,  5.5625,  0.3730, -2.6406, -5.1250],
        [-0.1099,  0.3945,  0.8398,  1.5781, -1.6016, -2.0625, -1.8672,  0.7617,  3.2656, -1.0547, -0.1914,  0.5547,  1.7578,  0.1816, -1.4531,  0.9141],
        [-0.5469,  0.2695,  0.6250,  0.0806, -4.2812, -7.1875, -0.1641,  2.8438,  4.5000, -0.4980,  0.8984, -0.7148, -2.6562, -0.0703, -2.2031, -1.9141],
        [-2.3125, -1.5703,  0.4512,  0.4141, -1.2812,  0.3633, -0.1709, -0.3887, -2.6875, -0.6094,  1.6250, -0.5352,  2.9531, -1.0859, -1.3828, -5.0938],
        [-0.1855, -1.2266, -0.9922, -0.1113, -4.4062,  0.8516, -0.4219, -1.3203,  1.9688,  0.9805,  0.8516,  0.0574, -0.2852, -0.2168, -1.9219, -6.3750],
        [-0.7500, -1.3984, -0.3926,  0.1465, -2.1250, -3.3906, -0.2285, -0.7188, -0.0464, -0.3965,  0.5312, -0.1338, -0.1348, -0.4531, -1.3281, -1.4531],
        [ 0.5273, -2.4531, -1.0859, -0.2832, -1.1172, -5.8125, -1.1562, -0.4629,  1.9141,  0.9648, -0.1377, -1.2578,  0.9297, -0.7695, -3.9062, -1.6172],
        [ 0.4062, -0.3184, -0.4492,  0.6367,  0.1660, -8.6875, -0.6680,  0.3691,  2.9062,  1.1797, -0.8359, -0.6094,  5.1875,  0.6719,  0.3594,  1.1719]]
name='residual layer 5'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.6641,      0.0508,     -0.2188,      1.8359,      3.9688,    -15.1875,     -0.8477,      1.1562,      5.5000,      3.4688,      0.2305,      0.0449,     -7.4062,     18.2500,     -0.5547,      4.7812],
        [     3.8125,     -2.4688,      1.3594,     -0.0898,     -4.5625,    -13.3750,      0.5859,      2.1875,     -6.4375,     -1.9297,     -1.0859,      0.9453,     24.5000,      2.0938,      2.5625,     -4.3125],
        [     3.5625,      0.3086,      2.4844,      0.6328,      1.6406,     -8.1250,      0.6172,      0.7812,     -2.3125,     -3.5938,     -0.9727,      0.5469,     22.2500,    -19.3750,      1.9219,     -3.8125],
        [    -0.9688,      0.9258,      3.0312,      0.4434,      3.0625,    -14.8125,      1.2188,      1.6094,     -3.5312,      1.2266,      1.2656,     -0.9688,     14.9375,      9.8750,      3.0938,     -3.8125],
        [    -3.0156,      1.1328,      3.3750,     -0.4824,     -1.4375,      0.3340,      0.7031,      2.2812,     -2.3438,     -7.7500,      1.3203,     -0.0234,     11.7500,     12.9375,     -0.8789,      6.6875],
        [    -1.8203,      0.3242,      1.5156,     -0.7188,    -10.8125,    -15.8125,     -0.2559,      0.0781,     -5.1875,      0.8125,      1.6562,      0.0947,     15.4375,    -12.5625,      1.3281,     -4.0938],
        [    -0.8008,      0.1211,      1.9688,      0.0117,    -10.1250,      5.5625,     -0.5898,      0.7656,     -4.1250,     -0.8438,     -0.6250,      1.0234,      8.6250,      4.5312,      1.0156,     -7.6250],
        [    -1.0547,     -1.5859,      1.3359,     -0.6172,     -5.0312,      0.1406,      0.2295,      1.0625,      2.1250,     -3.7188,     -1.2109,     -1.5625,      6.8750,    -45.7500,     -0.6250,      2.7812],
        [    -2.1875,      0.3789,     -0.1719,     -0.7344,     -4.3438,      9.2500,      1.1562,      3.4062,      5.9062,     -6.3125,     -0.7109,     -0.8633,      0.4141,    -22.3750,     -1.9453,      8.5000],
        [     2.3906,      0.1621,      1.4531,      0.5586,     -3.1406,     -7.4688,      0.0928,     -1.0703,     -5.7188,     -3.3125,     -2.0469,      0.1484,     -4.3750,      0.9922,      1.2656,     -3.3438]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.3027,     -0.3613,      0.0972,      1.2812,      0.8281,     -2.5781,     -0.3691,      0.3457,      1.0234,      0.5938,      0.3613,     -0.2246,     -1.2578,      3.1719,     -0.4121,      0.1406],
        [     0.9766,     -0.0947,      0.4141,     -0.0278,     -0.7422,     -1.7188,      0.0623,      0.2236,     -0.4277,     -0.7812,     -0.2158,      0.3711,      4.7500,      0.2344,      0.4199,     -1.4219],
        [     0.9766,      0.1416,      0.6016,      0.0698,     -0.2891,     -1.5469,     -0.1934,     -0.1001,     -0.4434,     -0.3848,     -0.5273,      0.2637,      3.8125,     -2.2812,     -0.1201,     -0.9883],
        [    -0.2500,      0.2344,      0.7578,      0.4121,      0.1875,     -1.7500,     -0.1582,      0.4629,     -0.0369,      0.0216,      0.2334,     -0.0840,      2.5000,      1.3281,      0.3008,     -0.3516],
        [    -0.6953,      0.2109,      0.6641,     -0.0693,     -0.6250,     -0.5977,      0.1113,      0.8477,      0.2539,     -0.8789,      0.4102,     -0.1270,      1.1562,      1.4375,     -0.4785,      0.4902],
        [    -0.8906,     -0.2070,      0.3574,     -0.0576,     -1.4453,     -1.4844,     -0.0972,     -0.0562,     -1.0156,      0.0238,      0.6641,     -0.0830,      2.5781,     -1.6797,     -0.0093,     -1.0312],
        [    -0.2461,     -0.2109,      0.2051,     -0.0220,     -2.0156,      0.7148,     -0.2656,     -0.1162,     -0.3223,      0.0186,      0.0532,      0.2363,      1.3438,      0.6172,     -0.1787,     -1.8281],
        [    -0.4785,     -0.6094,      0.2119,     -0.1104,     -1.0547,     -0.3867,      0.0003,      0.0771,      0.3320,     -0.5938,     -0.1709,     -0.3965,      1.1562,     -7.0000,     -0.4121,      0.1846],
        [    -0.3770,     -0.3613,     -0.2422,     -0.2041,     -0.6875,      0.3496,      0.0000,      0.5625,      1.0625,     -0.6602,     -0.1816,     -0.4219,      0.1982,     -3.0000,     -1.0547,      0.8164],
        [     0.6445,     -0.0277,      0.1953,      0.2422,     -0.3809,     -1.6562,     -0.1396,     -0.1357,     -0.3887,     -0.2676,     -0.6250,     -0.0933,      0.1211,      0.2188,      0.2969,     -0.2617]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     1.0234,     -1.5781,      0.3867,      4.8750,      5.0312,    -19.3750,     -1.1797,      1.3828,      5.7188,      3.6719,      1.2812,     -0.8594,     -6.5000,     18.6250,     -1.7500,      0.9062],
        [     5.1250,     -0.6484,      2.5781,     -0.1660,     -7.0000,    -20.2500,      0.3105,      1.3906,     -3.7500,     -7.5938,     -1.2031,      2.2188,     38.5000,      2.1562,      2.7969,    -14.2500],
        [     4.6562,      0.8750,      3.3750,      0.3750,     -2.4531,    -16.3750,     -0.8672,     -0.5625,     -3.5000,     -3.3438,     -2.6562,      1.4297,     27.7500,    -19.0000,     -0.7188,     -8.9375],
        [    -1.0781,      1.3203,      3.8750,      2.0156,      1.4609,    -16.8750,     -0.6484,      2.3750,     -0.2656,      0.1719,      1.0781,     -0.4141,     16.7500,     10.0625,      1.6406,     -2.9062],
        [    -3.5625,      1.4062,      4.0000,     -0.4023,     -5.7188,     -6.8438,      0.5391,      5.1250,      2.1562,     -8.2500,      2.2188,     -0.7383,      9.1250,     12.8750,     -3.0781,      4.7812],
        [    -4.1250,     -1.2500,      1.9688,     -0.3047,    -12.1250,    -15.4375,     -0.4258,     -0.3105,     -7.8750,      0.2031,      3.2812,     -0.4414,     18.3750,    -13.6250,     -0.0547,     -9.1875],
        [    -0.9844,     -1.1094,      0.9766,     -0.0996,    -14.5000,      6.4062,     -1.0156,     -0.5547,     -2.1562,      0.1367,      0.2266,      1.0781,      8.3125,      4.3125,     -0.9062,    -14.0000],
        [    -1.8047,     -2.9844,      0.9453,     -0.4707,     -7.1562,     -3.2500,      0.0010,      0.3438,      2.0781,     -4.1250,     -0.6797,     -1.6953,      6.7500,    -46.2500,     -1.9531,      1.3281],
        [    -1.6562,     -2.0781,     -1.2578,     -1.0156,     -5.4688,      3.4375,      0.0000,      2.9375,      7.8125,     -5.3438,     -0.8477,     -2.1250,      1.3438,    -23.1250,     -5.8438,      6.8750],
        [     2.7969,     -0.1562,      1.0000,      1.1953,     -2.9688,    -16.1250,     -0.5742,     -0.7031,     -2.8125,     -2.1250,     -2.8750,     -0.4609,      0.8125,      1.6641,      1.6250,     -2.1719]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.5742,      1.7266,     -0.1826,     -0.7500,      0.2393,     -3.0312,      0.1230,      0.5586,     -0.7930,     -1.4609,     -0.2891,      0.5859,     -2.0625,      0.1143,     -1.1250,      1.2656],
        [    -1.4766,      0.7500,     -0.1055,     -1.7656,      0.9453,     -4.2812,      0.1582,     -0.1445,     -5.4062,     -1.3438,     -1.1953,     -0.4941,      4.3438,      0.8438,     -1.8750,      2.3438],
        [    -1.1797,      1.1016,      0.1924,     -0.9844,     -0.1216,     -7.6250,     -0.2080,     -1.1484,     -1.5000,     -2.2500,      1.1328,     -0.4727,      4.0312,     -0.0596,     -0.4414,      1.8047],
        [    -0.8906,      3.0625,     -0.8281,     -0.3125,      0.5664,     -2.8750,      0.2393,     -0.9062,     -1.8438,     -3.4219,      0.5898,      0.7383,     10.1875,     -0.5273,     -0.2334,      0.4258],
        [    -1.6797,      1.7812,     -1.3359,     -0.4551,     -1.4609,     -0.3926,      0.0193,      0.3066,     -0.9922,     -1.2422,      1.2812,      1.2266,      6.0312,      0.4023,      0.7070,      1.5938],
        [    -0.7461,      0.3203,     -1.4297,     -0.1045,     -1.8828,     -0.3633,     -0.0086,     -0.0874,      1.2734,     -4.2188,      0.3223,      0.3359,      1.0547,     -0.2207,      0.4199,      0.9414],
        [    -0.3047,      0.2734,     -1.2031,      1.1250,      0.4355,     -6.7500,     -0.0038,      0.0248,     -2.0781,     -1.6094,     -0.0189,     -0.6055,      0.7188,      0.2656,     -0.5586,      1.8672],
        [    -0.2852,      0.8984,     -0.0532,      0.4141,     -0.6328,     -5.2812,     -0.1465,      0.0322,      0.5391,     -0.3242,      0.2432,      0.4922,      4.4062,     -0.9766,     -0.2129,      1.4844],
        [    -1.0625,      0.1611,     -0.6992,      0.3672,     -1.2188,      0.0669,     -0.0474,     -0.2539,     -0.8281,      0.1963,      0.6484,      0.0454,      7.3750,     -0.5742,      0.6250,      1.4062],
        [    -0.1738,      0.6719,     -0.3633,      0.5742,     -0.1787,     -2.8594,     -0.0391,     -0.4629,      1.6328,     -2.5000,      1.3359,      0.8242,      4.1250,      0.0058,      0.2344,     -0.5977]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.1709,  0.0342,  0.0723,  1.3828,  0.7852, -1.1562, -0.4531,  0.5820,  0.4512,  0.3164,  0.3184, -0.0869, -0.4609,  1.2812, -0.7930,  0.2656],
        [ 0.9180,  0.0155,  0.5781, -0.4316, -0.5977, -0.8398,  0.1328,  0.2461, -0.5547, -0.8516, -0.5078,  0.3613,  1.5312,  0.1357,  0.1680, -0.9688],
        [ 0.9922,  0.3418,  0.9453, -0.1543, -0.2891, -0.9297, -0.3477, -0.3828, -0.3438, -0.6016, -0.3672,  0.2275,  1.2891, -0.9766, -0.2402, -0.6562],
        [-0.6094,  0.8164,  0.8750,  0.4629,  0.2441, -0.8281, -0.1426,  0.3535, -0.1562, -0.3789,  0.4336,  0.0830,  1.1797,  0.5273,  0.3145, -0.2451],
        [-1.3750,  0.5078,  0.6484, -0.1992, -0.7383, -0.2578,  0.1660,  1.1172,  0.0737, -0.9414,  0.7734,  0.1069,  0.5664,  0.6250, -0.4531,  0.5391],
        [-1.5234, -0.1758,  0.1562, -0.1128, -1.7109, -0.6680, -0.1533, -0.0972, -0.4922, -0.4727,  0.9453, -0.0273,  0.8594, -0.7773,  0.0830, -0.8242],
        [-0.4434, -0.1738, -0.0723,  0.3105, -1.8984, -0.0161, -0.3945, -0.1436, -0.3496, -0.1914,  0.0601,  0.1348,  0.4414,  0.2832, -0.3652, -1.3438],
        [-0.7656, -0.4609,  0.3027, -0.0183, -1.1172, -0.4258, -0.0601,  0.1079,  0.2305, -0.6133, -0.1348, -0.3652,  0.5820, -3.1094, -0.5742,  0.3301],
        [-0.8984, -0.3828, -0.6016, -0.1885, -0.8672,  0.1572, -0.0177,  0.6953,  0.5547, -0.6406, -0.0554, -0.5703,  0.4082, -1.4141, -1.2500,  0.8789],
        [ 0.8594,  0.1025,  0.1943,  0.5117, -0.4043, -0.8477, -0.2266, -0.3008, -0.0928, -0.5703, -0.4258,  0.0991,  0.2295,  0.0986,  0.4434, -0.2930]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  0.4492,   0.1484,   0.2041,   4.1250,   5.2812, -22.3750,  -1.0547,   1.9375,   4.9375,   2.2188,   0.9922,  -0.2734,  -8.5625,  18.7500,  -2.8750,   2.1719],
        [  3.6562,   0.1016,   2.4688,  -1.9297,  -6.0625, -24.5000,   0.4688,   1.2500,  -9.1250,  -8.9375,  -2.4062,   1.7266,  42.7500,   3.0000,   0.9219, -11.8750],
        [  3.4688,   1.9766,   3.5625,  -0.6094,  -2.5781, -24.0000,  -1.0781,  -1.7109,  -5.0000,  -5.5938,  -1.5234,   0.9570,  31.7500, -19.0000,  -1.1562,  -7.1250],
        [ -1.9688,   4.3750,   3.0469,   1.7031,   2.0312, -19.7500,  -0.4102,   1.4688,  -2.1094,  -3.2500,   1.6719,   0.3242,  27.0000,   9.5625,   1.4062,  -2.4844],
        [ -5.2500,   3.1875,   2.6562,  -0.8594,  -7.1875,  -7.2500,   0.5586,   5.4375,   1.1641,  -9.5000,   3.5000,   0.4883,  15.1250,  13.2500,  -2.3750,   6.3750],
        [ -4.8750,  -0.9297,   0.5391,  -0.4102, -14.0000, -15.8125,  -0.4336,  -0.3984,  -6.5938,  -4.0000,   3.6094,  -0.1055,  19.3750, -13.8750,   0.3652,  -8.2500],
        [ -1.2891,  -0.8359,  -0.2266,   1.0234, -14.0625,  -0.3438,  -1.0156,  -0.5312,  -4.2500,  -1.4688,   0.2080,   0.4727,   9.0000,   4.5625,  -1.4688, -12.1250],
        [ -2.0938,  -2.0938,   0.8906,  -0.0566,  -7.7812,  -8.5000,  -0.1455,   0.3750,   2.6250,  -4.4375,  -0.4375,  -1.2031,  11.1250, -47.2500,  -2.1719,   2.8125],
        [ -2.7188,  -1.9141,  -1.9531,  -0.6484,  -6.6875,   3.5000,  -0.0474,   2.6875,   7.0000,  -5.1562,  -0.1992,  -2.0781,   8.7500, -23.7500,  -5.2188,   8.2500],
        [  2.6250,   0.5156,   0.6367,   1.7656,  -3.1406, -19.0000,  -0.6133,  -1.1641,  -1.1797,  -4.6250,  -1.5391,   0.3633,   4.9375,   1.6719,   1.8594,  -2.7656]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.7500,      5.9375,    -18.1250,     30.0000,      6.0625,      0.9766,     -2.9375,      0.4238,      6.4375,    -11.5625,      8.5000,     12.3750,    -20.1250,      0.7148,    -17.7500,    -16.3750],
        [     1.3281,      0.2314,     -0.2695,     -0.4258,     -1.6484,     -9.6250,      1.0391,     -2.1875,     -8.0000,     -2.2969,     -0.7695,      2.5781,     -6.8750,      0.8516,      1.9297,      0.1514],
        [    -0.2451,     -0.8359,      0.7109,     -0.8086,     -0.0225,    -13.9375,      0.9102,     -1.1797,     -2.5938,     -2.0781,      0.4062,      0.5234,      0.0167,     -0.2070,     -0.8203,      3.5625],
        [    -1.3594,      1.8672,      0.8047,      0.9180,      1.3047,     -0.5156,     -0.5742,     -0.6328,      0.7188,      0.0491,     -0.6523,      0.5312,      3.9688,     -0.5547,     -1.8906,     -2.2344],
        [     0.2490,      1.0469,      1.0000,     -0.5938,     -5.7812,      8.6250,      0.4863,      3.3125,      1.3438,      1.2422,      1.5078,      2.0469,      5.0312,      0.0488,     -1.2734,      4.1562],
        [    -0.7305,     -0.0454,      0.1611,     -1.2969,      0.3711,     -2.0781,      0.9727,      0.7773,      1.8984,      1.5547,     -1.1484,      0.1650,     -3.5156,     -0.7188,      0.0425,     -0.5586],
        [    -0.1338,      0.3906,      0.4688,     -1.4141,      0.8789,     -6.6250,      0.2773,     -0.7148,      0.8750,     -1.1953,     -1.4766,      1.3203,     -5.2812,     -0.7734,      0.2412,      0.1299],
        [    -0.7188,     -1.1719,      0.8203,     -0.0094,      2.1719,     -7.4688,      0.1260,     -0.0195,      0.0461,     -0.3066,     -0.2109,      0.3516,      0.6445,     -1.3984,     -0.3867,     -0.2617],
        [    -0.1230,     -1.4688,     -0.4453,     -0.5859,     -1.2812,     -3.7031,     -1.2578,     -0.5000,      0.4902,      1.4062,     -0.1748,      1.1016,     -3.2656,      0.1953,      0.2217,     -4.0312],
        [    -0.3027,      1.9922,     -1.2734,      1.1719,     -2.7969,     -2.3281,      0.4141,     -1.7188,     -1.8281,      1.9922,     -1.2812,      0.4043,      3.8750,     -0.4414,     -0.8242,      1.3125]]
-------------------------
name='positions layer 6'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 6'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.7500,      5.9375,    -18.1250,     30.0000,      6.0625,      0.9766,     -2.9375,      0.4238,      6.4375,    -11.5625,      8.5000,     12.3750,    -20.1250,      0.7148,    -17.7500,    -16.3750],
        [     1.3281,      0.2314,     -0.2695,     -0.4258,     -1.6484,     -9.6250,      1.0391,     -2.1875,     -8.0000,     -2.2969,     -0.7695,      2.5781,     -6.8750,      0.8516,      1.9297,      0.1514],
        [    -0.2451,     -0.8359,      0.7109,     -0.8086,     -0.0225,    -13.9375,      0.9102,     -1.1797,     -2.5938,     -2.0781,      0.4062,      0.5234,      0.0167,     -0.2070,     -0.8203,      3.5625],
        [    -1.3594,      1.8672,      0.8047,      0.9180,      1.3047,     -0.5156,     -0.5742,     -0.6328,      0.7188,      0.0491,     -0.6523,      0.5312,      3.9688,     -0.5547,     -1.8906,     -2.2344],
        [     0.2490,      1.0469,      1.0000,     -0.5938,     -5.7812,      8.6250,      0.4863,      3.3125,      1.3438,      1.2422,      1.5078,      2.0469,      5.0312,      0.0488,     -1.2734,      4.1562],
        [    -0.7305,     -0.0454,      0.1611,     -1.2969,      0.3711,     -2.0781,      0.9727,      0.7773,      1.8984,      1.5547,     -1.1484,      0.1650,     -3.5156,     -0.7188,      0.0425,     -0.5586],
        [    -0.1338,      0.3906,      0.4688,     -1.4141,      0.8789,     -6.6250,      0.2773,     -0.7148,      0.8750,     -1.1953,     -1.4766,      1.3203,     -5.2812,     -0.7734,      0.2412,      0.1299],
        [    -0.7188,     -1.1719,      0.8203,     -0.0094,      2.1719,     -7.4688,      0.1260,     -0.0195,      0.0461,     -0.3066,     -0.2109,      0.3516,      0.6445,     -1.3984,     -0.3867,     -0.2617],
        [    -0.1230,     -1.4688,     -0.4453,     -0.5859,     -1.2812,     -3.7031,     -1.2578,     -0.5000,      0.4902,      1.4062,     -0.1748,      1.1016,     -3.2656,      0.1953,      0.2217,     -4.0312],
        [    -0.3027,      1.9922,     -1.2734,      1.1719,     -2.7969,     -2.3281,      0.4141,     -1.7188,     -1.8281,      1.9922,     -1.2812,      0.4043,      3.8750,     -0.4414,     -0.8242,      1.3125]]
name='residual layer 6'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  0.4492,   0.1484,   0.2041,   4.1250,   5.2812, -22.3750,  -1.0547,   1.9375,   4.9375,   2.2188,   0.9922,  -0.2734,  -8.5625,  18.7500,  -2.8750,   2.1719],
        [  3.6562,   0.1016,   2.4688,  -1.9297,  -6.0625, -24.5000,   0.4688,   1.2500,  -9.1250,  -8.9375,  -2.4062,   1.7266,  42.7500,   3.0000,   0.9219, -11.8750],
        [  3.4688,   1.9766,   3.5625,  -0.6094,  -2.5781, -24.0000,  -1.0781,  -1.7109,  -5.0000,  -5.5938,  -1.5234,   0.9570,  31.7500, -19.0000,  -1.1562,  -7.1250],
        [ -1.9688,   4.3750,   3.0469,   1.7031,   2.0312, -19.7500,  -0.4102,   1.4688,  -2.1094,  -3.2500,   1.6719,   0.3242,  27.0000,   9.5625,   1.4062,  -2.4844],
        [ -5.2500,   3.1875,   2.6562,  -0.8594,  -7.1875,  -7.2500,   0.5586,   5.4375,   1.1641,  -9.5000,   3.5000,   0.4883,  15.1250,  13.2500,  -2.3750,   6.3750],
        [ -4.8750,  -0.9297,   0.5391,  -0.4102, -14.0000, -15.8125,  -0.4336,  -0.3984,  -6.5938,  -4.0000,   3.6094,  -0.1055,  19.3750, -13.8750,   0.3652,  -8.2500],
        [ -1.2891,  -0.8359,  -0.2266,   1.0234, -14.0625,  -0.3438,  -1.0156,  -0.5312,  -4.2500,  -1.4688,   0.2080,   0.4727,   9.0000,   4.5625,  -1.4688, -12.1250],
        [ -2.0938,  -2.0938,   0.8906,  -0.0566,  -7.7812,  -8.5000,  -0.1455,   0.3750,   2.6250,  -4.4375,  -0.4375,  -1.2031,  11.1250, -47.2500,  -2.1719,   2.8125],
        [ -2.7188,  -1.9141,  -1.9531,  -0.6484,  -6.6875,   3.5000,  -0.0474,   2.6875,   7.0000,  -5.1562,  -0.1992,  -2.0781,   8.7500, -23.7500,  -5.2188,   8.2500],
        [  2.6250,   0.5156,   0.6367,   1.7656,  -3.1406, -19.0000,  -0.6133,  -1.1641,  -1.1797,  -4.6250,  -1.5391,   0.3633,   4.9375,   1.6719,   1.8594,  -2.7656]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0122,      0.0112,     -0.0369,      0.0664,      0.0171,     -0.0294,     -0.0103,      0.0045,      0.0157,     -0.0128,      0.0214,      0.0267,     -0.0522,      0.0366,     -0.0388,     -0.0178],
        [     0.6758,      0.0354,      0.2598,     -0.2656,     -0.6719,     -2.6875,      0.2246,     -0.1040,     -1.3672,     -0.8867,     -0.4121,      0.5508,      3.7812,      0.4160,      0.3086,     -0.8438],
        [     0.4980,      0.1387,      0.5781,     -0.1826,     -0.2598,     -3.4219,     -0.0286,     -0.3672,     -0.6914,     -0.6953,     -0.1660,      0.2158,      3.8438,     -2.3750,     -0.2451,     -0.2930],
        [    -0.5039,      0.7383,      0.5078,      0.3281,      0.3223,     -1.7812,     -0.1621,      0.1025,     -0.1230,     -0.2793,      0.1465,      0.1216,      3.6406,      1.0859,     -0.0583,     -0.3770],
        [    -0.7031,      0.4688,      0.4473,     -0.1689,     -1.1719,      0.1123,      0.1602,      1.0000,      0.2061,     -0.6758,      0.6719,      0.3340,      2.2031,      1.4922,     -0.4102,      0.7852],
        [    -0.9258,     -0.1260,      0.1011,     -0.2334,     -1.4375,     -1.7109,      0.0977,      0.0508,     -0.4551,     -0.2354,      0.3887,      0.0092,      2.0312,     -1.9141,      0.0537,     -0.7695],
        [    -0.2695,     -0.0659,      0.0398,     -0.0610,     -1.5938,     -0.7656,     -0.1523,     -0.1914,     -0.3730,     -0.2930,     -0.2285,      0.3164,      0.5430,      0.5703,     -0.1846,     -1.2031],
        [    -0.5469,     -0.5000,      0.2910,     -0.0106,     -0.6992,     -1.8047,     -0.0042,      0.0564,      0.3047,     -0.5391,     -0.1211,     -0.1562,      1.7891,     -7.5625,     -0.3984,      0.2637],
        [    -0.4766,     -0.4453,     -0.3496,     -0.1719,     -0.8555,     -0.0199,     -0.2402,      0.2988,      0.7383,     -0.3652,     -0.0601,     -0.1533,      0.7148,     -3.1719,     -0.6680,      0.3750],
        [     0.4277,      0.3633,     -0.1025,      0.4492,     -0.7031,     -2.2969,     -0.0403,     -0.4336,     -0.3262,     -0.2812,     -0.4980,      0.1328,      1.2656,      0.1816,      0.1523,     -0.1426]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     5.1875,      6.0938,    -17.8750,     34.0000,     11.3750,    -21.3750,     -4.0000,      2.3594,     11.3750,     -9.3750,      9.5000,     12.1250,    -28.7500,     19.5000,    -20.6250,    -14.1875],
        [     5.0000,      0.3320,      2.2031,     -2.3594,     -7.7188,    -34.0000,      1.5078,     -0.9375,    -17.1250,    -11.2500,     -3.1719,      4.3125,     36.0000,      3.8438,      2.8438,    -11.7500],
        [     3.2188,      1.1406,      4.2812,     -1.4219,     -2.5938,    -38.0000,     -0.1680,     -2.8906,     -7.5938,     -7.6875,     -1.1172,      1.4844,     31.7500,    -19.2500,     -1.9766,     -3.5625],
        [    -3.3281,      6.2500,      3.8438,      2.6250,      3.3438,    -20.2500,     -0.9844,      0.8359,     -1.3906,     -3.2031,      1.0156,      0.8555,     31.0000,      9.0000,     -0.4844,     -4.7188],
        [    -5.0000,      4.2500,      3.6562,     -1.4531,    -13.0000,      1.3750,      1.0469,      8.7500,      2.5000,     -8.2500,      5.0000,      2.5312,     20.1250,     13.3125,     -3.6562,     10.5000],
        [    -5.5938,     -0.9766,      0.6992,     -1.7031,    -13.6250,    -17.8750,      0.5391,      0.3789,     -4.6875,     -2.4375,      2.4688,      0.0596,     15.8750,    -14.6250,      0.4082,     -8.8125],
        [    -1.4219,     -0.4453,      0.2422,     -0.3906,    -13.1875,     -6.9688,     -0.7383,     -1.2500,     -3.3750,     -2.6562,     -1.2656,      1.7969,      3.7188,      3.7812,     -1.2266,    -12.0000],
        [    -2.8125,     -3.2656,      1.7109,     -0.0659,     -5.6250,    -16.0000,     -0.0195,      0.3555,      2.6719,     -4.7500,     -0.6484,     -0.8516,     11.7500,    -48.7500,     -2.5625,      2.5469],
        [    -2.8438,     -3.3750,     -2.4062,     -1.2344,     -7.9688,     -0.2031,     -1.3047,      2.1875,      7.5000,     -3.7500,     -0.3750,     -0.9766,      5.5000,    -23.5000,     -5.0000,      4.2188],
        [     2.3281,      2.5000,     -0.6367,      2.9375,     -5.9375,    -21.3750,     -0.1992,     -2.8750,     -3.0000,     -2.6250,     -2.8125,      0.7656,      8.8125,      1.2344,      1.0312,     -1.4531]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-2.4219,  0.5508,  0.4277, -1.5547, -0.5078, -9.0000,  0.0864,  0.2812, -1.5625,  1.1875, -0.0967,  0.9805,  2.4531, -0.8750, -0.8164, -0.6602],
        [ 0.4258, -0.5156, -0.2354,  0.5742,  0.6602, -4.5312,  0.4961,  1.0781,  0.1758,  1.9297, -1.2266,  0.2275, -1.8047, -0.4414, -0.0796,  3.2812],
        [-0.0898,  1.4141,  0.5664, -0.0476, -0.0757, -9.0000,  0.7383,  1.0312, -0.0107,  0.7656, -0.6445,  0.2432,  4.7812, -0.1660,  0.8242,  2.0156],
        [-0.6406,  0.6562,  1.2031, -0.3555,  2.0312, -2.7031,  0.1611,  0.5547,  0.5117,  0.5195,  0.7695, -0.3223, -0.6055, -0.0352,  0.5234,  1.4531],
        [-1.2031,  0.6562,  0.6133, -1.1562,  0.0601,  0.1973,  0.3711,  0.0732, -3.0781, -0.6016,  1.4609, -1.3672, -2.6719,  0.2734, -0.2637,  1.5234],
        [-0.6133,  2.2656, -0.7109, -1.5938, -0.9141, -1.4062,  0.7539,  0.1387,  0.1494, -1.0000,  0.7188, -0.7070,  4.1875,  0.2520, -0.2021,  3.3125],
        [-0.2305,  1.8281, -0.4414, -1.3516, -0.5625, -0.7383,  1.1953, -0.2012,  0.7031,  0.4766, -0.1641, -0.3906,  2.1094,  0.3262, -1.7031,  1.0000],
        [-0.9180,  2.6094,  0.3398, -0.8438, -2.7812, -2.1250,  0.4590, -0.4102, -2.3906,  1.8672,  0.9805, -1.5938,  3.6094,  0.1201, -0.5234,  1.2734],
        [-0.4258,  1.4219,  0.6016, -1.0078, -1.2031,  0.7578,  0.5938,  0.3281, -1.6016,  0.3242,  1.4453, -0.4531, -2.2500,  0.0752, -0.9414,  1.4766],
        [-0.7812,  2.1562, -0.4414,  0.2451, -0.6211,  2.6719, -0.2031, -0.2695, -0.5156, -0.7266,  1.1484, -1.2500,  5.6562, -0.0327, -0.4570,  4.7188]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0089,      0.0133,     -0.0527,      0.0898,      0.0132,     -0.0143,     -0.0143,      0.0065,      0.0085,     -0.0101,      0.0249,      0.0342,     -0.0159,      0.0151,     -0.0483,     -0.0159],
        [     0.9961,     -0.0210,      0.3418,     -0.2812,     -0.4902,     -1.0312,      0.4199,      0.0198,     -0.8398,     -0.6562,     -0.6641,      0.6797,      1.1875,      0.1572,      0.3574,     -0.5156],
        [     0.6719,      0.3418,      0.9805,     -0.2715,     -0.2168,     -1.4688,      0.1396,     -0.3086,     -0.4414,     -0.5703,     -0.3125,      0.3008,      1.4766,     -1.0547,     -0.1738,     -0.1104],
        [    -0.8633,      0.9336,      1.0391,      0.4277,      0.4434,     -0.7305,     -0.2041,      0.2324,     -0.0518,     -0.2256,      0.3203,      0.0942,      1.2500,      0.4922,      0.0060,     -0.2373],
        [    -1.2656,      0.6250,      0.8242,     -0.4609,     -1.0078,      0.0471,      0.3301,      1.3906,     -0.0320,     -0.6992,      1.0938,      0.1934,      0.6719,      0.7031,     -0.5664,      0.8203],
        [    -1.4531,      0.1885,     -0.0026,     -0.6641,     -1.2891,     -0.6602,      0.3457,      0.0933,     -0.2871,     -0.3105,      0.6172,     -0.1235,      0.8867,     -0.8516,      0.0339,     -0.4297],
        [    -0.4766,      0.2471,     -0.0542,     -0.4336,     -1.5000,     -0.3262,      0.1504,     -0.3203,     -0.2090,     -0.2422,     -0.3398,      0.3301,      0.3164,      0.2988,     -0.5938,     -1.0547],
        [    -1.0625,     -0.1162,      0.5508,     -0.2236,     -0.9023,     -0.7539,      0.1416,     -0.0120,      0.0216,     -0.3145,      0.0776,     -0.5625,      0.8203,     -3.4844,     -0.6133,      0.3613],
        [    -0.8594,     -0.3184,     -0.4473,     -0.5078,     -0.9102,      0.0214,     -0.2129,      0.5078,      0.4199,     -0.3457,      0.2314,     -0.3047,      0.1611,     -1.5547,     -1.1016,      0.4980],
        [     0.4062,      0.7578,     -0.2656,      0.7188,     -0.6484,     -0.7148,     -0.1196,     -0.6289,     -0.2480,     -0.3379,     -0.3594,     -0.1030,      0.7148,      0.0791,      0.1055,      0.2852]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     2.7656,      6.6562,    -17.5000,     32.5000,     10.8750,    -30.3750,     -3.9062,      2.6406,      9.8125,     -8.1875,      9.3750,     13.1250,    -26.2500,     18.6250,    -21.5000,    -14.8750],
        [     5.4375,     -0.1836,      1.9688,     -1.7812,     -7.0625,    -38.5000,      2.0000,      0.1406,    -17.0000,     -9.3125,     -4.4062,      4.5312,     34.2500,      3.4062,      2.7656,     -8.5000],
        [     3.1250,      2.5625,      4.8438,     -1.4688,     -2.6719,    -47.0000,      0.5703,     -1.8594,     -7.5938,     -6.9375,     -1.7656,      1.7266,     36.5000,    -19.3750,     -1.1562,     -1.5469],
        [    -3.9688,      6.9062,      5.0625,      2.2656,      5.3750,    -23.0000,     -0.8242,      1.3906,     -0.8789,     -2.6875,      1.7812,      0.5312,     30.3750,      8.9375,      0.0391,     -3.2656],
        [    -6.1875,      4.9062,      4.2812,     -2.6094,    -12.9375,      1.5703,      1.4219,      8.8125,     -0.5781,     -8.8750,      6.4688,      1.1641,     17.5000,     13.5625,     -3.9219,     12.0000],
        [    -6.2188,      1.2891,     -0.0117,     -3.2969,    -14.5625,    -19.2500,      1.2969,      0.5156,     -4.5312,     -3.4375,      3.1875,     -0.6484,     20.0000,    -14.3750,      0.2061,     -5.5000],
        [    -1.6562,      1.3828,     -0.1992,     -1.7422,    -13.7500,     -7.7188,      0.4570,     -1.4531,     -2.6719,     -2.1875,     -1.4297,      1.4062,      5.8125,      4.0938,     -2.9375,    -11.0000],
        [    -3.7344,     -0.6562,      2.0469,     -0.9102,     -8.3750,    -18.1250,      0.4395,     -0.0547,      0.2812,     -2.8750,      0.3320,     -2.4375,     15.3750,    -48.7500,     -3.0938,      3.8125],
        [    -3.2656,     -1.9531,     -1.8047,     -2.2500,     -9.1875,      0.5547,     -0.7109,      2.5156,      5.9062,     -3.4219,      1.0703,     -1.4297,      3.2500,    -23.3750,     -5.9375,      5.6875],
        [     1.5469,      4.6562,     -1.0781,      3.1875,     -6.5625,    -18.7500,     -0.4023,     -3.1406,     -3.5156,     -3.3438,     -1.6641,     -0.4844,     14.5000,      1.2031,      0.5742,      3.2656]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.2754,      1.0625,     -0.0425,      0.4043,     -0.2256,     -1.0000,      0.4199,     -0.6797,      1.8672,     -0.4375,      0.5938,      0.8359,      1.6094,      0.0065,     -0.3438,      0.2305],
        [     0.6523,     -0.8125,      0.1309,     -0.6094,      1.1953,     -3.5156,     -0.7383,     -0.7969,      1.9219,     -2.0156,      0.8672,      1.7500,      2.7500,      0.2021,      3.5156,      5.3125],
        [     2.5938,      1.0547,      0.3652,     -0.4551,     -0.3223,     -3.6562,     -0.4824,      0.3965,      1.6250,      0.7227,     -1.2422,     -0.5273,      2.8906,     -0.4180,      2.5312,      3.4062],
        [    -1.1406,     -0.1348,      2.1719,      0.3789,     -4.2500,     -3.9062,      2.1562,      2.2500,     -2.2500,     -6.5312,      0.2012,     -0.2100,      3.6250,     -0.5586,     -1.7500,      2.0469],
        [     1.4297,      1.8047,      1.3906,     -0.1816,     -1.4219,     -3.3594,      0.8984,      0.3457,     -5.0312,     -1.3203,      1.5156,     -0.1572,      0.1357,     -0.3887,     -0.8203,      2.0312],
        [     2.8906,      4.6875,      0.2637,     -0.1904,      0.4453,     -1.2031,      1.4844,     -0.3125,     -4.7812,     -2.2812,     -0.5312,      0.7500,     -3.2344,      0.5898,      0.9414,     -4.4688],
        [     1.8438,     -0.0864,      1.6172,     -0.4238,      2.1719,     -1.1406,      0.6641,      0.3164,      5.4062,     -0.4258,     -1.4453,      0.7227,     -0.4805,      0.3867,     -1.7578,      0.7383],
        [     1.2969,      1.3281,      2.7188,     -0.6094,     -4.2500,      2.8594,     -0.1670,      1.8984,     -0.0120,      4.3438,      0.3730,      2.5312,      5.2188,      0.3867,     -0.6641,      1.0391],
        [     3.5625,      0.1216,     -1.5156,      2.2031,     -2.4219,      2.1250,     -1.6016,      5.1562,     -6.7188,     -2.1562,      2.9688,      1.5234,      1.7578,     -1.1641,     -0.3340,      1.4453],
        [     1.6484,      0.7383,     -0.8750,     -1.6641,     -1.7734,     -3.5781,     -0.5898,      0.9648,      0.7617,     -4.0000,      1.4141,      1.0391,      6.7500,      1.7812,      3.4844,      0.7188]]
-------------------------
name='positions layer 7'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 7'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.2754,      1.0625,     -0.0425,      0.4043,     -0.2256,     -1.0000,      0.4199,     -0.6797,      1.8672,     -0.4375,      0.5938,      0.8359,      1.6094,      0.0065,     -0.3438,      0.2305],
        [     0.6523,     -0.8125,      0.1309,     -0.6094,      1.1953,     -3.5156,     -0.7383,     -0.7969,      1.9219,     -2.0156,      0.8672,      1.7500,      2.7500,      0.2021,      3.5156,      5.3125],
        [     2.5938,      1.0547,      0.3652,     -0.4551,     -0.3223,     -3.6562,     -0.4824,      0.3965,      1.6250,      0.7227,     -1.2422,     -0.5273,      2.8906,     -0.4180,      2.5312,      3.4062],
        [    -1.1406,     -0.1348,      2.1719,      0.3789,     -4.2500,     -3.9062,      2.1562,      2.2500,     -2.2500,     -6.5312,      0.2012,     -0.2100,      3.6250,     -0.5586,     -1.7500,      2.0469],
        [     1.4297,      1.8047,      1.3906,     -0.1816,     -1.4219,     -3.3594,      0.8984,      0.3457,     -5.0312,     -1.3203,      1.5156,     -0.1572,      0.1357,     -0.3887,     -0.8203,      2.0312],
        [     2.8906,      4.6875,      0.2637,     -0.1904,      0.4453,     -1.2031,      1.4844,     -0.3125,     -4.7812,     -2.2812,     -0.5312,      0.7500,     -3.2344,      0.5898,      0.9414,     -4.4688],
        [     1.8438,     -0.0864,      1.6172,     -0.4238,      2.1719,     -1.1406,      0.6641,      0.3164,      5.4062,     -0.4258,     -1.4453,      0.7227,     -0.4805,      0.3867,     -1.7578,      0.7383],
        [     1.2969,      1.3281,      2.7188,     -0.6094,     -4.2500,      2.8594,     -0.1670,      1.8984,     -0.0120,      4.3438,      0.3730,      2.5312,      5.2188,      0.3867,     -0.6641,      1.0391],
        [     3.5625,      0.1216,     -1.5156,      2.2031,     -2.4219,      2.1250,     -1.6016,      5.1562,     -6.7188,     -2.1562,      2.9688,      1.5234,      1.7578,     -1.1641,     -0.3340,      1.4453],
        [     1.6484,      0.7383,     -0.8750,     -1.6641,     -1.7734,     -3.5781,     -0.5898,      0.9648,      0.7617,     -4.0000,      1.4141,      1.0391,      6.7500,      1.7812,      3.4844,      0.7188]]
name='residual layer 7'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     2.7656,      6.6562,    -17.5000,     32.5000,     10.8750,    -30.3750,     -3.9062,      2.6406,      9.8125,     -8.1875,      9.3750,     13.1250,    -26.2500,     18.6250,    -21.5000,    -14.8750],
        [     5.4375,     -0.1836,      1.9688,     -1.7812,     -7.0625,    -38.5000,      2.0000,      0.1406,    -17.0000,     -9.3125,     -4.4062,      4.5312,     34.2500,      3.4062,      2.7656,     -8.5000],
        [     3.1250,      2.5625,      4.8438,     -1.4688,     -2.6719,    -47.0000,      0.5703,     -1.8594,     -7.5938,     -6.9375,     -1.7656,      1.7266,     36.5000,    -19.3750,     -1.1562,     -1.5469],
        [    -3.9688,      6.9062,      5.0625,      2.2656,      5.3750,    -23.0000,     -0.8242,      1.3906,     -0.8789,     -2.6875,      1.7812,      0.5312,     30.3750,      8.9375,      0.0391,     -3.2656],
        [    -6.1875,      4.9062,      4.2812,     -2.6094,    -12.9375,      1.5703,      1.4219,      8.8125,     -0.5781,     -8.8750,      6.4688,      1.1641,     17.5000,     13.5625,     -3.9219,     12.0000],
        [    -6.2188,      1.2891,     -0.0117,     -3.2969,    -14.5625,    -19.2500,      1.2969,      0.5156,     -4.5312,     -3.4375,      3.1875,     -0.6484,     20.0000,    -14.3750,      0.2061,     -5.5000],
        [    -1.6562,      1.3828,     -0.1992,     -1.7422,    -13.7500,     -7.7188,      0.4570,     -1.4531,     -2.6719,     -2.1875,     -1.4297,      1.4062,      5.8125,      4.0938,     -2.9375,    -11.0000],
        [    -3.7344,     -0.6562,      2.0469,     -0.9102,     -8.3750,    -18.1250,      0.4395,     -0.0547,      0.2812,     -2.8750,      0.3320,     -2.4375,     15.3750,    -48.7500,     -3.0938,      3.8125],
        [    -3.2656,     -1.9531,     -1.8047,     -2.2500,     -9.1875,      0.5547,     -0.7109,      2.5156,      5.9062,     -3.4219,      1.0703,     -1.4297,      3.2500,    -23.3750,     -5.9375,      5.6875],
        [     1.5469,      4.6562,     -1.0781,      3.1875,     -6.5625,    -18.7500,     -0.4023,     -3.1406,     -3.5156,     -3.3438,     -1.6641,     -0.4844,     14.5000,      1.2031,      0.5742,      3.2656]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0059,      0.0143,     -0.0398,      0.0688,      0.0157,     -0.0383,     -0.0093,      0.0039,      0.0171,     -0.0120,      0.0227,      0.0297,     -0.0415,      0.0271,     -0.0432,     -0.0194],
        [     0.7461,     -0.0952,      0.2441,     -0.2559,     -0.4414,     -2.6406,      0.1729,     -0.0664,     -1.1328,     -0.8086,     -0.4121,      0.6836,      3.1875,      0.2695,      0.6328,     -0.2168],
        [     0.7891,      0.3887,      0.6875,     -0.2314,     -0.2539,     -3.5781,      0.0135,     -0.1670,     -0.5039,     -0.5000,     -0.3945,      0.1475,      3.8125,     -1.6641,      0.1572,      0.1416],
        [    -0.7109,      0.7383,      0.9648,      0.3223,      0.0972,     -1.9219,      0.2080,      0.4199,     -0.2676,     -0.7500,      0.2637,      0.0398,      3.3281,      0.7109,     -0.1973,     -0.0942],
        [    -0.5547,      0.6133,      0.6289,     -0.2832,     -1.0391,     -0.1069,      0.3027,      0.8828,     -0.4004,     -0.6953,      0.8867,      0.1045,      1.4453,      0.9375,     -0.4570,      0.9062],
        [    -0.4473,      0.6289,      0.0322,     -0.4102,     -1.1719,     -1.4062,      0.4180,      0.0226,     -0.7656,     -0.4492,      0.3418,      0.0122,      1.5781,     -1.1250,      0.1279,     -0.7461],
        [     0.0317,      0.1729,      0.2305,     -0.3223,     -1.2109,     -0.7695,      0.2129,     -0.1602,      0.2852,     -0.2598,     -0.4668,      0.3223,      0.6367,      0.4648,     -0.6602,     -0.9688],
        [    -0.4141,      0.0889,      0.7695,     -0.2256,     -1.3203,     -1.3281,      0.0515,      0.2578,      0.0280,      0.1455,      0.1143,      0.0142,      2.4531,     -5.0000,     -0.5273,      0.4551],
        [     0.0410,     -0.1973,     -0.4375,     -0.0056,     -0.9922,      0.1895,     -0.3574,      0.8789,     -0.0688,     -0.4492,      0.5352,      0.0115,      0.4844,     -2.0625,     -0.7148,      0.5469],
        [     0.4785,      0.6289,     -0.2793,      0.1992,     -0.7734,     -1.7109,     -0.1660,     -0.2695,     -0.2520,     -0.6406,     -0.0356,      0.0742,      2.2344,      0.2715,      0.5039,      0.3301]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     2.4844,      7.7188,    -17.5000,     33.0000,     10.6250,    -31.3750,     -3.4844,      1.9609,     11.6875,     -8.6250,     10.0000,     13.9375,    -24.6250,     18.6250,    -21.8750,    -14.6250],
        [     6.0938,     -0.9961,      2.0938,     -2.3906,     -5.8750,    -42.0000,      1.2656,     -0.6562,    -15.0625,    -11.3125,     -3.5312,      6.2812,     37.0000,      3.6094,      6.2812,     -3.1875],
        [     5.7188,      3.6250,      5.2188,     -1.9219,     -3.0000,    -50.7500,      0.0879,     -1.4609,     -5.9688,     -6.2188,     -3.0000,      1.2031,     39.5000,    -19.7500,      1.3750,      1.8594],
        [    -5.1250,      6.7812,      7.2500,      2.6406,      1.1250,    -26.8750,      1.3281,      3.6406,     -3.1250,     -9.2500,      1.9844,      0.3203,     34.0000,      8.3750,     -1.7109,     -1.2188],
        [    -4.7500,      6.7188,      5.6875,     -2.7969,    -14.3750,     -1.7891,      2.3125,      9.1875,     -5.6250,    -10.1875,      8.0000,      1.0078,     17.6250,     13.1875,     -4.7500,     14.0000],
        [    -3.3281,      5.9688,      0.2520,     -3.4844,    -14.1250,    -20.5000,      2.7812,      0.2031,     -9.3125,     -5.7188,      2.6562,      0.1016,     16.7500,    -13.8125,      1.1484,    -10.0000],
        [     0.1875,      1.2969,      1.4219,     -2.1719,    -11.5625,     -8.8750,      1.1250,     -1.1406,      2.7344,     -2.6094,     -2.8750,      2.1250,      5.3438,      4.4688,     -4.6875,    -10.2500],
        [    -2.4375,      0.6719,      4.7500,     -1.5156,    -12.6250,    -15.2500,      0.2734,      1.8438,      0.2695,      1.4688,      0.7031,      0.0938,     20.6250,    -48.2500,     -3.7500,      4.8438],
        [     0.2969,     -1.8281,     -3.3125,     -0.0469,    -11.6250,      2.6875,     -2.3125,      7.6875,     -0.8125,     -5.5625,      4.0312,      0.0938,      5.0000,    -24.5000,     -6.2812,      7.1250],
        [     3.1875,      5.4062,     -1.9531,      1.5234,     -8.3125,    -22.3750,     -0.9922,     -2.1719,     -2.7500,     -7.3438,     -0.2500,      0.5547,     21.2500,      2.9844,      4.0625,      3.9844]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     1.9688,      6.9062,     -1.8359,      2.3750,     -2.4688,     -6.3125,     -0.5117,      0.0601,     -3.7656,     -1.1484,      0.5039,     -4.2500,     13.6875,     -0.6055,     -0.6875,     -3.1094],
        [    -1.1875,     -1.0781,      0.5742,      0.3359,     -0.7109,     -1.1719,     -0.3008,      0.4785,     -1.5547,     -1.2812,      2.0469,     -0.3652,     -4.8125,     -0.5352,     -0.8125,     -1.5859],
        [     0.1729,      0.5352,     -1.1250,      0.4668,     -3.1250,     -0.6914,      1.2344,      1.3281,     -3.6719,     -2.9844,      1.5156,      0.4590,      0.9297,     -0.2256,     -1.0703,     -2.4531],
        [     0.3242,      1.0781,     -0.4199,      0.6914,     -0.5859,     -5.0938,      0.3828,     -0.0742,     -4.2500,     -1.7578,      1.8594,      0.9844,     -0.5820,      0.1758,     -1.9531,     -0.0041],
        [    -0.4316,     -0.4336,      0.7461,      0.2520,     -2.5156,     -6.6875,      0.4688,     -1.0156,     -3.2344,     -1.8203,      2.2188,      1.5312,     -3.0469,      0.3906,      0.4668,     -0.2949],
        [     1.7109,      0.2451,     -0.4805,      0.1006,     -1.6406,     -4.1562,     -0.6406,      0.5938,     -0.5234,     -7.4375,      2.4844,     -0.2061,      1.5547,      0.4277,      0.2930,     -1.7500],
        [    -0.0603,     -0.5195,     -0.5117,     -0.2734,      1.8047,     -1.7578,      0.0869,      0.4609,     -2.0938,     -5.3125,      2.9062,      0.6953,     -5.8125,      0.3223,      0.0212,      1.2578],
        [    -0.3027,      0.5430,      0.2217,     -0.2930,      0.0496,     -3.6719,     -0.1592,     -1.1484,     -2.0156,     -2.1250,      2.9219,     -2.1719,     -1.8984,      0.1504,     -0.2617,      0.0076],
        [    -2.2500,      0.1445,      1.1875,     -0.7266,      2.3594,     -3.6406,      0.5859,      0.2832,     -2.4688,     -3.0781,      0.6094,      0.4629,     -1.5312,     -0.1309,     -3.5000,      1.3125],
        [    -1.3047,     -0.4004,      0.4023,     -0.6641,      3.1875,      1.3594,     -1.9922,     -0.0154,      2.0938,     -3.6875,      1.3359,     -2.3281,      0.6328,      0.1562,     -1.0000,     -0.3809]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.0130,  0.0275, -0.0557,  0.0942,  0.0096, -0.0175, -0.0135,  0.0047,  0.0072, -0.0121,  0.0251,  0.0233, -0.0069,  0.0142, -0.0486, -0.0181],
        [ 0.7422, -0.2012,  0.4004, -0.2832, -0.4043, -1.0391,  0.1699, -0.0215, -0.7852, -0.8164, -0.1846,  0.7383,  1.0469,  0.1270,  0.6094, -0.2539],
        [ 1.0625,  0.4785,  0.7266, -0.2383, -0.4453, -1.4688,  0.2754, -0.0190, -0.5391, -0.7070, -0.2188,  0.2451,  1.5547, -0.9727,  0.0403, -0.0374],
        [-0.8242,  0.8633,  1.1484,  0.5195,  0.0374, -0.8711,  0.3398,  0.4844, -0.3926, -0.8047,  0.5391,  0.1836,  1.2266,  0.3945, -0.4629, -0.0732],
        [-0.7812,  0.6055,  0.9531, -0.3477, -1.0234, -0.2021,  0.4824,  0.9766, -0.4121, -0.7656,  1.2578,  0.3125,  0.4688,  0.5508, -0.4727,  0.7188],
        [-0.2734,  0.6719, -0.0381, -0.5195, -1.0703, -0.6602,  0.4180,  0.1069, -0.5156, -0.9453,  0.7109, -0.0145,  0.6641, -0.6133,  0.1797, -0.6953],
        [ 0.0275,  0.1069,  0.1934, -0.4805, -0.8516, -0.3633,  0.3027, -0.1167,  0.0427, -0.7305,  0.0055,  0.4980, -0.0216,  0.2793, -0.7422, -0.6797],
        [-0.5508,  0.1562,  0.9844, -0.3301, -1.0234, -0.6055,  0.0266,  0.1108, -0.1089, -0.0564,  0.5977, -0.3438,  0.8047, -2.6250, -0.5938,  0.3418],
        [-0.3379, -0.1865, -0.3633, -0.1216, -0.6484, -0.0261, -0.3457,  1.0938, -0.1758, -0.6367,  0.6562,  0.0791,  0.1289, -1.1562, -1.2500,  0.5117],
        [ 0.3652,  0.6250, -0.2969,  0.1523, -0.4023, -0.6445, -0.6719, -0.3379, -0.0396, -0.9141,  0.1729, -0.2832,  0.9102,  0.1650,  0.4395,  0.2441]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.4375,     14.6250,    -19.3750,     35.5000,      8.1250,    -37.7500,     -4.0000,      2.0156,      7.9375,     -9.7500,     10.5000,      9.6875,    -10.9375,     18.0000,    -22.5000,    -17.7500],
        [     4.9062,     -2.0781,      2.6719,     -2.0625,     -6.5938,    -43.2500,      0.9648,     -0.1777,    -16.6250,    -12.6250,     -1.4844,      5.9062,     32.2500,      3.0781,      5.4688,     -4.7812],
        [     5.9062,      4.1562,      4.0938,     -1.4531,     -6.1250,    -51.5000,      1.3203,     -0.1328,     -9.6250,     -9.1875,     -1.4844,      1.6641,     40.5000,    -20.0000,      0.3047,     -0.5938],
        [    -4.8125,      7.8750,      6.8438,      3.3281,      0.5391,    -32.0000,      1.7109,      3.5625,     -7.3750,    -11.0000,      3.8438,      1.3047,     33.5000,      8.5625,     -3.6562,     -1.2266],
        [    -5.1875,      6.2812,      6.4375,     -2.5469,    -16.8750,     -8.5000,      2.7812,      8.1875,     -8.8750,    -12.0000,     10.2500,      2.5312,     14.5625,     13.5625,     -4.2812,     13.6875],
        [    -1.6172,      6.2188,     -0.2285,     -3.3906,    -15.7500,    -24.6250,      2.1406,      0.7969,     -9.8125,    -13.1250,      5.1250,     -0.1045,     18.2500,    -13.3750,      1.4375,    -11.7500],
        [     0.1270,      0.7773,      0.9102,     -2.4375,     -9.7500,    -10.6250,      1.2109,     -0.6797,      0.6406,     -7.9375,      0.0312,      2.8125,     -0.4688,      4.7812,     -4.6562,     -9.0000],
        [    -2.7344,      1.2188,      4.9688,     -1.8125,    -12.5625,    -18.8750,      0.1143,      0.6953,     -1.7500,     -0.6562,      3.6250,     -2.0781,     18.7500,    -48.0000,     -4.0000,      4.8438],
        [    -1.9531,     -1.6875,     -2.1250,     -0.7734,     -9.2500,     -0.9531,     -1.7266,      7.9688,     -3.2812,     -8.6250,      4.6250,      0.5547,      3.4688,    -24.6250,     -9.7500,      8.4375],
        [     1.8828,      5.0000,     -1.5469,      0.8594,     -5.1250,    -21.0000,     -2.9844,     -2.1875,     -0.6562,    -11.0000,      1.0859,     -1.7734,     21.8750,      3.1406,      3.0625,      3.6094]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1172,      0.0583,     -0.8672,      1.0859,      0.7383,     -0.9102,      0.5781,     -1.3359,      0.9766,     -1.7578,     -1.0703,      1.1484,      2.2969,      0.1245,     -1.0469,      1.4141],
        [     0.4648,     -1.6094,      0.4258,      1.5078,     -2.2969,     -2.1719,      0.4082,     -1.8281,      4.8438,      0.1318,     -2.7031,      1.7656,      3.0156,      0.2754,     -0.8516,     -1.0547],
        [     2.4844,      0.8906,     -0.1455,      0.6758,      1.0391,     -0.4082,     -0.0334,     -1.1562,      4.3438,     -4.9375,      0.9141,      1.5000,     -0.4961,     -0.3359,      2.7031,      1.8047],
        [    -0.8945,     -3.0625,     -0.2314,      0.8594,     -1.2422,     -8.8125,     -1.6016,      0.0284,     -3.0000,     -8.3750,      1.9609,      1.1094,      6.2188,      0.1226,     -0.4590,     -4.6250],
        [    -0.2578,     -0.7109,      0.6680,     -1.8281,     -4.5938,     -8.6250,     -1.7812,      5.0312,     -2.9062,     -3.0469,      1.0469,      2.8281,      4.6562,     -0.2383,      2.6875,     -8.8125],
        [     3.1094,     -2.6250,     -1.6250,     -0.6094,     -1.4297,     -0.8750,     -0.9375,     -0.2754,      1.9922,     -1.0547,      2.6719,      3.3906,     -0.2119,      1.1797,      3.7812,     -6.7812],
        [     1.1562,      2.0625,      0.5312,     -2.5938,      2.4844,    -10.3750,     -1.9141,      2.5625,     10.5000,      0.6289,      0.6367,      4.1250,     -8.3125,      1.5625,      1.3516,      1.7188],
        [     3.8438,      0.9844,     -0.3750,     -3.5625,     -0.4160,     -3.6250,     -0.9141,      0.2578,     -3.0156,      2.9219,     -1.2422,      2.7031,      7.0000,     -0.0703,      1.2969,     -3.1094],
        [     1.4141,     -0.2852,     -0.5391,     -3.3125,     -3.7344,     -2.5781,     -2.7500,      5.0312,      0.0061,      6.0000,      2.8906,      1.2344,      3.3125,     -0.4707,     -0.8398,     -1.0703],
        [     1.2812,     -3.3125,     -1.8906,      1.3438,     -3.7812,      0.6016,      0.9883,     -1.2891,      0.4707,     -2.2656,     -0.0095,      1.5703,      7.8125,      0.9141,     -0.4980,     -5.7188]]
-------------------------
name='positions layer 8'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 8'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.1172,      0.0583,     -0.8672,      1.0859,      0.7383,     -0.9102,      0.5781,     -1.3359,      0.9766,     -1.7578,     -1.0703,      1.1484,      2.2969,      0.1245,     -1.0469,      1.4141],
        [     0.4648,     -1.6094,      0.4258,      1.5078,     -2.2969,     -2.1719,      0.4082,     -1.8281,      4.8438,      0.1318,     -2.7031,      1.7656,      3.0156,      0.2754,     -0.8516,     -1.0547],
        [     2.4844,      0.8906,     -0.1455,      0.6758,      1.0391,     -0.4082,     -0.0334,     -1.1562,      4.3438,     -4.9375,      0.9141,      1.5000,     -0.4961,     -0.3359,      2.7031,      1.8047],
        [    -0.8945,     -3.0625,     -0.2314,      0.8594,     -1.2422,     -8.8125,     -1.6016,      0.0284,     -3.0000,     -8.3750,      1.9609,      1.1094,      6.2188,      0.1226,     -0.4590,     -4.6250],
        [    -0.2578,     -0.7109,      0.6680,     -1.8281,     -4.5938,     -8.6250,     -1.7812,      5.0312,     -2.9062,     -3.0469,      1.0469,      2.8281,      4.6562,     -0.2383,      2.6875,     -8.8125],
        [     3.1094,     -2.6250,     -1.6250,     -0.6094,     -1.4297,     -0.8750,     -0.9375,     -0.2754,      1.9922,     -1.0547,      2.6719,      3.3906,     -0.2119,      1.1797,      3.7812,     -6.7812],
        [     1.1562,      2.0625,      0.5312,     -2.5938,      2.4844,    -10.3750,     -1.9141,      2.5625,     10.5000,      0.6289,      0.6367,      4.1250,     -8.3125,      1.5625,      1.3516,      1.7188],
        [     3.8438,      0.9844,     -0.3750,     -3.5625,     -0.4160,     -3.6250,     -0.9141,      0.2578,     -3.0156,      2.9219,     -1.2422,      2.7031,      7.0000,     -0.0703,      1.2969,     -3.1094],
        [     1.4141,     -0.2852,     -0.5391,     -3.3125,     -3.7344,     -2.5781,     -2.7500,      5.0312,      0.0061,      6.0000,      2.8906,      1.2344,      3.3125,     -0.4707,     -0.8398,     -1.0703],
        [     1.2812,     -3.3125,     -1.8906,      1.3438,     -3.7812,      0.6016,      0.9883,     -1.2891,      0.4707,     -2.2656,     -0.0095,      1.5703,      7.8125,      0.9141,     -0.4980,     -5.7188]]
name='residual layer 8'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.4375,     14.6250,    -19.3750,     35.5000,      8.1250,    -37.7500,     -4.0000,      2.0156,      7.9375,     -9.7500,     10.5000,      9.6875,    -10.9375,     18.0000,    -22.5000,    -17.7500],
        [     4.9062,     -2.0781,      2.6719,     -2.0625,     -6.5938,    -43.2500,      0.9648,     -0.1777,    -16.6250,    -12.6250,     -1.4844,      5.9062,     32.2500,      3.0781,      5.4688,     -4.7812],
        [     5.9062,      4.1562,      4.0938,     -1.4531,     -6.1250,    -51.5000,      1.3203,     -0.1328,     -9.6250,     -9.1875,     -1.4844,      1.6641,     40.5000,    -20.0000,      0.3047,     -0.5938],
        [    -4.8125,      7.8750,      6.8438,      3.3281,      0.5391,    -32.0000,      1.7109,      3.5625,     -7.3750,    -11.0000,      3.8438,      1.3047,     33.5000,      8.5625,     -3.6562,     -1.2266],
        [    -5.1875,      6.2812,      6.4375,     -2.5469,    -16.8750,     -8.5000,      2.7812,      8.1875,     -8.8750,    -12.0000,     10.2500,      2.5312,     14.5625,     13.5625,     -4.2812,     13.6875],
        [    -1.6172,      6.2188,     -0.2285,     -3.3906,    -15.7500,    -24.6250,      2.1406,      0.7969,     -9.8125,    -13.1250,      5.1250,     -0.1045,     18.2500,    -13.3750,      1.4375,    -11.7500],
        [     0.1270,      0.7773,      0.9102,     -2.4375,     -9.7500,    -10.6250,      1.2109,     -0.6797,      0.6406,     -7.9375,      0.0312,      2.8125,     -0.4688,      4.7812,     -4.6562,     -9.0000],
        [    -2.7344,      1.2188,      4.9688,     -1.8125,    -12.5625,    -18.8750,      0.1143,      0.6953,     -1.7500,     -0.6562,      3.6250,     -2.0781,     18.7500,    -48.0000,     -4.0000,      4.8438],
        [    -1.9531,     -1.6875,     -2.1250,     -0.7734,     -9.2500,     -0.9531,     -1.7266,      7.9688,     -3.2812,     -8.6250,      4.6250,      0.5547,      3.4688,    -24.6250,     -9.7500,      8.4375],
        [     1.8828,      5.0000,     -1.5469,      0.8594,     -5.1250,    -21.0000,     -2.9844,     -2.1875,     -0.6562,    -11.0000,      1.0859,     -1.7734,     21.8750,      3.1406,      3.0625,      3.6094]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0105,      0.0282,     -0.0439,      0.0732,      0.0110,     -0.0449,     -0.0081,      0.0013,      0.0112,     -0.0136,      0.0189,      0.0210,     -0.0108,      0.0236,     -0.0388,     -0.0173],
        [     0.5352,     -0.3066,      0.2891,     -0.0479,     -0.4766,     -2.2969,      0.1396,     -0.1719,     -0.6406,     -0.6406,     -0.3633,      0.6406,      1.8984,      0.1885,      0.3301,     -0.2676],
        [     0.8906,      0.4492,      0.3965,     -0.0718,     -0.2930,     -2.7969,      0.1406,     -0.1177,     -0.3066,     -0.7734,     -0.0530,      0.2852,      2.3125,     -1.2188,      0.2295,      0.0593],
        [    -0.6250,      0.4395,      0.6836,      0.4004,     -0.0417,     -2.2656,      0.0124,      0.3379,     -0.6211,     -1.0938,      0.5547,      0.2236,      2.3594,      0.5391,     -0.3223,     -0.2969],
        [    -0.4922,      0.4219,      0.6055,     -0.3457,     -1.0469,     -0.7852,      0.0933,      1.0234,     -0.5820,     -0.7031,      0.8945,      0.4082,      0.9453,      0.6836,     -0.1030,      0.2031],
        [     0.1562,      0.3125,     -0.1826,     -0.3633,     -0.9727,     -1.3516,      0.1289,      0.0471,     -0.4453,     -0.7617,      0.7109,      0.2910,      1.0234,     -0.7227,      0.3926,     -0.8945],
        [     0.1465,      0.2715,      0.1553,     -0.5000,     -0.4512,     -1.2188,     -0.0830,      0.1855,      0.6953,     -0.4297,      0.0669,      0.6680,     -0.5469,      0.4102,     -0.2715,     -0.3848],
        [     0.1338,      0.2227,      0.5234,     -0.5664,     -0.8516,     -1.3828,     -0.1001,      0.0991,     -0.3145,      0.1406,      0.2520,      0.0635,      1.6953,     -3.2812,     -0.2344,      0.0967],
        [    -0.0547,     -0.1670,     -0.2559,     -0.3594,     -0.7148,     -0.1816,     -0.4668,      1.1328,     -0.1807,     -0.1377,      0.6641,      0.1533,      0.3730,     -1.4375,     -0.7695,      0.3438],
        [     0.3574,      0.1602,     -0.3672,      0.2168,     -0.5469,     -1.1719,     -0.2334,     -0.3398,     -0.0115,     -0.7734,      0.1064,     -0.0194,      1.8281,      0.2598,      0.2090,     -0.1099]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  4.5625,  14.6875, -20.2500,  36.5000,   8.8750, -38.7500,  -3.4219,   0.6797,   8.9375, -11.5000,   9.4375,  10.8125,  -8.6250,  18.1250, -23.5000, -16.3750],
        [  5.3750,  -3.6875,   3.0938,  -0.5547,  -8.8750, -45.5000,   1.3750,  -2.0000, -11.7500, -12.5000,  -4.1875,   7.6875,  35.2500,   3.3594,   4.6250,  -5.8438],
        [  8.3750,   5.0625,   3.9531,  -0.7773,  -5.0938, -52.0000,   1.2891,  -1.2891,  -5.2812, -14.1250,  -0.5703,   3.1562,  40.0000, -20.3750,   3.0000,   1.2109],
        [ -5.7188,   4.8125,   6.6250,   4.1875,  -0.7031, -40.7500,   0.1094,   3.5938, -10.3750, -19.3750,   5.8125,   2.4062,  39.7500,   8.6875,  -4.1250,  -5.8438],
        [ -5.4375,   5.5625,   7.0938,  -4.3750, -21.5000, -17.1250,   1.0000,  13.2500, -11.7500, -15.0625,  11.3125,   5.3750,  19.2500,  13.3125,  -1.5938,   4.8750],
        [  1.4922,   3.5938,  -1.8516,  -4.0000, -17.1250, -25.5000,   1.2031,   0.5234,  -7.8125, -14.1875,   7.8125,   3.2812,  18.0000, -12.1875,   5.2188, -18.5000],
        [  1.2812,   2.8438,   1.4375,  -5.0312,  -7.2500, -21.0000,  -0.7031,   1.8828,  11.1250,  -7.3125,   0.6680,   6.9375,  -8.7500,   6.3438,  -3.3125,  -7.2812],
        [  1.1094,   2.2031,   4.5938,  -5.3750, -13.0000, -22.5000,  -0.8008,   0.9531,  -4.7500,   2.2656,   2.3750,   0.6250,  25.7500, -48.0000,  -2.7031,   1.7344],
        [ -0.5391,  -1.9688,  -2.6562,  -4.0938, -13.0000,  -3.5312,  -4.4688,  13.0000,  -3.2812,  -2.6250,   7.5000,   1.7891,   6.7812, -25.1250, -10.5625,   7.3750],
        [  3.1562,   1.6875,  -3.4375,   2.2031,  -8.8750, -20.3750,  -2.0000,  -3.4688,  -0.1855, -13.2500,   1.0781,  -0.2031,  29.7500,   4.0625,   2.5625,  -2.1094]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -7.4062,      8.9375,      3.2188,     -3.4375,      4.8125,    -23.8750,      1.7109,     -9.1250,     -3.9062,     -6.4375,      7.0625,     -1.4688,     10.7500,      1.6797,     11.5000,     -1.5469],
        [    -1.1719,      0.8711,      0.4785,     -1.3438,      1.6484,      0.6016,      0.6602,     -1.1250,      1.7734,     -0.2930,      0.0069,      0.5820,      0.8516,     -0.1963,     -0.4453,     -0.3730],
        [    -1.6094,      3.0156,      0.8203,     -2.2969,      1.5547,     -1.5000,      0.4023,     -1.4844,      1.7266,     -0.8984,     -2.3438,      0.9727,      2.0625,     -0.3711,      0.3984,     -0.9492],
        [    -0.2578,      3.2500,      0.5352,      0.2754,      0.4629,     -2.1406,      0.0928,     -1.6172,      1.9219,      0.0869,      0.2139,      0.5312,      3.2812,      0.0010,      2.0156,      0.6055],
        [     0.6094,      3.3906,     -0.9453,     -4.8750,     -6.2812,     -1.8828,     -1.1562,      0.5430,      1.9922,      1.1797,      0.9102,      1.0781,      2.1875,      0.0830,     -0.1133,     -0.0126],
        [     2.2812,      3.1250,      1.0156,     -7.0625,     -9.2500,     -2.5625,     -0.6016,      2.4062,      0.9141,     -0.5352,      2.9375,      0.9102,      4.1250,      0.1631,     -1.9922,     -4.4688],
        [     3.5938,      4.3125,     -1.7969,     -5.8438,     -9.7500,     -2.4219,     -0.4980,      2.1875,      1.5547,     -2.9844,      2.0781,      3.7500,      4.7500,      0.1816,     -1.3594,     -3.2969],
        [     3.3594,      1.9766,     -0.8008,     -0.6328,     -4.7188,      0.3184,     -1.1953,      3.0312,      0.4961,     -2.2500,      1.0312,     -0.8633,      4.5312,     -0.1377,      1.8281,     -2.5469],
        [     4.8750,      2.7969,     -1.2031,     -3.4062,     -3.6562,     -2.8594,      0.1621,      1.4453,      1.5000,     -1.2578,      0.5273,      1.2969,      2.7344,      0.0918,      1.9531,     -0.1099],
        [     1.9609,      2.5469,      1.4922,     -1.9297,     -2.2812,      2.6094,     -3.0938,      2.0469,     -0.4648,     -0.6328,      0.3789,     -0.8281,      3.0625,      0.0322,     -3.5781,     -0.2930]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0073,      0.0405,     -0.0476,      0.0820,      0.0153,     -0.0342,     -0.0052,     -0.0186,      0.0045,     -0.0209,      0.0342,      0.0201,      0.0013,      0.0173,     -0.0238,     -0.0183],
        [     0.5312,     -0.2373,      0.4883,     -0.2305,     -0.3984,     -1.2031,      0.3047,     -0.3379,     -0.4453,     -0.7305,     -0.4277,      0.8789,      1.0547,      0.1367,      0.4062,     -0.3125],
        [     0.9102,      0.7227,      0.6953,     -0.3984,     -0.2080,     -1.5234,      0.2715,     -0.3184,     -0.1689,     -0.9141,     -0.3164,      0.4668,      1.3047,     -0.9492,      0.3516,      0.0140],
        [    -0.7930,      0.7070,      1.0312,      0.5703,     -0.0139,     -1.2031,      0.0320,      0.2246,     -0.3945,     -1.1562,      0.6445,      0.3262,      1.3125,      0.3926,     -0.2158,     -0.2773],
        [    -0.5508,      0.6836,      0.7617,     -1.0156,     -1.3828,     -0.4609,     -0.0212,      1.3516,     -0.3906,     -0.7188,      1.1328,      0.6211,      0.5625,      0.5195,     -0.1504,      0.2217],
        [     0.5312,      0.6328,     -0.1279,     -1.5078,     -1.6172,     -0.8398,      0.1016,      0.3535,     -0.3438,     -0.9414,      1.2344,      0.4961,      0.7188,     -0.5781,      0.3516,     -1.2969],
        [     0.7461,      0.7305,     -0.0598,     -1.6016,     -1.1328,     -0.7617,     -0.2197,      0.5312,      0.6836,     -0.7148,      0.3398,      1.3750,     -0.1406,      0.3418,     -0.5508,     -0.6484],
        [     0.6797,      0.4258,      0.6289,     -0.8828,     -1.1719,     -0.7188,     -0.3613,      0.5195,     -0.2295,      0.0011,      0.4219,     -0.0306,      1.0625,     -2.5000,     -0.1025,     -0.0493],
        [     0.5547,      0.0703,     -0.5352,     -0.9219,     -0.9258,     -0.1729,     -0.6562,      1.5781,     -0.0801,     -0.2246,      0.8281,      0.3320,      0.2793,     -1.0938,     -0.8516,      0.3711],
        [     0.7031,      0.3906,     -0.2910,      0.0364,     -0.6719,     -0.5195,     -0.8398,     -0.1680,     -0.0315,     -0.8672,      0.1631,     -0.1201,      1.0469,      0.1934,     -0.1079,     -0.1328]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -2.8438,     23.6250,    -17.0000,     33.0000,     13.6875,    -62.5000,     -1.7109,     -8.4375,      5.0312,    -18.0000,     16.5000,      9.3750,      2.1250,     19.7500,    -12.0000,    -17.8750],
        [     4.1875,     -2.8125,      3.5781,     -1.8984,     -7.2188,    -45.0000,      2.0312,     -3.1250,    -10.0000,    -12.8125,     -4.1875,      8.2500,     36.0000,      3.1562,      4.1875,     -6.2188],
        [     6.7500,      8.0625,      4.7812,     -3.0781,     -3.5312,    -53.5000,      1.6875,     -2.7812,     -3.5625,    -15.0000,     -2.9062,      4.1250,     42.0000,    -20.7500,      3.4062,      0.2617],
        [    -5.9688,      8.0625,      7.1562,      4.4688,     -0.2402,    -43.0000,      0.2021,      1.9766,     -8.4375,    -19.2500,      6.0312,      2.9375,     43.0000,      8.6875,     -2.1094,     -5.2500],
        [    -4.8125,      8.9375,      6.1562,     -9.2500,    -27.7500,    -19.0000,     -0.1562,     13.8125,     -9.7500,    -13.8750,     12.2500,      6.4375,     21.5000,     13.3750,     -1.7031,      4.8750],
        [     3.7812,      6.7188,     -0.8359,    -11.0625,    -26.3750,    -28.0000,      0.6016,      2.9375,     -6.9062,    -14.7500,     10.7500,      4.1875,     22.1250,    -12.0000,      3.2188,    -23.0000],
        [     4.8750,      7.1562,     -0.3594,    -10.8750,    -17.0000,    -23.3750,     -1.2031,      4.0625,     12.6875,    -10.3125,      2.7500,     10.6875,     -4.0000,      6.5312,     -4.6875,    -10.5625],
        [     4.4688,      4.1875,      3.7969,     -6.0000,    -17.7500,    -22.1250,     -2.0000,      3.9844,     -4.2500,      0.0156,      3.4062,     -0.2383,     30.2500,    -48.2500,     -0.8750,     -0.8125],
        [     4.3438,      0.8281,     -3.8594,     -7.5000,    -16.6250,     -6.3750,     -4.3125,     14.4375,     -1.7812,     -3.8750,      8.0000,      3.0938,      9.5000,    -25.0000,     -8.6250,      7.2500],
        [     5.1250,      4.2500,     -1.9453,      0.2734,    -11.1250,    -17.7500,     -5.0938,     -1.4219,     -0.6484,    -13.8750,      1.4531,     -1.0312,     32.7500,      4.0938,     -1.0156,     -2.4062]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -4.9375,    -11.1250,      7.9062,    -13.6875,      5.0938,      2.5000,     -6.4375,     -7.7188,     -4.9375,      1.8672,     12.0625,     20.0000,      6.1562,     -0.5312,      4.9375,     -8.4375],
        [     3.9844,      4.7500,     -1.3359,      3.6406,     -2.8125,    -13.1875,      1.5625,     -0.1826,      2.0000,     -3.7969,      3.3438,      1.4922,      3.2969,      0.3320,      0.9180,      0.2021],
        [     3.5312,      5.9375,     -1.9844,      5.2812,     -0.4785,     -6.3750,      2.1719,     -0.9492,      0.3965,     -0.4746,     -0.4316,      2.8281,      1.0625,      0.5391,      2.3281,     -4.5312],
        [    -1.1875,      3.2188,      0.9531,      2.3750,     -4.3125,     -7.1250,     -0.4805,      0.9648,     -0.9727,     -4.4062,      1.9688,     -2.7031,      4.1875,      0.5039,      0.5078,     -0.3438],
        [     3.0000,     -1.9141,     -1.4219,      2.8594,     -8.7500,     -7.4062,      4.0312,      2.5000,     -3.7656,     -3.0625,      4.3125,      2.5469,      1.9922,      0.0981,     -2.2344,     -4.0625],
        [     6.3438,      3.6406,     -3.2656,      7.9062,     -5.9375,     -1.9688,      2.7812,     -0.1123,     -2.8594,     -5.1562,      8.4375,      3.4531,     -3.8125,      0.5156,      3.0000,     -7.8438],
        [     3.2500,     -1.3516,     -1.0000,      0.7734,     -9.1875,      0.6953,      1.3438,      1.1797,     -0.0121,     -1.4297,      1.6562,      1.5078,     -5.7500,     -0.0771,      4.4375,     -4.5312],
        [     0.8867,     -4.0312,     -2.3594,      3.6875,      2.4375,     -1.6172,      1.7734,     -1.7969,      1.6719,      0.6289,      3.1250,      2.9062,      8.1875,     -0.1455,     -0.1172,     -2.7500],
        [    -0.2363,     -2.7969,     -2.1875,     -0.5391,      8.7500,     -1.6406,      0.2617,     -4.2812,     -1.6484,      1.4609,      3.6406,     -3.2500,      0.2227,      0.3145,     -3.7344,      2.3125],
        [     2.2656,      0.7969,     -1.2656,      1.3828,     -5.5312,     -3.7969,     -0.4277,     -1.2031,     -4.6875,      1.3359,      0.7891,     -2.1406,      3.0781,     -0.3750,     -0.6992,      3.7812]]
-------------------------
name='positions layer 9'       | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 9'               | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -4.9375,    -11.1250,      7.9062,    -13.6875,      5.0938,      2.5000,     -6.4375,     -7.7188,     -4.9375,      1.8672,     12.0625,     20.0000,      6.1562,     -0.5312,      4.9375,     -8.4375],
        [     3.9844,      4.7500,     -1.3359,      3.6406,     -2.8125,    -13.1875,      1.5625,     -0.1826,      2.0000,     -3.7969,      3.3438,      1.4922,      3.2969,      0.3320,      0.9180,      0.2021],
        [     3.5312,      5.9375,     -1.9844,      5.2812,     -0.4785,     -6.3750,      2.1719,     -0.9492,      0.3965,     -0.4746,     -0.4316,      2.8281,      1.0625,      0.5391,      2.3281,     -4.5312],
        [    -1.1875,      3.2188,      0.9531,      2.3750,     -4.3125,     -7.1250,     -0.4805,      0.9648,     -0.9727,     -4.4062,      1.9688,     -2.7031,      4.1875,      0.5039,      0.5078,     -0.3438],
        [     3.0000,     -1.9141,     -1.4219,      2.8594,     -8.7500,     -7.4062,      4.0312,      2.5000,     -3.7656,     -3.0625,      4.3125,      2.5469,      1.9922,      0.0981,     -2.2344,     -4.0625],
        [     6.3438,      3.6406,     -3.2656,      7.9062,     -5.9375,     -1.9688,      2.7812,     -0.1123,     -2.8594,     -5.1562,      8.4375,      3.4531,     -3.8125,      0.5156,      3.0000,     -7.8438],
        [     3.2500,     -1.3516,     -1.0000,      0.7734,     -9.1875,      0.6953,      1.3438,      1.1797,     -0.0121,     -1.4297,      1.6562,      1.5078,     -5.7500,     -0.0771,      4.4375,     -4.5312],
        [     0.8867,     -4.0312,     -2.3594,      3.6875,      2.4375,     -1.6172,      1.7734,     -1.7969,      1.6719,      0.6289,      3.1250,      2.9062,      8.1875,     -0.1455,     -0.1172,     -2.7500],
        [    -0.2363,     -2.7969,     -2.1875,     -0.5391,      8.7500,     -1.6406,      0.2617,     -4.2812,     -1.6484,      1.4609,      3.6406,     -3.2500,      0.2227,      0.3145,     -3.7344,      2.3125],
        [     2.2656,      0.7969,     -1.2656,      1.3828,     -5.5312,     -3.7969,     -0.4277,     -1.2031,     -4.6875,      1.3359,      0.7891,     -2.1406,      3.0781,     -0.3750,     -0.6992,      3.7812]]
name='residual layer 9'        | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -2.8438,     23.6250,    -17.0000,     33.0000,     13.6875,    -62.5000,     -1.7109,     -8.4375,      5.0312,    -18.0000,     16.5000,      9.3750,      2.1250,     19.7500,    -12.0000,    -17.8750],
        [     4.1875,     -2.8125,      3.5781,     -1.8984,     -7.2188,    -45.0000,      2.0312,     -3.1250,    -10.0000,    -12.8125,     -4.1875,      8.2500,     36.0000,      3.1562,      4.1875,     -6.2188],
        [     6.7500,      8.0625,      4.7812,     -3.0781,     -3.5312,    -53.5000,      1.6875,     -2.7812,     -3.5625,    -15.0000,     -2.9062,      4.1250,     42.0000,    -20.7500,      3.4062,      0.2617],
        [    -5.9688,      8.0625,      7.1562,      4.4688,     -0.2402,    -43.0000,      0.2021,      1.9766,     -8.4375,    -19.2500,      6.0312,      2.9375,     43.0000,      8.6875,     -2.1094,     -5.2500],
        [    -4.8125,      8.9375,      6.1562,     -9.2500,    -27.7500,    -19.0000,     -0.1562,     13.8125,     -9.7500,    -13.8750,     12.2500,      6.4375,     21.5000,     13.3750,     -1.7031,      4.8750],
        [     3.7812,      6.7188,     -0.8359,    -11.0625,    -26.3750,    -28.0000,      0.6016,      2.9375,     -6.9062,    -14.7500,     10.7500,      4.1875,     22.1250,    -12.0000,      3.2188,    -23.0000],
        [     4.8750,      7.1562,     -0.3594,    -10.8750,    -17.0000,    -23.3750,     -1.2031,      4.0625,     12.6875,    -10.3125,      2.7500,     10.6875,     -4.0000,      6.5312,     -4.6875,    -10.5625],
        [     4.4688,      4.1875,      3.7969,     -6.0000,    -17.7500,    -22.1250,     -2.0000,      3.9844,     -4.2500,      0.0156,      3.4062,     -0.2383,     30.2500,    -48.2500,     -0.8750,     -0.8125],
        [     4.3438,      0.8281,     -3.8594,     -7.5000,    -16.6250,     -6.3750,     -4.3125,     14.4375,     -1.7812,     -3.8750,      8.0000,      3.0938,      9.5000,    -25.0000,     -8.6250,      7.2500],
        [     5.1250,      4.2500,     -1.9453,      0.2734,    -11.1250,    -17.7500,     -5.0938,     -1.4219,     -0.6484,    -13.8750,      1.4531,     -1.0312,     32.7500,      4.0938,     -1.0156,     -2.4062]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0168,      0.0242,     -0.0211,      0.0378,      0.0293,     -0.0977,     -0.0204,     -0.0317,      0.0001,     -0.0245,      0.0583,      0.0549,      0.0154,      0.0311,     -0.0129,     -0.0381],
        [     0.7070,      0.1494,      0.2080,      0.1357,     -0.6250,     -3.7812,      0.3594,     -0.2598,     -0.4688,     -1.0078,     -0.0688,      0.7266,      2.9062,      0.2256,      0.3691,     -0.3477],
        [     0.9375,      1.1406,      0.2754,      0.1826,     -0.2637,     -4.1250,      0.4082,     -0.3086,     -0.1963,     -0.9961,     -0.2871,      0.5508,      3.3906,     -1.3828,      0.4414,     -0.2617],
        [    -0.6914,      0.9727,      0.8438,      0.5977,     -0.3164,     -3.6406,     -0.0311,      0.2578,     -0.6211,     -1.6016,      0.7305,      0.0197,      3.9219,      0.6641,     -0.1309,     -0.3633],
        [    -0.1514,      0.5234,      0.4258,     -0.4844,     -2.2031,     -1.6641,      0.3770,      1.2344,     -0.7656,     -0.9961,      1.3047,      0.6484,      1.6953,      0.8438,     -0.2773,      0.0457],
        [     0.8867,      0.8086,     -0.3867,     -0.2500,     -2.0312,     -1.9766,      0.3418,      0.2246,     -0.5781,     -1.2266,      1.5859,      0.5781,      1.3750,     -0.7500,      0.4590,     -1.8125],
        [     0.8008,      0.5078,     -0.1436,     -0.9062,     -1.8672,     -1.6797,      0.0160,      0.4668,      0.8477,     -0.8125,      0.4082,      1.0391,     -0.8242,      0.4766,     -0.0208,     -0.9961],
        [     0.5625,      0.0145,      0.1611,     -0.2188,     -1.1562,     -1.8672,     -0.0275,      0.2070,     -0.1826,      0.0471,      0.6445,      0.2412,      3.4531,     -3.7969,     -0.0874,     -0.2490],
        [     0.3691,     -0.1572,     -0.5820,     -0.6562,     -0.5078,     -0.5430,     -0.4219,      0.8281,     -0.2090,     -0.1523,      0.9844,     -0.0121,      0.7500,     -1.6641,     -0.9336,      0.5742],
        [     0.6836,      0.4160,     -0.3164,      0.1387,     -1.1094,     -1.4922,     -0.5898,     -0.2188,     -0.3340,     -0.8125,      0.1953,     -0.2520,      2.8438,      0.2559,     -0.1328,      0.0845]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -7.7812,  12.5000,  -9.1250,  19.2500,  18.7500, -60.0000,  -8.1250, -16.1250,   0.0938, -16.1250,  28.5000,  29.3750,   8.2500,  19.2500,  -7.0625, -26.2500],
        [  8.1875,   1.9375,   2.2500,   1.7422, -10.0000, -58.2500,   3.5938,  -3.3125,  -8.0000, -16.6250,  -0.8438,   9.7500,  39.2500,   3.4844,   5.0938,  -6.0312],
        [ 10.2500,  14.0000,   2.7969,   2.2031,  -4.0000, -60.0000,   3.8594,  -3.7344,  -3.1719, -15.5000,  -3.3438,   6.9375,  43.0000, -20.2500,   5.7500,  -4.2812],
        [ -7.1562,  11.2500,   8.1250,   6.8438,  -4.5625, -50.0000,  -0.2773,   2.9375,  -9.4375, -23.6250,   8.0000,   0.2344,  47.2500,   9.1875,  -1.6016,  -5.5938],
        [ -1.8125,   7.0312,   4.7500,  -6.3750, -36.5000, -26.3750,   3.8750,  16.2500, -13.5000, -17.0000,  16.5000,   9.0000,  23.5000,  13.5000,  -3.9375,   0.8125],
        [ 10.1250,  10.3750,  -4.0938,  -3.1562, -32.2500, -30.0000,   3.3750,   2.8281,  -9.7500, -19.8750,  19.2500,   7.6250,  18.2500, -11.5000,   6.2188, -30.8750],
        [  8.1250,   5.8125,  -1.3594, -10.1250, -26.2500, -22.6250,   0.1406,   5.2500,  12.6875, -11.7500,   4.4062,  12.1875,  -9.7500,   6.4688,  -0.2500, -15.1250],
        [  5.3438,   0.1562,   1.4375,  -2.3125, -15.3125, -23.7500,  -0.2266,   2.1875,  -2.5781,   0.6445,   6.5312,   2.6719,  38.5000, -48.5000,  -0.9922,  -3.5625],
        [  4.0938,  -1.9688,  -6.0625,  -8.0625,  -7.8750,  -8.0000,  -4.0625,  10.1250,  -3.4375,  -2.4062,  11.6250,  -0.1562,   9.7500, -24.6250, -12.3750,   9.5625],
        [  7.3750,   5.0625,  -3.2188,   1.6562, -16.6250, -21.5000,  -5.5312,  -2.6250,  -5.3438, -12.5625,   2.2500,  -3.1719,  35.7500,   3.7188,  -1.7188,   1.3750]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -1.4766,      4.1875,      1.6328,      1.8984,      0.4199,     -0.7070,     -1.9375,     -2.6406,      1.2891,     -0.0664,     -1.2891,     -0.1196,      5.0312,     -0.1777,      2.9844,      0.0114],
        [     1.4609,      1.3750,     -0.0928,     -0.8008,     -0.7148,     -2.2656,      1.4766,     -0.9062,     -0.0405,      1.4141,     -1.0547,      1.0625,      2.9688,      0.1553,     -0.7109,      0.6953],
        [     2.7031,      0.2500,     -1.7891,     -1.7266,     -2.6406,     -1.4453,      2.6250,     -1.9453,     -0.2773,      0.9492,     -3.7656,      0.6172,     -3.3281,      0.3516,      0.8047,     -0.8555],
        [     2.0469,      2.1094,     -0.9883,     -0.3887,     -1.5938,     -1.3125,     -0.5820,     -2.2656,     -3.7344,     -1.3594,     -1.4609,      2.0781,      0.2617,      0.3418,      0.0099,     -2.7969],
        [     1.5234,      3.3125,     -2.0469,     -2.3750,     -0.3379,     -5.8438,      2.6719,      1.6875,     -8.8125,     -0.1279,      0.8672,      0.3477,      0.1572,     -0.0972,     -0.2480,     -0.6445],
        [     2.5312,      1.3047,     -1.3438,     -1.9766,     -1.1328,     -5.0000,      2.5625,      1.5781,     -5.8438,     -1.9297,     -0.0135,      0.4277,     -1.7422,     -0.2021,      1.4922,     -0.4414],
        [     3.5469,      1.7344,     -1.2031,     -1.7109,     -0.1982,     -1.1875,     -0.3750,      1.7656,     -3.4844,     -2.3125,      1.7109,     -0.7617,      1.5469,     -0.0294,      1.0703,      1.0625],
        [     4.3750,     -0.3457,     -0.8438,     -1.5469,     -0.2188,     -2.5625,     -0.1709,      2.5625,     -1.6797,      4.4688,      1.6250,     -1.2266,      2.0938,     -0.0840,      0.6094,      0.1436],
        [     3.6562,      0.4707,     -0.3945,     -0.6562,      0.3086,     -5.9688,     -0.7695,      3.2344,     -2.6562,      3.0000,     -0.4121,     -2.4219,      1.7578,     -0.0002,      2.6719,      1.5625],
        [     1.2266,     -0.2656,      0.4844,     -1.0312,     -2.3594,     -7.0000,      2.3906,      1.7812,     -1.8594,      2.5938,      2.1406,     -1.7656,      0.9219,     -0.2451,      3.4375,     -0.6836]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0219,      0.0294,     -0.0205,      0.0503,      0.0221,     -0.0374,     -0.0287,     -0.0383,      0.0013,     -0.0195,      0.0525,      0.0586,      0.0085,      0.0150,     -0.0081,     -0.0276],
        [     0.9727,      0.2480,      0.2520,      0.0957,     -0.5273,     -1.5938,      0.6172,     -0.3691,     -0.3125,     -0.7891,     -0.1562,      0.9258,      1.1484,      0.1226,      0.3730,     -0.2393],
        [     1.4141,      1.1484,      0.1270,      0.0525,     -0.3535,     -1.7422,      0.8516,     -0.5352,     -0.1445,     -0.8125,     -0.6328,      0.6992,      1.1641,     -0.7227,      0.6016,     -0.2490],
        [    -0.5781,      1.1250,      0.9297,      0.7344,     -0.3398,     -1.5078,     -0.1172,      0.0654,     -0.5742,     -1.4531,      0.6016,      0.2207,      1.4531,      0.3594,     -0.1514,     -0.4219],
        [    -0.0288,      0.7656,      0.3105,     -0.8789,     -1.7812,     -0.8359,      0.7812,      1.5391,     -0.8516,     -0.8750,      1.4141,      0.7852,      0.6367,      0.4453,     -0.3496,      0.0074],
        [     1.2891,      0.8867,     -0.6367,     -0.5273,     -1.6562,     -0.9297,      0.7266,      0.3867,     -0.6094,     -1.1406,      1.5938,      0.6953,      0.4551,     -0.3984,      0.6602,     -1.4141],
        [     1.3047,      0.6289,     -0.3320,     -1.3359,     -1.4453,     -0.6953,     -0.0315,      0.6797,      0.3945,     -0.8086,      0.5586,      1.0781,     -0.2490,      0.2402,      0.0776,     -0.7031],
        [     1.1953,     -0.0172,      0.0840,     -0.4746,     -0.9258,     -0.8359,     -0.0586,      0.5039,     -0.2002,      0.3203,      0.8164,      0.1494,      1.3438,     -1.9844,     -0.0396,     -0.1855],
        [     0.7969,     -0.1143,     -0.7656,     -0.9023,     -0.3789,     -0.3750,     -0.5977,      1.1875,     -0.2402,      0.0312,      0.9375,     -0.2236,      0.3203,     -0.8438,     -0.8398,      0.5078],
        [     0.9102,      0.3770,     -0.3340,      0.0664,     -0.9805,     -0.7852,     -0.4004,     -0.0771,     -0.2930,     -0.5430,      0.3789,     -0.4414,      1.0469,      0.1226,      0.1533,      0.0325]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -9.2500,  16.7500,  -7.5000,  21.1250,  19.1250, -60.7500, -10.0625, -18.7500,   1.3828, -16.2500,  27.2500,  29.2500,  13.2500,  19.1250,  -4.0625, -26.2500],
        [  9.6250,   3.3125,   2.1562,   0.9414, -10.6875, -60.5000,   5.0625,  -4.2188,  -8.0625, -15.1875,  -1.8984,  10.8125,  42.2500,   3.6406,   4.3750,  -5.3438],
        [ 12.9375,  14.2500,   1.0078,   0.4766,  -6.6250, -61.5000,   6.5000,  -5.6875,  -3.4531, -14.5625,  -7.1250,   7.5625,  39.7500, -19.8750,   6.5625,  -5.1250],
        [ -5.1250,  13.3750,   7.1250,   6.4688,  -6.1562, -51.2500,  -0.8594,   0.6719, -13.1875, -25.0000,   6.5312,   2.3125,  47.5000,   9.5000,  -1.5938,  -8.3750],
        [ -0.2891,  10.3750,   2.7031,  -8.7500, -36.7500, -32.2500,   6.5625,  18.0000, -22.2500, -17.1250,  17.3750,   9.3750,  23.6250,  13.3750,  -4.1875,   0.1680],
        [ 12.6250,  11.6875,  -5.4375,  -5.1250, -33.5000, -35.0000,   5.9375,   4.4062, -15.6250, -21.7500,  19.2500,   8.0625,  16.5000, -11.6875,   7.7188, -31.3750],
        [ 11.6875,   7.5625,  -2.5625, -11.8125, -26.5000, -23.7500,  -0.2344,   7.0000,   9.1875, -14.0625,   6.1250,  11.4375,  -8.1875,   6.4375,   0.8203, -14.0625],
        [  9.7500,  -0.1895,   0.5938,  -3.8594, -15.5000, -26.2500,  -0.3984,   4.7500,  -4.2500,   5.1250,   8.1250,   1.4453,  40.5000, -48.5000,  -0.3828,  -3.4219],
        [  7.7500,  -1.5000,  -6.4688,  -8.7500,  -7.5625, -14.0000,  -4.8438,  13.3750,  -6.0938,   0.5938,  11.1875,  -2.5781,  11.5000, -24.6250,  -9.6875,  11.1250],
        [  8.6250,   4.8125,  -2.7344,   0.6250, -19.0000, -28.5000,  -3.1406,  -0.8438,  -7.1875, -10.0000,   4.3750,  -4.9375,  36.7500,   3.4688,   1.7188,   0.6914]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -0.3340,   6.0625,   9.6875, -14.7500,  -3.7969,  21.2500, -17.2500,  -5.2812,   0.4883,   1.2266,  20.6250,  21.2500,   1.2578,  -1.1250,   3.8906, -16.6250],
        [  4.3438,   2.6406,  -2.7188,  -5.2500,  -5.2500,  -2.5156,   2.4844,   1.0156,   2.5312,   3.6875,   5.4375,   2.1094,   4.1875,  -0.0879,  -2.9375,   1.4453],
        [  0.8008,   7.0000,  -9.5625,  -5.4062,  -6.5938,  -6.0938,  -2.8281,   5.0000,  -7.2500,   1.2656,  -6.6250,   2.5312,   6.4375,   0.1157,   0.1099,   0.3848],
        [  0.0588,   1.9922,  -2.0000,   2.1094,   3.9062,  -0.5547,   0.0864,   4.4062,  -0.9922,  -7.3438,   2.6094,  -0.4805,   2.6094,  -0.2002,  -0.4766,   0.5820],
        [-10.6250,  -0.3496,  -1.7344,  -1.2969, -27.5000,  -4.0312,   4.7500,   1.5703,  -0.3105,  -1.3906,  14.5625,   7.9688,   2.4062,  -0.3730,  -0.9414,  -6.2500],
        [ -1.0625,  -3.8281,  -5.8438,  -3.6875, -23.0000,  -5.4375,   5.5625,  -0.2695,  -2.8438,  14.7500,  -2.7500,  -1.2812,   2.2812,  -0.4941,  -0.8008,  -3.6875],
        [  3.5000,   2.3438,  -3.3906,   0.9414,  -3.0781,  -0.1357,  -2.4531,  -2.9688,   1.5547,  -2.7656,   4.5938,   5.4375,  -1.9297,   0.1221,  -1.5703,   4.5312],
        [ -4.4062,  -2.5156,  -3.1250,   8.3125, -15.3750,  -1.8438,   7.8438,  -6.0625,  -0.9180,  12.7500,  15.4375,   7.5000,   7.0000,  -0.2480,   0.0659,  -2.8438],
        [ -2.9375,  -2.6094,  -6.1250,  -1.6641,   8.3750,   4.0938,   2.9688,  -1.7656,   1.2578,   0.8633,   1.5938,   1.7109,   0.0474,  -0.3340,   0.1079,  -4.6250],
        [ -2.5312,   3.2812,  -1.2578,  -6.9688,  -5.5000,   1.3047,   4.4062,  -2.7031,  -0.4883,  -0.6172,  -1.7109,   3.1250,   2.1875,  -0.1040,   2.5938,  -3.7500]]
-------------------------
name='positions layer 10'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 10'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -0.3340,   6.0625,   9.6875, -14.7500,  -3.7969,  21.2500, -17.2500,  -5.2812,   0.4883,   1.2266,  20.6250,  21.2500,   1.2578,  -1.1250,   3.8906, -16.6250],
        [  4.3438,   2.6406,  -2.7188,  -5.2500,  -5.2500,  -2.5156,   2.4844,   1.0156,   2.5312,   3.6875,   5.4375,   2.1094,   4.1875,  -0.0879,  -2.9375,   1.4453],
        [  0.8008,   7.0000,  -9.5625,  -5.4062,  -6.5938,  -6.0938,  -2.8281,   5.0000,  -7.2500,   1.2656,  -6.6250,   2.5312,   6.4375,   0.1157,   0.1099,   0.3848],
        [  0.0588,   1.9922,  -2.0000,   2.1094,   3.9062,  -0.5547,   0.0864,   4.4062,  -0.9922,  -7.3438,   2.6094,  -0.4805,   2.6094,  -0.2002,  -0.4766,   0.5820],
        [-10.6250,  -0.3496,  -1.7344,  -1.2969, -27.5000,  -4.0312,   4.7500,   1.5703,  -0.3105,  -1.3906,  14.5625,   7.9688,   2.4062,  -0.3730,  -0.9414,  -6.2500],
        [ -1.0625,  -3.8281,  -5.8438,  -3.6875, -23.0000,  -5.4375,   5.5625,  -0.2695,  -2.8438,  14.7500,  -2.7500,  -1.2812,   2.2812,  -0.4941,  -0.8008,  -3.6875],
        [  3.5000,   2.3438,  -3.3906,   0.9414,  -3.0781,  -0.1357,  -2.4531,  -2.9688,   1.5547,  -2.7656,   4.5938,   5.4375,  -1.9297,   0.1221,  -1.5703,   4.5312],
        [ -4.4062,  -2.5156,  -3.1250,   8.3125, -15.3750,  -1.8438,   7.8438,  -6.0625,  -0.9180,  12.7500,  15.4375,   7.5000,   7.0000,  -0.2480,   0.0659,  -2.8438],
        [ -2.9375,  -2.6094,  -6.1250,  -1.6641,   8.3750,   4.0938,   2.9688,  -1.7656,   1.2578,   0.8633,   1.5938,   1.7109,   0.0474,  -0.3340,   0.1079,  -4.6250],
        [ -2.5312,   3.2812,  -1.2578,  -6.9688,  -5.5000,   1.3047,   4.4062,  -2.7031,  -0.4883,  -0.6172,  -1.7109,   3.1250,   2.1875,  -0.1040,   2.5938,  -3.7500]]
name='residual layer 10'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -9.2500,  16.7500,  -7.5000,  21.1250,  19.1250, -60.7500, -10.0625, -18.7500,   1.3828, -16.2500,  27.2500,  29.2500,  13.2500,  19.1250,  -4.0625, -26.2500],
        [  9.6250,   3.3125,   2.1562,   0.9414, -10.6875, -60.5000,   5.0625,  -4.2188,  -8.0625, -15.1875,  -1.8984,  10.8125,  42.2500,   3.6406,   4.3750,  -5.3438],
        [ 12.9375,  14.2500,   1.0078,   0.4766,  -6.6250, -61.5000,   6.5000,  -5.6875,  -3.4531, -14.5625,  -7.1250,   7.5625,  39.7500, -19.8750,   6.5625,  -5.1250],
        [ -5.1250,  13.3750,   7.1250,   6.4688,  -6.1562, -51.2500,  -0.8594,   0.6719, -13.1875, -25.0000,   6.5312,   2.3125,  47.5000,   9.5000,  -1.5938,  -8.3750],
        [ -0.2891,  10.3750,   2.7031,  -8.7500, -36.7500, -32.2500,   6.5625,  18.0000, -22.2500, -17.1250,  17.3750,   9.3750,  23.6250,  13.3750,  -4.1875,   0.1680],
        [ 12.6250,  11.6875,  -5.4375,  -5.1250, -33.5000, -35.0000,   5.9375,   4.4062, -15.6250, -21.7500,  19.2500,   8.0625,  16.5000, -11.6875,   7.7188, -31.3750],
        [ 11.6875,   7.5625,  -2.5625, -11.8125, -26.5000, -23.7500,  -0.2344,   7.0000,   9.1875, -14.0625,   6.1250,  11.4375,  -8.1875,   6.4375,   0.8203, -14.0625],
        [  9.7500,  -0.1895,   0.5938,  -3.8594, -15.5000, -26.2500,  -0.3984,   4.7500,  -4.2500,   5.1250,   8.1250,   1.4453,  40.5000, -48.5000,  -0.3828,  -3.4219],
        [  7.7500,  -1.5000,  -6.4688,  -8.7500,  -7.5625, -14.0000,  -4.8438,  13.3750,  -6.0938,   0.5938,  11.1875,  -2.5781,  11.5000, -24.6250,  -9.6875,  11.1250],
        [  8.6250,   4.8125,  -2.7344,   0.6250, -19.0000, -28.5000,  -3.1406,  -0.8438,  -7.1875, -10.0000,   4.3750,  -4.9375,  36.7500,   3.4688,   1.7188,   0.6914]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0212,      0.0393,      0.0046,      0.0125,      0.0243,     -0.0781,     -0.0654,     -0.0500,      0.0029,     -0.0211,      0.0938,      0.0991,      0.0344,      0.0339,     -0.0004,     -0.0603],
        [     0.9102,      0.3027,     -0.0349,     -0.2490,     -0.7461,     -3.6875,      0.5352,     -0.1973,     -0.2559,     -0.4746,      0.2041,      0.7461,      3.2656,      0.1982,      0.0864,     -0.1611],
        [     0.8203,      0.9844,     -0.4824,     -0.2598,     -0.5625,     -3.5938,      0.2383,     -0.0386,     -0.4512,     -0.5039,     -0.7266,      0.5312,      2.9688,     -1.0078,      0.3672,     -0.1797],
        [    -0.3516,      0.8281,      0.3359,      0.5273,     -0.1113,     -3.2031,     -0.0581,      0.3320,     -0.6953,     -1.4141,      0.5586,      0.1123,      3.7500,      0.5508,     -0.1318,     -0.3418],
        [    -0.7266,      0.5234,      0.0610,     -0.5938,     -3.0625,     -2.1719,      0.8164,      1.2344,     -1.0625,     -0.7812,      1.8828,      1.0234,      1.8750,      0.7422,     -0.3145,     -0.2559],
        [     0.7734,      0.4102,     -0.7148,     -0.5195,     -2.7031,     -2.4219,      0.8359,      0.2598,     -0.8750,     -0.2969,      0.9766,      0.4004,      1.3516,     -0.6953,      0.4258,     -1.4844],
        [     1.0703,      0.5469,     -0.3984,     -0.6797,     -1.4922,     -1.5078,     -0.2061,      0.2676,      0.5352,     -0.7539,      0.6680,      1.0547,     -0.7695,      0.3965,     -0.0488,     -0.4258],
        [     0.3750,     -0.1484,     -0.1680,      0.2773,     -1.5547,     -1.7578,      0.5664,     -0.0869,     -0.2559,      0.7930,      1.4609,      0.5547,      3.5938,     -2.9219,     -0.0205,     -0.2773],
        [     0.3320,     -0.2217,     -0.8242,     -0.6367,      0.0400,     -0.6133,     -0.1406,      0.7578,     -0.2363,      0.0640,      0.7812,     -0.0530,      0.8555,     -1.4688,     -0.6094,      0.2852],
        [     0.4512,      0.4688,     -0.2812,     -0.4180,     -1.3047,     -1.8047,      0.1021,     -0.2480,     -0.4023,     -0.5000,      0.1748,     -0.1191,      3.1094,      0.2129,      0.2949,     -0.1445]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -9.5625,  22.7500,   2.1875,   6.3750,  15.3125, -39.5000, -27.2500, -24.0000,   1.8750, -15.0000,  48.0000,  50.5000,  14.5000,  18.0000,  -0.1719, -43.0000],
        [ 14.0000,   5.9375,  -0.5625,  -4.3125, -15.9375, -63.0000,   7.5625,  -3.2031,  -5.5312, -11.5000,   3.5312,  12.9375,  46.5000,   3.5469,   1.4375,  -3.9062],
        [ 13.7500,  21.2500,  -8.5625,  -4.9375, -13.2500, -67.5000,   3.6719,  -0.6875, -10.6875, -13.3125, -13.7500,  10.1250,  46.2500, -19.7500,   6.6875,  -4.7500],
        [ -5.0625,  15.3750,   5.1250,   8.5625,  -2.2500, -51.7500,  -0.7734,   5.0625, -14.1875, -32.2500,   9.1250,   1.8281,  50.0000,   9.3125,  -2.0625,  -7.7812],
        [-10.9375,  10.0000,   0.9688, -10.0625, -64.0000, -36.2500,  11.3125,  19.6250, -22.5000, -18.5000,  32.0000,  17.3750,  26.0000,  13.0000,  -5.1250,  -6.0938],
        [ 11.5625,   7.8750, -11.2500,  -8.8125, -56.5000, -40.5000,  11.5000,   4.1250, -18.5000,  -7.0000,  16.5000,   6.7812,  18.7500, -12.1875,   6.9062, -35.0000],
        [ 15.1875,   9.8750,  -5.9375, -10.8750, -29.6250, -23.8750,  -2.6875,   4.0312,  10.7500, -16.8750,  10.7500,  16.8750, -10.1250,   6.5625,  -0.7500,  -9.5000],
        [  5.3438,  -2.7031,  -2.5312,   4.4375, -30.8750, -28.1250,   7.4375,  -1.3125,  -5.1562,  17.8750,  23.5000,   8.9375,  47.5000, -48.7500,  -0.3164,  -6.2500],
        [  4.8125,  -4.1250, -12.6250, -10.4375,   0.8125,  -9.8750,  -1.8750,  11.6250,  -4.8438,   1.4531,  12.7500,  -0.8672,  11.5625, -25.0000,  -9.5625,   6.5000],
        [  6.0938,   8.1250,  -4.0000,  -6.3438, -24.5000, -27.2500,   1.2656,  -3.5469,  -7.6875, -10.6250,   2.6562,  -1.8125,  39.0000,   3.3594,   4.3125,  -3.0625]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     3.6562,     -2.4531,      3.4531,     -2.7656,     -0.3984,      2.7344,     -1.6172,      4.4688,      0.6953,      0.4082,     -0.4609,     -1.5469,     -0.9922,     -0.1631,      4.5625,      2.9688],
        [     2.7344,      1.8281,      1.1172,     -0.0635,      1.2188,     -0.5586,     -0.1729,     -1.8438,     -0.5156,      0.6836,      0.8242,     -0.1562,     -0.8281,     -0.1001,      1.8125,     -0.2754],
        [     0.7617,      0.9922,      1.9375,     -0.6445,      0.0835,     -2.0312,      2.0156,     -1.0703,     -0.5586,      0.4062,      3.5625,      1.1094,     -0.8008,     -0.0850,      1.6250,     -0.3242],
        [    -0.1592,      1.5469,      3.3750,     -2.5156,     -0.7656,     -1.0469,      1.3516,     -0.1084,     -1.0469,      1.8359,      1.9531,      2.1562,     -1.3672,     -0.1191,      0.8008,     -0.3965],
        [    -0.9922,      1.2109,      0.1445,     -1.0781,      3.7344,      2.0156,      1.3516,     -0.1562,     -1.9375,     -0.4102,     -3.9688,      3.0156,     -1.5625,     -0.0244,      2.4844,      0.0679],
        [     3.3750,     -0.3945,     -4.7500,     -1.5781,      1.3672,      0.2285,     -0.3711,      2.1562,     -1.0703,     -7.6250,      3.9531,      2.9219,     -2.0156,     -0.2168,     -2.1094,      0.1592],
        [     0.2715,      1.1953,     -6.9375,     -2.8594,     -2.6719,      0.1104,      1.2891,      2.7500,      0.3965,     -4.5000,     -0.1187,      3.2656,     -1.5391,     -0.1289,     -1.2344,     -0.1465],
        [    -1.0234,      4.9375,     -0.6680,      0.0050,      2.0469,     -4.2500,     -1.5391,     -0.4590,     -2.4375,     -1.1016,     -2.6719,      1.0391,      0.0198,     -0.1143,      0.8672,     -0.0830],
        [    -1.4844,      3.4219,     -4.4062,      1.3281,      0.2070,     -2.5469,      3.4062,      1.0781,     -3.0781,     -0.6641,     -4.9062,     -3.4688,     -2.8125,      0.1084,      0.0354,      2.0469],
        [    -0.6797,     -2.0625,     -5.5312,      0.3594,     -1.4375,     -0.8164,     -3.3594,      5.0938,     -2.3438,     -4.8750,      0.8320,     -2.5000,     -4.7812,     -0.1367,     -4.1562,      1.4688]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0125,  0.0347,  0.0146,  0.0081,  0.0175, -0.0276, -0.0742, -0.0398,  0.0026, -0.0186,  0.0859,  0.0967,  0.0088,  0.0179,  0.0085, -0.0447],
        [ 1.2656,  0.4746,  0.0513, -0.3516, -0.6133, -1.7109,  0.6797, -0.3691, -0.2168, -0.4922,  0.2832,  0.9102,  1.0625,  0.1235,  0.2266, -0.1670],
        [ 1.0781,  1.3281, -0.6016, -0.4375, -0.5391, -1.8359,  0.5117, -0.1260, -0.3984, -0.5781, -0.6484,  0.7852,  1.0391, -0.7031,  0.5703, -0.1992],
        [-0.4238,  1.1172,  0.8477,  0.5195, -0.1357, -1.5312,  0.0571,  0.3887, -0.5898, -1.4922,  0.7695,  0.3027,  1.2188,  0.3555, -0.0952, -0.3516],
        [-0.9258,  0.7031,  0.1055, -0.9102, -2.5625, -0.9414,  1.1875,  1.4531, -0.8984, -0.8789,  1.8594,  1.4766,  0.5820,  0.4766, -0.1885, -0.2461],
        [ 1.1562,  0.4668, -1.5156, -0.8516, -2.3594, -1.1094,  1.0469,  0.4688, -0.7188, -0.6797,  1.3516,  0.7031,  0.4004, -0.4551,  0.3418, -1.4219],
        [ 1.2812,  0.7422, -1.3125, -1.2109, -1.4766, -0.7031, -0.1416,  0.5469,  0.4395, -1.0703,  0.7539,  1.5703, -0.2988,  0.2559, -0.1523, -0.4238],
        [ 0.3633,  0.1523, -0.3301,  0.3945, -1.3359, -0.9688,  0.6016, -0.1445, -0.3027,  0.8516,  1.5000,  0.7852,  1.2344, -1.9531,  0.0427, -0.2812],
        [ 0.2617, -0.0444, -1.6406, -0.7539,  0.0442, -0.3477,  0.1465,  0.9609, -0.2969,  0.0374,  0.5273, -0.3184,  0.2119, -0.9297, -0.6914,  0.3535],
        [ 0.4492,  0.4082, -0.9688, -0.5273, -1.1875, -0.8320, -0.2109,  0.1240, -0.3984, -0.7773,  0.2490, -0.3359,  0.8789,  0.1279,  0.0120, -0.0698]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -5.9062,  20.2500,   5.6250,   3.6094,  14.9375, -36.7500, -28.8750, -19.5000,   2.5625, -14.5625,  47.5000,  49.0000,  13.5000,  17.8750,   4.3750, -40.0000],
        [ 16.7500,   7.7500,   0.5547,  -4.3750, -14.7500, -63.5000,   7.3750,  -5.0625,  -6.0625, -10.8125,   4.3438,  12.7500,  45.7500,   3.4531,   3.2500,  -4.1875],
        [ 14.5000,  22.2500,  -6.6250,  -5.5938, -13.1875, -69.5000,   5.6875,  -1.7578, -11.2500, -12.8750, -10.1875,  11.2500,  45.5000, -19.8750,   8.3125,  -5.0625],
        [ -5.2188,  16.8750,   8.5000,   6.0625,  -3.0156, -52.7500,   0.5781,   4.9688, -15.2500, -30.3750,  11.0625,   3.9844,  48.7500,   9.1875,  -1.2656,  -8.1875],
        [-11.9375,  11.1875,   1.1094, -11.1250, -60.2500, -34.2500,  12.6875,  19.5000, -24.5000, -18.8750,  28.0000,  20.3750,  24.5000,  13.0000,  -2.6406,  -6.0312],
        [ 14.9375,   7.4688, -16.0000, -10.3750, -55.2500, -40.2500,  11.1250,   6.2812, -19.6250, -14.6250,  20.5000,   9.6875,  16.7500, -12.3750,   4.8125, -34.7500],
        [ 15.4375,  11.0625, -12.8750, -13.7500, -32.2500, -23.7500,  -1.3984,   6.7812,  11.1250, -21.3750,  10.6250,  20.1250, -11.6875,   6.4375,  -1.9844,  -9.6250],
        [  4.3125,   2.2344,  -3.2031,   4.4375, -28.8750, -32.5000,   5.9062,  -1.7734,  -7.5938,  16.7500,  20.8750,  10.0000,  47.5000, -48.7500,   0.5508,  -6.3438],
        [  3.3281,  -0.7031, -17.0000,  -9.1250,   1.0156, -12.4375,   1.5312,  12.6875,  -7.9375,   0.7891,   7.8438,  -4.3438,   8.7500, -24.8750,  -9.5000,   8.5625],
        [  5.4062,   6.0625,  -9.5000,  -6.0000, -26.0000, -28.1250,  -2.0938,   1.5469, -10.0000, -15.5000,   3.4844,  -4.3125,  34.2500,   3.2188,   0.1562,  -1.5938]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     3.3594,      1.3906,     -2.2500,      0.3516,     -1.0547,     -1.1094,      0.7383,     -1.7109,      0.4883,     -2.3594,     -1.5078,      1.9297,      1.7734,      0.1357,     -3.9062,      0.0698],
        [     0.3203,     -3.8750,      0.5156,     -2.3125,    -12.2500,    -11.7500,     -1.0234,     16.0000,     -4.0000,     -6.1250,      4.6562,     -0.2773,      7.0312,     -0.7031,      4.1875,     -8.5000],
        [     3.1562,      0.0337,      0.3613,     -3.4531,    -14.8750,    -12.6250,      0.0530,     13.3750,     -1.1328,    -11.5625,     10.6875,      2.0312,      6.5312,     -0.9844,     12.1875,     -4.1250],
        [    -6.0312,      1.7656,     -1.7344,      1.1641,    -14.8125,     -7.5312,     -1.0625,      5.4375,     -8.8750,     -8.1250,      2.1094,      0.1475,      1.9219,      0.5938,      4.9688,     -2.1406],
        [     1.5938,      1.6719,     -2.0625,      5.2188,     -7.4062,     -2.2344,      0.8281,      7.5938,     -4.3750,      2.3125,      5.6875,      7.0625,      0.0339,     -0.4863,      2.2812,     -6.1250],
        [   -12.6875,      2.7031,      8.3750,    -17.5000,    -25.8750,    -23.3750,      3.7969,      2.5469,     -0.1357,    -22.2500,    -10.0000,     17.1250,      3.0000,     -1.4766,      1.8828,     -1.7500],
        [    -5.1875,     -3.3281,      2.6094,    -11.1250,    -18.7500,    -12.8125,      1.8359,     -3.6875,     -3.2031,     -0.7969,    -10.5000,     11.2500,     -0.6836,     -0.0571,      4.0000,     -2.1406],
        [    -4.2812,      2.5781,      1.6250,      3.0625,     -7.8438,    -11.1250,     -0.9023,      7.0625,     -3.7500,      4.0625,      2.3750,     -3.7188,      4.2500,     -0.5820,      3.7344,     -6.0625],
        [     7.6250,      2.8594,      4.7188,      2.0938,      0.9727,      2.1562,     -1.2344,     -0.1021,     -2.7969,      0.6602,      4.0938,     -3.3125,     -0.0016,      0.3789,      0.7422,      3.5469],
        [    -9.8750,     -7.3750,      9.0625,    -12.2500,    -10.6250,    -16.2500,     -1.7578,     -1.1562,      1.5000,     -8.2500,     -6.3438,     13.0625,      3.4531,     -0.6094,     -1.2656,     -7.9375]]
-------------------------
name='positions layer 11'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 11'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     3.3594,      1.3906,     -2.2500,      0.3516,     -1.0547,     -1.1094,      0.7383,     -1.7109,      0.4883,     -2.3594,     -1.5078,      1.9297,      1.7734,      0.1357,     -3.9062,      0.0698],
        [     0.3203,     -3.8750,      0.5156,     -2.3125,    -12.2500,    -11.7500,     -1.0234,     16.0000,     -4.0000,     -6.1250,      4.6562,     -0.2773,      7.0312,     -0.7031,      4.1875,     -8.5000],
        [     3.1562,      0.0337,      0.3613,     -3.4531,    -14.8750,    -12.6250,      0.0530,     13.3750,     -1.1328,    -11.5625,     10.6875,      2.0312,      6.5312,     -0.9844,     12.1875,     -4.1250],
        [    -6.0312,      1.7656,     -1.7344,      1.1641,    -14.8125,     -7.5312,     -1.0625,      5.4375,     -8.8750,     -8.1250,      2.1094,      0.1475,      1.9219,      0.5938,      4.9688,     -2.1406],
        [     1.5938,      1.6719,     -2.0625,      5.2188,     -7.4062,     -2.2344,      0.8281,      7.5938,     -4.3750,      2.3125,      5.6875,      7.0625,      0.0339,     -0.4863,      2.2812,     -6.1250],
        [   -12.6875,      2.7031,      8.3750,    -17.5000,    -25.8750,    -23.3750,      3.7969,      2.5469,     -0.1357,    -22.2500,    -10.0000,     17.1250,      3.0000,     -1.4766,      1.8828,     -1.7500],
        [    -5.1875,     -3.3281,      2.6094,    -11.1250,    -18.7500,    -12.8125,      1.8359,     -3.6875,     -3.2031,     -0.7969,    -10.5000,     11.2500,     -0.6836,     -0.0571,      4.0000,     -2.1406],
        [    -4.2812,      2.5781,      1.6250,      3.0625,     -7.8438,    -11.1250,     -0.9023,      7.0625,     -3.7500,      4.0625,      2.3750,     -3.7188,      4.2500,     -0.5820,      3.7344,     -6.0625],
        [     7.6250,      2.8594,      4.7188,      2.0938,      0.9727,      2.1562,     -1.2344,     -0.1021,     -2.7969,      0.6602,      4.0938,     -3.3125,     -0.0016,      0.3789,      0.7422,      3.5469],
        [    -9.8750,     -7.3750,      9.0625,    -12.2500,    -10.6250,    -16.2500,     -1.7578,     -1.1562,      1.5000,     -8.2500,     -6.3438,     13.0625,      3.4531,     -0.6094,     -1.2656,     -7.9375]]
name='residual layer 11'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -5.9062,  20.2500,   5.6250,   3.6094,  14.9375, -36.7500, -28.8750, -19.5000,   2.5625, -14.5625,  47.5000,  49.0000,  13.5000,  17.8750,   4.3750, -40.0000],
        [ 16.7500,   7.7500,   0.5547,  -4.3750, -14.7500, -63.5000,   7.3750,  -5.0625,  -6.0625, -10.8125,   4.3438,  12.7500,  45.7500,   3.4531,   3.2500,  -4.1875],
        [ 14.5000,  22.2500,  -6.6250,  -5.5938, -13.1875, -69.5000,   5.6875,  -1.7578, -11.2500, -12.8750, -10.1875,  11.2500,  45.5000, -19.8750,   8.3125,  -5.0625],
        [ -5.2188,  16.8750,   8.5000,   6.0625,  -3.0156, -52.7500,   0.5781,   4.9688, -15.2500, -30.3750,  11.0625,   3.9844,  48.7500,   9.1875,  -1.2656,  -8.1875],
        [-11.9375,  11.1875,   1.1094, -11.1250, -60.2500, -34.2500,  12.6875,  19.5000, -24.5000, -18.8750,  28.0000,  20.3750,  24.5000,  13.0000,  -2.6406,  -6.0312],
        [ 14.9375,   7.4688, -16.0000, -10.3750, -55.2500, -40.2500,  11.1250,   6.2812, -19.6250, -14.6250,  20.5000,   9.6875,  16.7500, -12.3750,   4.8125, -34.7500],
        [ 15.4375,  11.0625, -12.8750, -13.7500, -32.2500, -23.7500,  -1.3984,   6.7812,  11.1250, -21.3750,  10.6250,  20.1250, -11.6875,   6.4375,  -1.9844,  -9.6250],
        [  4.3125,   2.2344,  -3.2031,   4.4375, -28.8750, -32.5000,   5.9062,  -1.7734,  -7.5938,  16.7500,  20.8750,  10.0000,  47.5000, -48.7500,   0.5508,  -6.3438],
        [  3.3281,  -0.7031, -17.0000,  -9.1250,   1.0156, -12.4375,   1.5312,  12.6875,  -7.9375,   0.7891,   7.8438,  -4.3438,   8.7500, -24.8750,  -9.5000,   8.5625],
        [  5.4062,   6.0625,  -9.5000,  -6.0000, -26.0000, -28.1250,  -2.0938,   1.5469, -10.0000, -15.5000,   3.4844,  -4.3125,  34.2500,   3.2188,   0.1562,  -1.5938]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0055,      0.0415,      0.0075,      0.0081,      0.0217,     -0.0645,     -0.0679,     -0.0405,      0.0051,     -0.0283,      0.0923,      0.1011,      0.0342,      0.0378,      0.0009,     -0.0620],
        [     0.9922,      0.2002,      0.0645,     -0.3691,     -1.1406,     -3.4688,      0.4121,      0.5664,     -0.4570,     -0.7656,      0.4863,      0.6641,      3.1875,      0.1553,      0.3926,     -0.5312],
        [     1.0469,      1.1719,     -0.3828,     -0.5078,     -1.2031,     -3.8438,      0.3789,      0.6133,     -0.5742,     -1.1250,      0.0275,      0.7227,      3.2031,     -1.2031,      1.0938,     -0.3926],
        [    -0.7383,      1.0859,      0.4590,      0.4492,     -0.8477,     -3.1094,     -0.0354,      0.6055,     -1.2344,     -1.9531,      0.8008,      0.2480,      3.4375,      0.6211,      0.2178,     -0.4863],
        [    -0.6484,      0.7148,     -0.0620,     -0.3516,     -3.0781,     -1.8125,      0.9492,      1.5078,     -1.4141,     -0.8047,      1.9688,      1.5781,      1.6016,      0.7656,     -0.0204,     -0.5508],
        [     0.1250,      0.5000,     -0.4375,     -1.4766,     -3.2656,     -2.8125,      0.9258,      0.4355,     -0.8555,     -1.5938,      0.5430,      1.3672,      1.1406,     -0.7500,      0.3359,     -1.4688],
        [     0.6836,      0.4590,     -0.7031,     -1.5703,     -2.4531,     -1.9219,      0.0327,      0.1826,      0.4102,     -1.1484,      0.0077,      1.9141,     -0.8555,      0.4121,      0.1206,     -0.5625],
        [     0.0021,      0.2871,     -0.1094,      0.4766,     -1.7891,     -2.3125,      0.3750,      0.3145,     -0.5938,      1.0859,      1.4531,      0.3867,      3.6094,     -3.2188,      0.2598,     -0.5977],
        [     0.6992,      0.1226,     -0.8086,     -0.4238,      0.0918,     -0.5195,      0.0212,      0.7148,     -0.5352,      0.0718,      0.7070,     -0.4492,      0.5781,     -1.5156,     -0.5039,      0.5586],
        [    -0.2793,     -0.0728,     -0.0282,     -1.0859,     -1.6562,     -2.2031,     -0.2695,      0.0217,     -0.4160,     -1.1562,     -0.1660,      0.5039,      2.4531,      0.1582,     -0.0625,     -0.4297]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -2.5469,     21.6250,      3.3750,      3.9688,     13.8750,    -37.7500,    -28.1250,    -21.2500,      3.0469,    -16.8750,     46.0000,     51.0000,     15.2500,     18.0000,      0.4688,    -40.0000],
        [    17.1250,      3.8750,      1.0703,     -6.6875,    -27.0000,    -75.0000,      6.3438,     10.9375,    -10.0625,    -17.0000,      9.0000,     12.5000,     52.7500,      2.7500,      7.4375,    -12.6875],
        [    17.6250,     22.2500,     -6.2500,     -9.0625,    -28.0000,    -82.0000,      5.7500,     11.6250,    -12.3750,    -24.5000,      0.5000,     13.2500,     52.0000,    -20.8750,     20.5000,     -9.1875],
        [   -11.2500,     18.6250,      6.7500,      7.2188,    -17.8750,    -60.2500,     -0.4844,     10.3750,    -24.1250,    -38.5000,     13.1875,      4.1250,     50.7500,      9.7500,      3.7031,    -10.3125],
        [   -10.3750,     12.8750,     -0.9531,     -5.9062,    -67.5000,    -36.5000,     13.5000,     27.1250,    -28.8750,    -16.5000,     33.7500,     27.5000,     24.5000,     12.5000,     -0.3594,    -12.1250],
        [     2.2500,     10.1875,     -7.6250,    -27.8750,    -81.0000,    -63.5000,     14.9375,      8.8125,    -19.7500,    -37.0000,     10.5000,     26.7500,     19.7500,    -13.8750,      6.6875,    -36.5000],
        [    10.2500,      7.7500,    -10.2500,    -24.8750,    -51.0000,    -36.5000,      0.4375,      3.0938,      7.9375,    -22.1250,      0.1250,     31.3750,    -12.3750,      6.3750,      2.0156,    -11.7500],
        [     0.0312,      4.8125,     -1.5781,      7.5000,    -36.7500,    -43.5000,      5.0000,      5.2812,    -11.3750,     20.7500,     23.2500,      6.2812,     51.7500,    -49.2500,      4.2812,    -12.3750],
        [    10.9375,      2.1562,    -12.2500,     -7.0312,      1.9844,    -10.2500,      0.2969,     12.5625,    -10.7500,      1.4531,     11.9375,     -7.6562,      8.7500,    -24.5000,     -8.7500,     12.1250],
        [    -4.4688,     -1.3125,     -0.4375,    -18.2500,    -36.5000,    -44.5000,     -3.8438,      0.3906,     -8.5000,    -23.7500,     -2.8594,      8.7500,     37.7500,      2.6094,     -1.1094,     -9.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.8125, -0.6953, -0.2207,  0.0889,  0.4766, -0.7812,  0.5820,  0.4785, -0.1953,  0.3320, -0.6211, -0.5391, -0.9375, -0.0752, -0.6523, -0.4238],
        [-4.2500, -0.2129,  0.0432,  0.2656,  3.9844, -1.8281, -1.6406, -0.3535,  0.3633, -0.7539,  1.3672,  6.0625, -1.4609, -0.2559,  2.3125,  1.1250],
        [-5.5625,  0.5508, -0.5195,  1.6719,  4.4688, -3.3125, -3.0938, -1.3203,  0.5625, -3.2812,  4.1875,  9.6250, -0.0518, -0.4355,  2.6719,  2.2344],
        [-0.6445, -2.6250,  0.6406, -1.0938,  5.1250, -3.4062, -2.2969,  0.7344,  0.3691,  0.4141,  0.4102,  6.5938, -1.7656, -0.3145,  1.2734,  1.7344],
        [-0.9062,  0.4375,  2.0781, -4.1562,  2.7031, -5.4375,  1.0547, -1.2812,  0.0518, -1.3203,  0.1553,  3.8438, -1.1250, -0.3047,  2.6406, -0.1001],
        [-0.2637, -1.2109,  2.0156, -0.2754,  5.1875,  2.2812, -5.7812,  0.2100, -0.9844, -5.6875, -1.1016,  6.6562, -2.3438, -0.3145,  3.0156,  0.2559],
        [ 4.1562, -0.7305,  2.1406, -0.7969, -2.2500, -1.3828, -2.3438,  0.7383, -2.2344, -1.3984, -2.5469, 10.2500, -2.3438, -0.3047, -7.2500, -1.4062],
        [ 0.0854,  0.7344,  0.9883, -6.1250,  3.6562, -2.1562, -3.4688,  5.8125, -2.1562, -1.0625, -0.6055,  5.3750, -1.8047, -0.3066, -2.2500,  0.8438],
        [ 0.6445, -0.8477, -0.2637, -2.2031,  3.3594, -3.2500, -4.0625,  0.7617, -3.3750,  2.6250,  9.8750,  3.4375, -2.8125, -0.2402,  3.5625,  1.6719],
        [-0.1758, -1.3359,  2.8594, -0.1245,  3.2969,  1.2422, -3.9531,  0.1260, -3.1719, -3.6875,  7.8125,  0.7070,  0.2715, -0.1689,  2.5312, -4.4688]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0064,      0.0400,      0.0076,      0.0085,      0.0178,     -0.0317,     -0.0649,     -0.0427,      0.0034,     -0.0223,      0.0771,      0.0933,      0.0114,      0.0186,     -0.0003,     -0.0518],
        [     0.7070,      0.2012,      0.0776,     -0.3848,     -0.8203,     -1.8203,      0.3203,      0.6250,     -0.3379,     -0.6914,      0.5078,      0.9922,      1.1797,      0.0742,      0.5195,     -0.4258],
        [     0.7109,      1.3516,     -0.5078,     -0.4766,     -0.9062,     -2.1719,      0.1934,      0.6562,     -0.4414,     -1.1641,      0.2480,      1.3125,      1.2812,     -0.6836,      1.3203,     -0.2754],
        [    -0.7734,      1.0469,      0.6094,      0.4355,     -0.5352,     -1.7812,     -0.2236,      0.7773,     -0.9766,     -1.7500,      0.7891,      0.6758,      1.3281,      0.3320,      0.3125,     -0.3730],
        [    -0.6875,      0.8164,      0.0869,     -0.6680,     -2.5625,     -1.1016,      1.1016,      1.6953,     -1.1094,     -0.7695,      1.8438,      1.8516,      0.5938,      0.4043,      0.1348,     -0.5000],
        [     0.1089,      0.4961,     -0.3906,     -1.6953,     -2.7031,     -1.4453,      0.6211,      0.5312,     -0.7188,     -1.6641,      0.4609,      1.7891,      0.3984,     -0.4219,      0.5156,     -1.3359],
        [     0.9336,      0.4590,     -0.6719,     -1.8281,     -2.2500,     -1.0625,     -0.1533,      0.2695,      0.2354,     -1.0859,     -0.1406,      2.6406,     -0.4004,      0.2139,     -0.3301,     -0.5742],
        [     0.0075,      0.3574,     -0.0481,      0.0962,     -1.3828,     -1.2578,      0.1216,      0.7656,     -0.5469,      0.8945,      1.2969,      0.7227,      1.3438,     -1.7266,      0.1260,     -0.4961],
        [     0.7500,      0.0850,     -1.0234,     -0.6523,      0.2256,     -0.3789,     -0.3008,      0.9336,     -0.5781,      0.1875,      1.2656,     -0.2656,      0.1602,     -0.8711,     -0.3262,      0.6016],
        [    -0.2832,     -0.1621,      0.1875,     -1.2344,     -1.3203,     -1.1406,     -0.5898,      0.0339,     -0.4512,     -1.1875,      0.2715,      0.5625,      0.9688,      0.0811,      0.0840,     -0.5742]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -3.3594,  20.8750,   3.1562,   4.0625,  14.3750, -38.5000, -27.5000, -20.7500,   2.8438, -16.5000,  45.5000,  50.5000,  14.3125,  17.8750,  -0.1836, -40.5000],
        [ 12.8750,   3.6562,   1.1172,  -6.4375, -23.0000, -77.0000,   4.6875,  10.5625,  -9.6875, -17.7500,  10.3750,  18.5000,  51.2500,   2.5000,   9.7500, -11.5625],
        [ 12.0625,  22.7500,  -6.7812,  -7.3750, -23.5000, -85.5000,   2.6562,  10.3125, -11.8125, -27.7500,   4.6875,  22.8750,  52.0000, -21.2500,  23.1250,  -6.9375],
        [-11.8750,  16.0000,   7.3750,   6.1250, -12.7500, -63.7500,  -2.7812,  11.1250, -23.7500, -38.0000,  13.6250,  10.7500,  49.0000,   9.4375,   4.9688,  -8.5625],
        [-11.2500,  13.3125,   1.1250, -10.0625, -65.0000, -42.0000,  14.5625,  25.8750, -28.8750, -17.8750,  34.0000,  31.3750,  23.3750,  12.1875,   2.2812, -12.2500],
        [  1.9844,   9.0000,  -5.6250, -28.1250, -76.0000, -61.2500,   9.1250,   9.0000, -20.7500, -42.7500,   9.3750,  33.5000,  17.3750, -14.1875,   9.6875, -36.2500],
        [ 14.3750,   7.0312,  -8.1250, -25.6250, -53.2500, -38.0000,  -1.9062,   3.8281,   5.6875, -23.5000,  -2.4219,  41.5000, -14.7500,   6.0625,  -5.2500, -13.1250],
        [  0.1167,   5.5625,  -0.5898,   1.3750, -33.0000, -45.7500,   1.5312,  11.1250, -13.5000,  19.7500,  22.6250,  11.6250,  50.0000, -49.5000,   2.0312, -11.5000],
        [ 11.5625,   1.3125, -12.5000,  -9.2500,   5.3438, -13.5000,  -3.7656,  13.3125, -14.1250,   4.0625,  21.7500,  -4.2188,   5.9375, -24.7500,  -5.1875,  13.8125],
        [ -4.6562,  -2.6562,   2.4219, -18.3750, -33.2500, -43.2500,  -7.8125,   0.5156, -11.6875, -27.5000,   4.9375,   9.4375,  38.0000,   2.4375,   1.4219, -14.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     1.0156,      3.5625,     -1.0391,      0.6797,     -0.2119,     -1.5625,      0.6914,      0.8125,      2.7031,     -1.8203,      0.5703,      1.9844,      1.2969,      0.0225,     -1.9375,      0.8984],
        [    -6.3750,     10.8125,      9.1250,      1.2578,     -0.8672,     -6.0312,      1.1094,     -5.0312,     -0.6836,      3.6406,     -0.4141,      5.8125,      0.4688,      0.0090,     11.4375,     -4.2188],
        [     2.0938,      7.5312,      1.9453,     -3.1719,     18.1250,     -2.5000,      1.7266,     -4.3750,    -10.8125,      7.5000,      3.3906,      6.6250,      1.9531,      0.5352,     -2.5625,      3.7500],
        [    -5.6250,      1.3203,      1.1406,      8.0625,      6.2188,      4.0625,      8.8750,      1.7812,     -6.5625,     -0.2432,     -9.6875,     10.6875,     -2.4219,     -0.1865,      4.9688,      0.2617],
        [    -9.8125,      0.6680,     10.1250,      4.0938,      3.6250,      4.1562,      3.9062,      6.6562,     -6.2188,     13.0000,    -14.0625,     -2.5938,     -1.7031,     -0.6289,     14.7500,    -13.4375],
        [    -4.3438,     16.7500,      7.7188,      2.1250,      8.5000,     -8.8125,      0.7422,     -2.4531,     -6.2188,      1.0469,      7.9688,     13.0625,     11.0625,     -0.2021,     15.3750,     -6.0312],
        [     4.5000,     13.2500,      0.4180,     -5.2812,     11.0625,    -11.0625,      8.8125,     -0.7266,      9.5000,      0.5625,      9.8750,      3.2812,      5.8750,      0.0923,      2.3438,     -9.4375],
        [    -3.2188,     -0.9727,      6.0625,      6.5312,     -5.6875,     -0.4395,     -2.1719,     -5.8438,      2.4062,      9.8750,      5.4688,     -4.3750,      5.3438,     -0.3906,     15.2500,     -0.3086],
        [     2.4688,     11.4375,      3.4219,      3.8125,      6.4375,      9.3750,      6.8438,      2.1406,      3.8125,    -10.0625,     -6.8438,      2.5156,     -3.8594,     -0.2559,     -2.8750,     -1.6094],
        [    -0.2363,      3.9688,      8.0000,      0.5391,     -9.2500,     -9.6250,     -2.9688,     -5.9688,     -2.2500,     -5.3438,     14.2500,     -2.9219,      3.3438,     -0.4766,     20.0000,      3.3281]]
-------------------------
name='positions layer 12'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 12'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     1.0156,      3.5625,     -1.0391,      0.6797,     -0.2119,     -1.5625,      0.6914,      0.8125,      2.7031,     -1.8203,      0.5703,      1.9844,      1.2969,      0.0225,     -1.9375,      0.8984],
        [    -6.3750,     10.8125,      9.1250,      1.2578,     -0.8672,     -6.0312,      1.1094,     -5.0312,     -0.6836,      3.6406,     -0.4141,      5.8125,      0.4688,      0.0090,     11.4375,     -4.2188],
        [     2.0938,      7.5312,      1.9453,     -3.1719,     18.1250,     -2.5000,      1.7266,     -4.3750,    -10.8125,      7.5000,      3.3906,      6.6250,      1.9531,      0.5352,     -2.5625,      3.7500],
        [    -5.6250,      1.3203,      1.1406,      8.0625,      6.2188,      4.0625,      8.8750,      1.7812,     -6.5625,     -0.2432,     -9.6875,     10.6875,     -2.4219,     -0.1865,      4.9688,      0.2617],
        [    -9.8125,      0.6680,     10.1250,      4.0938,      3.6250,      4.1562,      3.9062,      6.6562,     -6.2188,     13.0000,    -14.0625,     -2.5938,     -1.7031,     -0.6289,     14.7500,    -13.4375],
        [    -4.3438,     16.7500,      7.7188,      2.1250,      8.5000,     -8.8125,      0.7422,     -2.4531,     -6.2188,      1.0469,      7.9688,     13.0625,     11.0625,     -0.2021,     15.3750,     -6.0312],
        [     4.5000,     13.2500,      0.4180,     -5.2812,     11.0625,    -11.0625,      8.8125,     -0.7266,      9.5000,      0.5625,      9.8750,      3.2812,      5.8750,      0.0923,      2.3438,     -9.4375],
        [    -3.2188,     -0.9727,      6.0625,      6.5312,     -5.6875,     -0.4395,     -2.1719,     -5.8438,      2.4062,      9.8750,      5.4688,     -4.3750,      5.3438,     -0.3906,     15.2500,     -0.3086],
        [     2.4688,     11.4375,      3.4219,      3.8125,      6.4375,      9.3750,      6.8438,      2.1406,      3.8125,    -10.0625,     -6.8438,      2.5156,     -3.8594,     -0.2559,     -2.8750,     -1.6094],
        [    -0.2363,      3.9688,      8.0000,      0.5391,     -9.2500,     -9.6250,     -2.9688,     -5.9688,     -2.2500,     -5.3438,     14.2500,     -2.9219,      3.3438,     -0.4766,     20.0000,      3.3281]]
name='residual layer 12'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -3.3594,  20.8750,   3.1562,   4.0625,  14.3750, -38.5000, -27.5000, -20.7500,   2.8438, -16.5000,  45.5000,  50.5000,  14.3125,  17.8750,  -0.1836, -40.5000],
        [ 12.8750,   3.6562,   1.1172,  -6.4375, -23.0000, -77.0000,   4.6875,  10.5625,  -9.6875, -17.7500,  10.3750,  18.5000,  51.2500,   2.5000,   9.7500, -11.5625],
        [ 12.0625,  22.7500,  -6.7812,  -7.3750, -23.5000, -85.5000,   2.6562,  10.3125, -11.8125, -27.7500,   4.6875,  22.8750,  52.0000, -21.2500,  23.1250,  -6.9375],
        [-11.8750,  16.0000,   7.3750,   6.1250, -12.7500, -63.7500,  -2.7812,  11.1250, -23.7500, -38.0000,  13.6250,  10.7500,  49.0000,   9.4375,   4.9688,  -8.5625],
        [-11.2500,  13.3125,   1.1250, -10.0625, -65.0000, -42.0000,  14.5625,  25.8750, -28.8750, -17.8750,  34.0000,  31.3750,  23.3750,  12.1875,   2.2812, -12.2500],
        [  1.9844,   9.0000,  -5.6250, -28.1250, -76.0000, -61.2500,   9.1250,   9.0000, -20.7500, -42.7500,   9.3750,  33.5000,  17.3750, -14.1875,   9.6875, -36.2500],
        [ 14.3750,   7.0312,  -8.1250, -25.6250, -53.2500, -38.0000,  -1.9062,   3.8281,   5.6875, -23.5000,  -2.4219,  41.5000, -14.7500,   6.0625,  -5.2500, -13.1250],
        [  0.1167,   5.5625,  -0.5898,   1.3750, -33.0000, -45.7500,   1.5312,  11.1250, -13.5000,  19.7500,  22.6250,  11.6250,  50.0000, -49.5000,   2.0312, -11.5000],
        [ 11.5625,   1.3125, -12.5000,  -9.2500,   5.3438, -13.5000,  -3.7656,  13.3125, -14.1250,   4.0625,  21.7500,  -4.2188,   5.9375, -24.7500,  -5.1875,  13.8125],
        [ -4.6562,  -2.6562,   2.4219, -18.3750, -33.2500, -43.2500,  -7.8125,   0.5156, -11.6875, -27.5000,   4.9375,   9.4375,  38.0000,   2.4375,   1.4219, -14.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0052,  0.0527,  0.0043,  0.0081,  0.0247, -0.0903, -0.0625, -0.0371,  0.0108, -0.0347,  0.0957,  0.0981,  0.0354,  0.0315, -0.0038, -0.0640],
        [ 0.2891,  0.6328,  0.4180, -0.1807, -0.8398, -3.7812,  0.2734,  0.2090, -0.4102, -0.5391,  0.4180,  0.9219,  2.3750,  0.0894,  0.7656, -0.5156],
        [ 0.5781,  1.2109, -0.1816, -0.3379, -0.1729, -3.6719,  0.1904,  0.2061, -0.8203, -0.7109,  0.3125,  1.0312,  2.2812, -0.6797,  0.6836, -0.0957],
        [-0.8125,  0.7891,  0.3613,  0.5117, -0.2393, -2.8281,  0.2988,  0.5078, -1.2422, -1.5156,  0.1729,  0.8398,  2.2188,  0.3438,  0.3750, -0.2832],
        [-1.0156,  0.6641,  0.4980, -0.2246, -2.3438, -1.8672,  0.9453,  1.3359, -1.5000, -0.2012,  0.9102,  1.1797,  1.0781,  0.4492,  0.6680, -0.9062],
        [-0.1074,  1.1562,  0.0879, -0.9219, -2.4219, -3.2656,  0.4766,  0.2539, -1.0859, -1.6328,  0.7500,  1.8047,  1.3359, -0.5273,  0.9297, -1.4141],
        [ 0.8398,  0.8867, -0.3164, -1.0781, -1.4922, -2.2500,  0.3281,  0.1177,  0.6016, -0.8789,  0.3145,  1.6953, -0.4082,  0.2207, -0.1055, -0.7422],
        [-0.1445,  0.2109,  0.2354,  0.2871, -1.4297, -2.2031, -0.0317,  0.2090, -0.4590,  1.1875,  1.2422,  0.2871,  2.6719, -1.8672,  0.6562, -0.4062],
        [ 0.6992,  0.6250, -0.4141, -0.2109,  0.4648, -0.2100,  0.1631,  0.6562, -0.4570, -0.2559,  0.7031, -0.0718,  0.1069, -1.0078, -0.3262,  0.4473],
        [-0.2539,  0.0674,  0.4980, -0.7266, -1.7500, -2.8125, -0.5938, -0.2412, -0.6406, -1.4688,  0.9414,  0.2871,  2.2188,  0.0820,  0.9062, -0.4082]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -2.3438,  24.5000,   2.1250,   4.7500,  14.1875, -40.0000, -26.7500, -20.0000,   5.5625, -18.3750,  46.0000,  52.5000,  15.6250,  17.8750,  -2.1250, -39.5000],
        [  6.5000,  14.5000,  10.2500,  -5.1875, -23.8750, -83.0000,   5.8125,   5.5312, -10.3750, -14.1250,   9.9375,  24.2500,  51.7500,   2.5156,  21.2500, -15.7500],
        [ 14.1250,  30.2500,  -4.8438, -10.5625,  -5.3750, -88.0000,   4.3750,   5.9375, -22.6250, -20.2500,   8.0625,  29.5000,  54.0000, -20.7500,  20.5000,  -3.1875],
        [-17.5000,  17.3750,   8.5000,  14.1875,  -6.5312, -59.7500,   6.0938,  12.8750, -30.2500, -38.2500,   3.9375,  21.5000,  46.5000,   9.2500,   9.9375,  -8.3125],
        [-21.0000,  14.0000,  11.2500,  -5.9688, -61.5000, -37.7500,  18.5000,  32.5000, -35.0000,  -4.8750,  20.0000,  28.7500,  21.6250,  11.5625,  17.0000, -25.7500],
        [ -2.3594,  25.7500,   2.0938, -26.0000, -67.5000, -70.0000,   9.8750,   6.5625, -27.0000, -41.7500,  17.3750,  46.5000,  28.5000, -14.3750,  25.0000, -42.2500],
        [ 18.8750,  20.2500,  -7.7188, -30.8750, -42.2500, -49.0000,   6.9062,   3.0938,  15.1875, -23.0000,   7.4375,  44.7500,  -8.8750,   6.1562,  -2.9062, -22.5000],
        [ -3.1094,   4.5938,   5.4688,   7.9062, -38.7500, -46.2500,  -0.6406,   5.2812, -11.1250,  29.6250,  28.1250,   7.2500,  55.2500, -50.0000,  17.2500, -11.8125],
        [ 14.0000,  12.7500,  -9.0625,  -5.4375,  11.7500,  -4.1250,   3.0781,  15.4375, -10.3125,  -6.0000,  14.8750,  -1.7031,   2.0781, -25.0000,  -8.0625,  12.1875],
        [ -4.9062,   1.3125,  10.4375, -17.8750, -42.5000, -53.0000, -10.7500,  -5.4375, -13.9375, -32.7500,  19.2500,   6.5000,  41.2500,   1.9609,  21.3750, -10.6875]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -1.7656,  -0.7578,   4.6875,  -6.3125,  -0.6133,  -6.5625,   3.9688,  -0.3164,   0.7422,   0.3438,  10.8125,   4.5312,   2.7656,  -0.1855,   6.4062,   1.0156],
        [  8.4375,   3.7500,  -0.2734,  -1.8984,   2.3438,  -4.6562,   1.7891,   0.2910,   0.8672,   2.5781,   6.8125,   2.7656,   1.2578,  -0.4258,   5.2500,  -2.3906],
        [ 10.9375,   1.6484,  -0.8828,   1.0703,   2.5000,  -5.9062,  -1.6172,   4.5938,  -0.2832,   3.5781,   0.3066,   3.9531,   3.0938,  -0.1826,   5.3438,  -1.6562],
        [  6.9375,   4.5625,   1.2344,  -2.9219,   1.7500,  -7.0938,   0.8398,   3.8906,   0.3535,   5.4062,   4.0625,   3.0000,   1.5000,  -0.0879,   7.6250,  -0.9219],
        [  4.6250,   3.3906,  -2.4062,  -7.8438,   4.7500,  -4.3125,   1.6719,   4.5625,  -1.1875,   2.7500,   8.6250,   1.8281,  -1.2031,   0.1050,   4.8125,  -2.3906],
        [  5.0938,  -2.5469,  -5.3438,  -7.5938,   9.0000,  -0.1426,   2.2969,   8.0000,   0.2656,   1.5625,  10.3125,  -2.7969,   1.4141,  -0.2734,   5.8750,  -1.9531],
        [  5.0312,   0.3105,  -6.3438, -10.8750,   1.7891,  -3.6406,   3.5156,   4.4688,  -0.6641,  -0.1631,  12.1875,   1.6250,   3.0156,  -0.1953,   5.1562,  -1.8047],
        [ -3.3750,   3.7500,  -3.4531,  -7.2500,   4.1875,  -2.6094,   0.2949,   0.7344,  -2.3125,   3.2500,  14.3125,  -2.0938,   1.1250,  -0.1592,   6.7812,  -3.4219],
        [ -0.7070,   4.3438,  -5.3750,  -9.6875,   0.3145,   1.4375,  -0.1533,   2.7500,   2.1562,   0.7969,   7.2812,  -1.3906,   1.5312,  -0.2246,   3.0938,  -2.2969],
        [ -2.6719,   2.8438,   7.0312,  -5.7500,   7.9688,  -0.2988,   3.4688,   4.5938,   0.1475,  -0.7773,   1.4688,   2.1562,   1.3672,  -0.3359,   7.9688,  -0.5195]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0071,  0.0466,  0.0166, -0.0031,  0.0186, -0.0427, -0.0498, -0.0405,  0.0092, -0.0289,  0.0918,  0.1025,  0.0183,  0.0312,  0.0081, -0.0596],
        [ 0.7031,  0.9766,  0.6602, -0.3848, -0.7969, -2.1875,  0.4512,  0.3164, -0.3750, -0.5039,  0.7344,  1.3125,  1.4297,  0.1001,  1.3672, -0.7617],
        [ 1.1250,  1.6172, -0.3594, -0.4922, -0.1011, -2.2188,  0.1553,  0.5430, -0.8594, -0.6875,  0.3496,  1.5547,  1.4688, -0.9570,  1.2656, -0.1924],
        [-0.5039,  1.1875,  0.6523,  0.6211, -0.1797, -1.6875,  0.4180,  0.9180, -1.1953, -1.4453,  0.3535,  1.2109,  1.3125,  0.4453,  0.9180, -0.3926],
        [-0.7734,  0.9297,  0.5859, -0.7539, -2.0938, -1.0469,  1.1953,  2.0156, -1.4297, -0.0928,  1.2578,  1.4922,  0.5508,  0.5625,  1.1250, -1.1797],
        [ 0.1221,  1.1719, -0.2031, -1.7344, -2.0469, -1.6562,  0.6836,  0.7500, -0.9961, -1.6562,  1.1562,  2.0156,  0.7656, -0.6641,  1.5000, -1.7578],
        [ 1.0312,  1.0078, -0.8516, -2.0938, -1.3750, -1.2031,  0.5703,  0.3770,  0.5273, -0.9258,  0.7891,  2.0938, -0.1455,  0.2637,  0.1069, -0.9375],
        [-0.3301,  0.4824,  0.1436,  0.0386, -1.3750, -1.3125, -0.0221,  0.3516, -0.5703,  1.5469,  2.0000,  0.2715,  1.6328, -2.5938,  1.3359, -0.6914],
        [ 0.6445,  0.9414, -0.9766, -0.8516,  0.4590, -0.0688,  0.1787,  1.0156, -0.3320, -0.2324,  1.0000, -0.1553,  0.1001, -1.2422, -0.2637,  0.4277],
        [-0.3867,  0.2393,  1.2500, -1.3906, -1.3828, -1.4297, -0.4668, -0.0496, -0.5898, -1.5781,  0.9805,  0.4570,  1.2422,  0.0845,  1.6406, -0.5078]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -4.1250,  23.7500,   6.8125,  -1.5625,  13.5625, -46.5000, -22.7500, -20.3750,   6.3125, -18.0000,  56.7500,  57.0000,  18.3750,  17.7500,   4.2812, -38.5000],
        [ 14.9375,  18.2500,  10.0000,  -7.0938, -21.5000, -87.5000,   7.5938,   5.8125,  -9.5000, -11.5625,  16.7500,  27.0000,  53.0000,   2.0938,  26.5000, -18.1250],
        [ 25.0000,  31.8750,  -5.7188,  -9.5000,  -2.8750, -94.0000,   2.7500,  10.5000, -22.8750, -16.6250,   8.3750,  33.5000,  57.0000, -20.8750,  25.8750,  -4.8438],
        [-10.5625,  22.0000,   9.7500,  11.2500,  -4.7812, -67.0000,   6.9375,  16.7500, -29.8750, -32.7500,   8.0000,  24.5000,  48.0000,   9.1875,  17.5000,  -9.2500],
        [-16.3750,  17.3750,   8.8750, -13.8125, -56.7500, -42.0000,  20.1250,  37.0000, -36.2500,  -2.1250,  28.6250,  30.6250,  20.3750,  11.6875,  21.7500, -28.1250],
        [  2.7344,  23.2500,  -3.2500, -33.5000, -58.5000, -70.0000,  12.1875,  14.5625, -26.7500, -40.2500,  27.7500,  43.7500,  29.8750, -14.6250,  30.8750, -44.2500],
        [ 23.8750,  20.5000, -14.0625, -41.7500, -40.5000, -52.7500,  10.4375,   7.5625,  14.5000, -23.1250,  19.6250,  46.5000,  -5.8750,   5.9688,   2.2500, -24.2500],
        [ -6.5000,   8.3750,   2.0156,   0.6562, -34.5000, -48.7500,  -0.3457,   6.0000, -13.4375,  33.0000,  42.5000,   5.1562,  56.5000, -50.2500,  24.0000, -15.2500],
        [ 13.3125,  17.1250, -14.4375, -15.1250,  12.0625,  -2.6875,   2.9219,  18.2500,  -8.1250,  -5.1875,  22.1250,  -3.0938,   3.6094, -25.2500,  -4.9688,   9.8750],
        [ -7.5625,   4.1562,  17.5000, -23.6250, -34.5000, -53.2500,  -7.2812,  -0.8438, -13.8125, -33.5000,  20.7500,   8.6250,  42.5000,   1.6250,  29.3750, -11.1875]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  3.5156,   4.9688,  -0.3477,   2.7500,   0.2363,  -2.3750,   5.3750,  -1.7656,   0.3203,  -0.9688,  -0.6016,   1.0234,   2.0312,  -0.0381,  -4.7188,  -0.2637],
        [-13.8750, -15.3750,  -9.3125,  11.9375, -19.3750,  -9.6875,  16.1250,   1.4922,  -5.7812,  15.1875,   8.6875,  -7.4688,   1.7422,   0.5664,   8.8750,   3.1406],
        [ 12.5625,  -9.6250,  17.2500,  -9.9375, -12.0625,  -8.3125,   3.4375, -12.1250,  -5.8750,  10.6875,   4.8125,   0.3359,   1.5078,  -0.0957,  29.3750,  -3.7812],
        [-10.7500,  -1.0625,   4.9688,   4.6250,  -9.3750,  -6.1250,   8.3750,   8.2500,  -4.1562,  -5.4062,   5.4375,   7.2812,  -3.9219,  -0.1562,  16.3750,   4.9062],
        [ -8.4375,   5.7812, -13.1250,   3.7500, -23.7500,  -4.6562,   4.6250,   2.8906, -14.6250,   9.1875,  20.3750,  11.5000,  -0.3555,   0.6016,   6.3438,  -7.7500],
        [  0.8984,  -3.1875,   9.6250,   4.7812,  -7.9375,  -3.2656,   3.8750,  -6.8438,  -4.8125,  -3.8125,  13.5625,   3.1094,   1.8438,  -0.3086,  32.0000, -10.3750],
        [  1.6016,   7.5625,   6.1875, -17.3750,  -7.5000,  -5.2812,  -1.3672,   1.8359,   0.6484,  -1.2266,   0.4883,   8.6875,  -0.8633,   0.1025,   3.4375,  -4.0312],
        [-21.5000,  -3.9062,   6.7188,  -1.0469,  -6.7188, -10.0000,   5.5312,   7.2500,   3.0469,   2.2188,  19.6250,   1.2109,   7.3438,  -0.2373,  11.8125,   5.8438],
        [-13.0000,   6.0000,  -7.0000, -15.0000,  -0.9961,  -8.3125, -23.6250,  -3.0000,  -3.6875,  -0.6641,  -1.0625,   8.6250,  -4.6250,   0.0588,   2.5312,   5.0625],
        [  4.4688,  -1.8672,  -0.8555,  -2.5625,   7.0312,   3.7969,  11.5625,   8.0625,   3.8906,  -3.0938,   3.8594,  -1.7344,   1.8672,   0.0457,   4.4375, -11.9375]]
-------------------------
name='positions layer 13'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 13'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  3.5156,   4.9688,  -0.3477,   2.7500,   0.2363,  -2.3750,   5.3750,  -1.7656,   0.3203,  -0.9688,  -0.6016,   1.0234,   2.0312,  -0.0381,  -4.7188,  -0.2637],
        [-13.8750, -15.3750,  -9.3125,  11.9375, -19.3750,  -9.6875,  16.1250,   1.4922,  -5.7812,  15.1875,   8.6875,  -7.4688,   1.7422,   0.5664,   8.8750,   3.1406],
        [ 12.5625,  -9.6250,  17.2500,  -9.9375, -12.0625,  -8.3125,   3.4375, -12.1250,  -5.8750,  10.6875,   4.8125,   0.3359,   1.5078,  -0.0957,  29.3750,  -3.7812],
        [-10.7500,  -1.0625,   4.9688,   4.6250,  -9.3750,  -6.1250,   8.3750,   8.2500,  -4.1562,  -5.4062,   5.4375,   7.2812,  -3.9219,  -0.1562,  16.3750,   4.9062],
        [ -8.4375,   5.7812, -13.1250,   3.7500, -23.7500,  -4.6562,   4.6250,   2.8906, -14.6250,   9.1875,  20.3750,  11.5000,  -0.3555,   0.6016,   6.3438,  -7.7500],
        [  0.8984,  -3.1875,   9.6250,   4.7812,  -7.9375,  -3.2656,   3.8750,  -6.8438,  -4.8125,  -3.8125,  13.5625,   3.1094,   1.8438,  -0.3086,  32.0000, -10.3750],
        [  1.6016,   7.5625,   6.1875, -17.3750,  -7.5000,  -5.2812,  -1.3672,   1.8359,   0.6484,  -1.2266,   0.4883,   8.6875,  -0.8633,   0.1025,   3.4375,  -4.0312],
        [-21.5000,  -3.9062,   6.7188,  -1.0469,  -6.7188, -10.0000,   5.5312,   7.2500,   3.0469,   2.2188,  19.6250,   1.2109,   7.3438,  -0.2373,  11.8125,   5.8438],
        [-13.0000,   6.0000,  -7.0000, -15.0000,  -0.9961,  -8.3125, -23.6250,  -3.0000,  -3.6875,  -0.6641,  -1.0625,   8.6250,  -4.6250,   0.0588,   2.5312,   5.0625],
        [  4.4688,  -1.8672,  -0.8555,  -2.5625,   7.0312,   3.7969,  11.5625,   8.0625,   3.8906,  -3.0938,   3.8594,  -1.7344,   1.8672,   0.0457,   4.4375, -11.9375]]
name='residual layer 13'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -4.1250,  23.7500,   6.8125,  -1.5625,  13.5625, -46.5000, -22.7500, -20.3750,   6.3125, -18.0000,  56.7500,  57.0000,  18.3750,  17.7500,   4.2812, -38.5000],
        [ 14.9375,  18.2500,  10.0000,  -7.0938, -21.5000, -87.5000,   7.5938,   5.8125,  -9.5000, -11.5625,  16.7500,  27.0000,  53.0000,   2.0938,  26.5000, -18.1250],
        [ 25.0000,  31.8750,  -5.7188,  -9.5000,  -2.8750, -94.0000,   2.7500,  10.5000, -22.8750, -16.6250,   8.3750,  33.5000,  57.0000, -20.8750,  25.8750,  -4.8438],
        [-10.5625,  22.0000,   9.7500,  11.2500,  -4.7812, -67.0000,   6.9375,  16.7500, -29.8750, -32.7500,   8.0000,  24.5000,  48.0000,   9.1875,  17.5000,  -9.2500],
        [-16.3750,  17.3750,   8.8750, -13.8125, -56.7500, -42.0000,  20.1250,  37.0000, -36.2500,  -2.1250,  28.6250,  30.6250,  20.3750,  11.6875,  21.7500, -28.1250],
        [  2.7344,  23.2500,  -3.2500, -33.5000, -58.5000, -70.0000,  12.1875,  14.5625, -26.7500, -40.2500,  27.7500,  43.7500,  29.8750, -14.6250,  30.8750, -44.2500],
        [ 23.8750,  20.5000, -14.0625, -41.7500, -40.5000, -52.7500,  10.4375,   7.5625,  14.5000, -23.1250,  19.6250,  46.5000,  -5.8750,   5.9688,   2.2500, -24.2500],
        [ -6.5000,   8.3750,   2.0156,   0.6562, -34.5000, -48.7500,  -0.3457,   6.0000, -13.4375,  33.0000,  42.5000,   5.1562,  56.5000, -50.2500,  24.0000, -15.2500],
        [ 13.3125,  17.1250, -14.4375, -15.1250,  12.0625,  -2.6875,   2.9219,  18.2500,  -8.1250,  -5.1875,  22.1250,  -3.0938,   3.6094, -25.2500,  -4.9688,   9.8750],
        [ -7.5625,   4.1562,  17.5000, -23.6250, -34.5000, -53.2500,  -7.2812,  -0.8438, -13.8125, -33.5000,  20.7500,   8.6250,  42.5000,   1.6250,  29.3750, -11.1875]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0013,      0.0693,      0.0165,      0.0025,      0.0275,     -0.1147,     -0.0427,     -0.0488,      0.0142,     -0.0425,      0.1201,      0.1250,      0.0654,      0.0625,     -0.0010,     -0.0815],
        [     0.0454,      0.1357,      0.0342,      0.2002,     -1.5859,     -4.4688,      1.1328,      0.3164,     -0.6367,      0.1582,      1.0547,      0.8242,      3.4375,      0.1826,      1.5234,     -0.6172],
        [     1.5469,      1.0156,      0.5547,     -0.7773,     -0.5586,     -4.5312,      0.2852,     -0.0679,     -1.1562,     -0.2500,      0.5273,      1.3750,      3.5312,     -1.3984,      2.2969,     -0.3418],
        [    -0.9805,      1.0625,      0.7930,      0.7070,     -0.5938,     -3.6250,      0.7891,      1.1641,     -1.5312,     -1.8047,      0.6016,      1.4453,      2.9688,      0.6719,      1.5781,     -0.1924],
        [    -1.0625,      1.0859,     -0.2109,     -0.4160,     -3.1250,     -2.1406,      1.1797,      1.7109,     -2.1250,      0.3086,      2.0312,      1.7734,      1.2500,      0.8438,      1.2109,     -1.4688],
        [     0.1572,      0.9570,      0.3223,     -1.2031,     -2.6094,     -3.4062,      0.7773,      0.3379,     -1.3359,     -1.9531,      1.7422,      2.0000,      2.0156,     -1.0391,      2.7500,     -2.2656],
        [     1.1484,      1.3906,     -0.4121,     -2.5781,     -1.9609,     -2.7969,      0.4570,      0.4258,      0.6680,     -1.1172,      0.8828,      2.4531,     -0.4434,      0.4414,      0.2598,     -1.2188],
        [    -1.2812,      0.2256,      0.4668,     -0.0173,     -1.7109,     -2.8906,      0.2656,      0.6133,     -0.4648,      1.6484,      2.7656,      0.2871,      4.2812,     -3.7188,      1.6562,     -0.4121],
        [     0.0141,      1.1562,     -1.1250,     -1.3125,      0.4531,     -0.5352,     -1.0469,      0.6953,     -0.5195,     -0.2715,      0.9258,      0.2461,     -0.0674,     -1.8359,     -0.1113,      0.6484],
        [    -0.1523,      0.1240,      0.9570,     -1.2500,     -1.2266,     -2.6094,      0.2363,      0.3594,     -0.4785,     -1.8438,      1.1797,      0.3340,      3.2031,      0.1328,      1.6875,     -1.0938]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -0.6094,   28.7500,    6.4688,    1.1875,   13.8125,  -49.0000,  -17.3750,  -22.1250,    6.6250,  -19.0000,   56.2500,   58.0000,   20.3750,   17.7500,   -0.4375,  -38.7500],
        [   1.0625,    2.8750,    0.6875,    4.8438,  -41.0000,  -97.0000,   23.7500,    7.3125,  -15.2500,    3.6250,   25.5000,   19.5000,   54.7500,    2.6562,   35.5000,  -15.0000],
        [  37.5000,   22.2500,   11.5000,  -19.5000,  -14.9375, -102.5000,    6.1875,   -1.6250,  -28.7500,   -5.9375,   13.1875,   33.7500,   58.5000,  -21.0000,   55.2500,   -8.6250],
        [ -21.2500,   21.0000,   14.7500,   15.8750,  -14.1250,  -73.0000,   15.3125,   25.0000,  -34.0000,  -38.2500,   13.4375,   31.7500,   44.0000,    9.0000,   34.0000,   -4.3438],
        [ -24.7500,   23.1250,   -4.2500,  -10.0625,  -80.5000,  -46.7500,   24.7500,   40.0000,  -51.0000,    7.0625,   49.0000,   42.0000,   20.0000,   12.3125,   28.1250,  -36.0000],
        [   3.6250,   20.0000,    6.3750,  -28.7500,  -66.5000,  -73.5000,   16.0000,    7.7188,  -31.5000,  -44.0000,   41.2500,   46.7500,   31.7500,  -14.9375,   63.0000,  -54.5000],
        [  25.5000,   28.0000,   -7.8750,  -59.0000,  -48.0000,  -58.0000,    9.0625,    9.3750,   15.1250,  -24.3750,   20.1250,   55.2500,   -6.7500,    6.0625,    5.6875,  -28.2500],
        [ -28.0000,    4.4688,    8.7500,   -0.3906,  -41.2500,  -58.7500,    5.1875,   13.2500,  -10.3750,   35.2500,   62.0000,    6.3750,   63.7500,  -50.5000,   35.7500,   -9.3750],
        [   0.3125,   23.1250,  -21.5000,  -30.1250,   11.0625,  -11.0000,  -20.7500,   15.2500,  -11.8125,   -5.8438,   21.0000,    5.5312,   -1.0156,  -25.2500,   -2.4375,   14.9375],
        [  -3.0938,    2.2812,   16.6250,  -26.2500,  -27.5000,  -49.5000,    4.2812,    7.2188,   -9.9375,  -36.5000,   24.6250,    6.8750,   44.2500,    1.6719,   33.7500,  -23.1250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.3809,      0.2295,      1.1250,      0.2520,     -0.3418,     -0.5625,     -1.2031,     -0.0469,     -0.0320,     -0.3906,     -2.1875,      0.5000,     -0.7031,     -0.1250,      1.8125,     -0.2969],
        [     4.7500,      2.3281,     -0.2539,      0.7969,      3.3438,     -6.0625,      1.7578,     -3.5156,      1.6094,     -4.0938,     -7.4062,      0.6758,     -0.9062,     -0.4570,     -6.8750,      2.0625],
        [     0.2188,      5.5938,      2.1875,     -1.0859,      3.7500,     -1.1797,      3.2500,      0.7148,     -5.9062,     -7.0312,      3.1406,      4.0000,      1.8125,     -0.3594,      2.3906,     -1.0469],
        [    11.4375,      6.2188,      8.3125,     -3.6875,      2.9219,     -3.4375,     -0.3203,      0.0322,     -5.6875,     -2.9375,      2.7344,     -5.3438,      3.1875,     -0.5039,     -3.2656,     -4.5312],
        [     8.3750,      3.5938,      6.0312,     -3.3438,      0.9062,      1.7266,     -2.6875,     -1.8750,     -4.4062,     -3.0000,     -0.4258,     -2.0469,     -0.9219,     -0.5586,      3.1094,     -2.9375],
        [     3.2031,      1.8125,      2.5469,      5.3438,      1.4453,     -0.6484,     -0.6914,     -6.5000,      3.6562,     -9.2500,     16.3750,     -1.6328,      2.5938,     -0.0121,      6.6562,     -2.3281],
        [    -9.6875,      5.3125,      7.3438,     -1.8125,     11.0000,     -8.9375,     -0.6602,      0.1235,     -0.6484,     -2.2969,     -1.5625,     -5.5000,      4.7500,     -0.3477,      0.1777,     -5.0000],
        [   -23.0000,     -4.0625,     -0.6602,     -2.0000,      7.0000,     -8.8125,      0.8242,     -4.4688,     -0.2246,     -3.3750,     -3.6406,      2.1875,      1.7188,      0.2256,      8.6875,     -3.9531],
        [   -10.0625,     -1.1797,      1.9609,    -14.5625,      8.1875,    -13.7500,     -1.0078,     -1.8203,     -2.2188,     -1.6562,    -10.1875,      9.9375,      0.2539,      0.0986,      9.3750,     -4.2500],
        [    -2.9219,      3.8281,     -7.8438,      3.8750,      1.1016,     -3.3125,      2.6406,     -7.0312,     -0.9883,      4.9062,     -5.8125,     -2.5469,      4.1875,     -0.2930,      0.3750,      1.9844]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0016,      0.0583,      0.0173,      0.0029,      0.0193,     -0.0559,     -0.0374,     -0.0427,      0.0123,     -0.0344,      0.0830,      0.1040,      0.0344,      0.0601,      0.0025,     -0.0713],
        [     0.1855,      0.2109,      0.0199,      0.2285,     -1.0781,     -2.3438,      1.0312,      0.1475,     -0.5117,     -0.0168,      0.5586,      0.7188,      1.8984,      0.1504,      1.0547,     -0.4727],
        [     1.2188,      1.1484,      0.6367,     -0.8477,     -0.3242,     -2.3906,      0.3867,     -0.0359,     -1.3203,     -0.4707,      0.5117,      1.3672,      2.1719,     -1.4844,      2.1719,     -0.3594],
        [    -0.3457,      1.2188,      1.1641,      0.5469,     -0.3535,     -1.9141,      0.6680,      1.0703,     -1.6484,     -1.6250,      0.5508,      1.0391,      1.8359,      0.6406,      1.2500,     -0.3574],
        [    -0.5508,      1.1406,      0.0859,     -0.5742,     -2.4062,     -1.0781,      0.9414,      1.5625,     -2.1875,      0.1533,      1.5781,      1.5000,      0.7109,      0.8516,      1.2188,     -1.5078],
        [     0.2314,      0.9414,      0.4316,     -1.0078,     -1.9766,     -1.7891,      0.6562,      0.0503,     -1.1094,     -2.0156,      1.8906,      1.7109,      1.2891,     -1.0859,      2.7344,     -2.2031],
        [     0.5469,      1.4688,     -0.0264,     -2.6875,     -1.1562,     -1.6562,      0.3691,      0.3984,      0.5898,     -1.0391,      0.6211,      1.9375,     -0.0767,      0.4258,      0.2363,     -1.3203],
        [    -1.7734,      0.0181,      0.4043,     -0.1060,     -1.0703,     -1.6719,      0.2637,      0.3711,     -0.4355,      1.2422,      1.9688,      0.3320,      2.5156,     -3.7500,      1.7891,     -0.5312],
        [    -0.3398,      0.9766,     -0.9805,     -1.9922,      0.6016,     -0.6172,     -0.9648,      0.5703,     -0.5781,     -0.2949,      0.3672,      0.6055,     -0.0295,     -1.8906,      0.2812,      0.4277],
        [    -0.2217,      0.2852,      0.4629,     -1.0469,     -0.8750,     -1.3906,      0.3223,      0.0084,     -0.4746,     -1.2969,      0.6680,      0.1787,      1.9766,      0.1089,      1.4531,     -0.8906]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -0.9922,   29.0000,    7.5938,    1.4375,   13.5000,  -49.5000,  -18.6250,  -22.1250,    6.5938,  -19.3750,   54.0000,   58.5000,   19.6250,   17.6250,    1.3750,  -39.0000],
        [   5.8125,    5.1875,    0.4336,    5.6250,  -37.7500, -103.0000,   25.5000,    3.7969,  -13.6250,   -0.4688,   18.1250,   20.1250,   53.7500,    2.2031,   28.6250,  -12.9375],
        [  37.7500,   27.8750,   13.6875,  -20.6250,  -11.1875, -103.5000,    9.4375,   -0.9102,  -34.7500,  -13.0000,   16.3750,   37.7500,   60.2500,  -21.3750,   57.7500,   -9.6875],
        [  -9.8125,   27.2500,   23.0000,   12.1875,  -11.1875,  -76.5000,   15.0000,   25.0000,  -39.7500,  -41.2500,   16.1250,   26.3750,   47.2500,    8.5000,   30.7500,   -8.8750],
        [ -16.3750,   26.7500,    1.7812,  -13.3750,  -79.5000,  -45.0000,   22.0000,   38.0000,  -55.5000,    4.0625,   48.5000,   40.0000,   19.1250,   11.7500,   31.2500,  -39.0000],
        [   6.8125,   21.7500,    8.9375,  -23.3750,  -65.0000,  -74.0000,   15.3125,    1.2188,  -27.8750,  -53.2500,   57.5000,   45.0000,   34.2500,  -14.9375,   69.5000,  -56.7500],
        [  15.8125,   33.2500,   -0.5312,  -60.7500,  -37.0000,  -67.0000,    8.3750,    9.5000,   14.5000,  -26.6250,   18.5000,   49.7500,   -2.0000,    5.7188,    5.8750,  -33.2500],
        [ -51.0000,    0.4062,    8.0625,   -2.3906,  -34.2500,  -67.5000,    6.0000,    8.7500,  -10.6250,   31.8750,   58.2500,    8.5625,   65.5000,  -50.2500,   44.5000,  -13.3125],
        [  -9.7500,   22.0000,  -19.5000,  -44.7500,   19.2500,  -24.7500,  -21.7500,   13.4375,  -14.0000,   -7.5000,   10.8125,   15.5000,   -0.7617,  -25.1250,    6.9375,   10.6875],
        [  -6.0000,    6.1250,    8.7500,  -22.3750,  -26.3750,  -52.7500,    6.9375,    0.1875,  -10.9375,  -31.6250,   18.7500,    4.3125,   48.5000,    1.3750,   34.0000,  -21.1250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -0.3750,   3.1562,   2.4219,   9.1875,  -0.2490,  -2.7188,   1.1641,  -2.6719,  -0.7383,  -3.9531,  -0.4238,   6.3750,   0.5781,   0.0669,  -2.1719,   0.5938],
        [ -5.5000,   7.1250,   1.6094, -21.5000, -10.4375,  10.8750,  10.6875,  17.6250,  -4.4062,   7.1250, -25.8750,  -6.7188,   5.1875,   0.1934,  11.1250, -12.6875],
        [ 24.7500,  -3.4531,  -7.8438, -19.6250,  -0.4785,   4.5625,   6.3125,  10.3750,   1.2344,   5.7188,   3.3750,   5.8438,   3.5312,  -0.1777,  -4.9375,  -8.7500],
        [ 10.6875,  12.8125,   6.3125,  -5.6250,   0.1582,  -3.0469,   9.3750,  18.5000,   5.7812,   0.2949,  -5.3750,  11.6875,  -0.1245,  -0.3359,   1.2656,  -6.6875],
        [ 21.3750,  -1.0469,   2.7031, -13.1250,  -2.4062,  -6.9375,  11.1250,   0.7695,   1.6953,   0.4023,  10.6875,  -7.8750,  -1.3125,  -0.1797,   1.1641,   4.1875],
        [-21.1250,  -4.5312, -21.8750, -13.9375,  -6.4375,   8.7500,  26.0000,   3.7031,   4.3750,  12.6250,  -4.9688,  28.8750,  -2.1875,  -0.5352,  12.1250, -11.8750],
        [-17.1250,   9.5625,  -2.4375,  -4.5312, -11.0625,   1.2969,  -6.6875,  10.1875,   6.7188,   0.1387, -13.6250,  10.6875,   1.0234,  -0.5898,  16.3750,  -2.0000],
        [-17.3750,   5.0938,  -2.4375,  -2.3281, -23.5000,   2.5312,  -0.2773, -12.8125,  17.7500,  -2.2031, -24.3750,  11.0000,   8.0625,  -0.1099,   6.5938,  -2.4062],
        [  8.3750,   5.0312,  19.0000,  19.1250, -20.5000,   1.8984,  -1.0312, -19.7500,   5.9375,  -5.3125,  -0.0967,   5.7812,  -6.2188,   0.7383,  -0.6445,  -2.4375],
        [ -1.5938,   6.4688,  -4.8438,   8.6250, -13.2500,   6.0938, -17.2500,   0.7148,  -6.2500,  -1.2500,   1.7266,   0.2852,  -3.1406,  -1.0078,   5.9688,  -7.3750]]
-------------------------
name='positions layer 14'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 14'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -0.3750,   3.1562,   2.4219,   9.1875,  -0.2490,  -2.7188,   1.1641,  -2.6719,  -0.7383,  -3.9531,  -0.4238,   6.3750,   0.5781,   0.0669,  -2.1719,   0.5938],
        [ -5.5000,   7.1250,   1.6094, -21.5000, -10.4375,  10.8750,  10.6875,  17.6250,  -4.4062,   7.1250, -25.8750,  -6.7188,   5.1875,   0.1934,  11.1250, -12.6875],
        [ 24.7500,  -3.4531,  -7.8438, -19.6250,  -0.4785,   4.5625,   6.3125,  10.3750,   1.2344,   5.7188,   3.3750,   5.8438,   3.5312,  -0.1777,  -4.9375,  -8.7500],
        [ 10.6875,  12.8125,   6.3125,  -5.6250,   0.1582,  -3.0469,   9.3750,  18.5000,   5.7812,   0.2949,  -5.3750,  11.6875,  -0.1245,  -0.3359,   1.2656,  -6.6875],
        [ 21.3750,  -1.0469,   2.7031, -13.1250,  -2.4062,  -6.9375,  11.1250,   0.7695,   1.6953,   0.4023,  10.6875,  -7.8750,  -1.3125,  -0.1797,   1.1641,   4.1875],
        [-21.1250,  -4.5312, -21.8750, -13.9375,  -6.4375,   8.7500,  26.0000,   3.7031,   4.3750,  12.6250,  -4.9688,  28.8750,  -2.1875,  -0.5352,  12.1250, -11.8750],
        [-17.1250,   9.5625,  -2.4375,  -4.5312, -11.0625,   1.2969,  -6.6875,  10.1875,   6.7188,   0.1387, -13.6250,  10.6875,   1.0234,  -0.5898,  16.3750,  -2.0000],
        [-17.3750,   5.0938,  -2.4375,  -2.3281, -23.5000,   2.5312,  -0.2773, -12.8125,  17.7500,  -2.2031, -24.3750,  11.0000,   8.0625,  -0.1099,   6.5938,  -2.4062],
        [  8.3750,   5.0312,  19.0000,  19.1250, -20.5000,   1.8984,  -1.0312, -19.7500,   5.9375,  -5.3125,  -0.0967,   5.7812,  -6.2188,   0.7383,  -0.6445,  -2.4375],
        [ -1.5938,   6.4688,  -4.8438,   8.6250, -13.2500,   6.0938, -17.2500,   0.7148,  -6.2500,  -1.2500,   1.7266,   0.2852,  -3.1406,  -1.0078,   5.9688,  -7.3750]]
name='residual layer 14'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -0.9922,   29.0000,    7.5938,    1.4375,   13.5000,  -49.5000,  -18.6250,  -22.1250,    6.5938,  -19.3750,   54.0000,   58.5000,   19.6250,   17.6250,    1.3750,  -39.0000],
        [   5.8125,    5.1875,    0.4336,    5.6250,  -37.7500, -103.0000,   25.5000,    3.7969,  -13.6250,   -0.4688,   18.1250,   20.1250,   53.7500,    2.2031,   28.6250,  -12.9375],
        [  37.7500,   27.8750,   13.6875,  -20.6250,  -11.1875, -103.5000,    9.4375,   -0.9102,  -34.7500,  -13.0000,   16.3750,   37.7500,   60.2500,  -21.3750,   57.7500,   -9.6875],
        [  -9.8125,   27.2500,   23.0000,   12.1875,  -11.1875,  -76.5000,   15.0000,   25.0000,  -39.7500,  -41.2500,   16.1250,   26.3750,   47.2500,    8.5000,   30.7500,   -8.8750],
        [ -16.3750,   26.7500,    1.7812,  -13.3750,  -79.5000,  -45.0000,   22.0000,   38.0000,  -55.5000,    4.0625,   48.5000,   40.0000,   19.1250,   11.7500,   31.2500,  -39.0000],
        [   6.8125,   21.7500,    8.9375,  -23.3750,  -65.0000,  -74.0000,   15.3125,    1.2188,  -27.8750,  -53.2500,   57.5000,   45.0000,   34.2500,  -14.9375,   69.5000,  -56.7500],
        [  15.8125,   33.2500,   -0.5312,  -60.7500,  -37.0000,  -67.0000,    8.3750,    9.5000,   14.5000,  -26.6250,   18.5000,   49.7500,   -2.0000,    5.7188,    5.8750,  -33.2500],
        [ -51.0000,    0.4062,    8.0625,   -2.3906,  -34.2500,  -67.5000,    6.0000,    8.7500,  -10.6250,   31.8750,   58.2500,    8.5625,   65.5000,  -50.2500,   44.5000,  -13.3125],
        [  -9.7500,   22.0000,  -19.5000,  -44.7500,   19.2500,  -24.7500,  -21.7500,   13.4375,  -14.0000,   -7.5000,   10.8125,   15.5000,   -0.7617,  -25.1250,    6.9375,   10.6875],
        [  -6.0000,    6.1250,    8.7500,  -22.3750,  -26.3750,  -52.7500,    6.9375,    0.1875,  -10.9375,  -31.6250,   18.7500,    4.3125,   48.5000,    1.3750,   34.0000,  -21.1250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0023,      0.0664,      0.0183,      0.0167,      0.0227,     -0.1172,     -0.0356,     -0.0474,      0.0118,     -0.0439,      0.1030,      0.1113,      0.0576,      0.0466,     -0.0014,     -0.0625],
        [     0.0078,      0.3809,      0.0557,     -0.3730,     -1.2344,     -3.1094,      1.1016,      0.6133,     -0.5430,      0.1885,     -0.2217,      0.3418,      2.5156,      0.0942,      1.0391,     -0.6250],
        [     1.4609,      0.7070,      0.1494,     -0.8867,     -0.2793,     -3.1094,      0.4492,      0.2539,     -0.9453,     -0.1924,      0.5312,      1.0469,      2.5469,     -0.8008,      1.3047,     -0.4219],
        [     0.0223,      1.2734,      0.8164,      0.1582,     -0.2891,     -2.7344,      0.7617,      1.2734,     -1.0469,     -1.1797,      0.3164,      1.0000,      2.0469,      0.3301,      0.8594,     -0.3887],
        [     0.1260,      0.8086,      0.1235,     -0.6328,     -2.1250,     -1.7656,      1.0234,      1.1250,     -1.6484,      0.1279,      1.7188,      0.8359,      0.7656,      0.4609,      0.8594,     -0.8594],
        [    -0.3516,      0.5273,     -0.3516,     -0.8711,     -1.8125,     -2.1719,      1.2422,      0.1396,     -0.7031,     -1.1328,      1.4922,      1.8750,      1.3516,     -0.6094,      2.1250,     -1.6641],
        [    -0.0344,      1.3984,     -0.0854,     -1.6172,     -1.3047,     -2.3281,      0.0544,      0.5938,      0.6758,     -0.7891,      0.1475,      1.6406,     -0.0439,      0.2139,      0.6172,     -0.9102],
        [    -1.7500,      0.1748,      0.1592,     -0.1147,     -1.5234,     -2.2656,      0.1807,     -0.1201,      0.2217,      0.8633,      1.0000,      0.5195,      3.2344,     -2.0469,      1.3828,     -0.3945],
        [    -0.0361,      0.8828,     -0.0144,     -0.6367,     -0.0339,     -0.8125,     -0.7344,     -0.1914,     -0.2578,     -0.3828,      0.3242,      0.5781,     -0.3145,     -1.0156,      0.1748,      0.2139],
        [    -0.2207,      0.4531,      0.1245,     -0.3789,     -1.1797,     -1.8203,     -0.3672,      0.0300,     -0.6055,     -1.0859,      0.6836,      0.1377,      2.2500,      0.0168,      1.2188,     -0.8125]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -1.3672,  32.2500,  10.0000,  10.6250,  13.2500, -52.2500, -17.5000, -24.7500,   5.8438, -23.3750,  53.5000,  65.0000,  20.2500,  17.7500,  -0.7969, -38.5000],
        [  0.3125,  12.3125,   2.0469, -15.8750, -48.2500, -92.0000,  36.2500,  21.3750, -18.0000,   6.6562,  -7.7500,  13.3750,  59.0000,   2.3906,  39.7500, -25.6250],
        [ 62.5000,  24.3750,   5.8438, -40.2500, -11.6875, -99.0000,  15.7500,   9.4375, -33.5000,  -7.2812,  19.7500,  43.5000,  63.7500, -21.5000,  52.7500, -18.5000],
        [  0.8750,  40.0000,  29.2500,   6.5625, -11.0000, -79.5000,  24.3750,  43.5000, -34.0000, -41.0000,  10.7500,  38.0000,  47.2500,   8.1875,  32.0000, -15.5625],
        [  5.0000,  25.7500,   4.5000, -26.5000, -82.0000, -52.0000,  33.0000,  38.7500, -53.7500,   4.4688,  59.2500,  32.0000,  17.7500,  11.5625,  32.5000, -34.7500],
        [-14.3125,  17.2500, -12.9375, -37.2500, -71.5000, -65.0000,  41.2500,   4.9375, -23.5000, -40.5000,  52.5000,  74.0000,  32.0000, -15.5000,  81.5000, -68.5000],
        [ -1.3125,  42.7500,  -2.9688, -65.5000, -48.0000, -65.5000,   1.6875,  19.7500,  21.2500, -26.5000,   4.8750,  60.5000,  -0.9766,   5.1250,  22.2500, -35.2500],
        [-68.5000,   5.5000,   5.6250,  -4.7188, -57.7500, -65.0000,   5.7188,  -4.0625,   7.1250,  29.6250,  34.0000,  19.5000,  73.5000, -50.2500,  51.0000, -15.7500],
        [ -1.3750,  27.0000,  -0.5000, -25.6250,  -1.2500, -22.8750, -22.7500,  -6.3125,  -8.0625, -12.8125,  10.6875,  21.2500,  -6.9688, -24.3750,   6.2812,   8.2500],
        [ -7.5938,  12.6250,   3.9062, -13.7500, -39.5000, -46.7500, -10.3125,   0.9023, -17.2500, -33.0000,  20.5000,   4.5938,  45.2500,   0.3672,  40.0000, -28.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.6328,     -1.2500,     -1.2656,     -0.3672,      1.5703,     -2.5469,      4.2812,     -0.2471,     -0.2598,      1.4375,      0.8789,      2.8281,     -0.5352,     -0.1191,      3.7344,     -1.1094],
        [   -16.7500,      1.1406,     -1.5625,     -4.5625,     -1.2812,     -1.8828,     -0.4980,      0.7695,     -4.6875,      0.9297,     17.0000,     -6.9062,      0.4160,     -0.2471,      3.7500,     -2.8125],
        [   -12.3125,      3.4844,      2.8438,     -6.0625,      3.6406,     -2.3594,     -0.7109,      5.8438,     -7.2812,     -0.1816,     27.7500,     -4.5625,     -0.1904,     -0.1426,      2.4062,     -0.2129],
        [   -18.1250,     -1.8828,     -1.1875,     -3.0312,      0.0483,     -4.1562,     -2.4375,      5.3125,     -8.5000,      2.1094,     23.3750,     -7.5312,     -0.4648,      0.0013,      4.2500,     -0.2383],
        [   -33.2500,      0.4883,     -2.5469,     -4.2188,     -0.8984,     -3.3125,     -0.9844,      3.3438,     -3.0781,      4.5312,     27.1250,     -3.2188,      0.2305,     -0.1182,      5.6875,      1.6719],
        [   -24.1250,      0.7422,      0.1846,     -2.7969,      4.9375,     -2.5312,     -5.9688,      8.1250,     -5.9688,     -3.5938,     16.5000,     -6.8125,     -0.2812,     -0.1494,      4.6875,      5.2188],
        [   -23.6250,     -1.9062,     -7.4062,     -0.4082,      2.0156,     -3.2812,     -4.0312,     12.0000,    -10.2500,     -3.9844,     24.5000,    -11.0625,      1.8828,      0.0659,      2.6250,      0.2559],
        [     0.8242,     -6.0000,     -6.6250,     -0.8789,     -2.6719,     -1.0625,      3.9375,      6.5625,     -8.8750,     -1.9766,     25.6250,    -14.9375,      1.1562,     -0.0767,      0.8594,     -2.8281],
        [   -24.3750,      2.1250,    -10.2500,     -9.6250,    -10.9375,     -0.1299,      5.5000,     -3.0312,     -5.3438,     -0.9805,     19.5000,     -4.6562,      3.6875,      0.1084,     -2.4219,     -4.1562],
        [   -16.7500,     -8.4375,     -4.2188,     -1.4219,     -1.8750,      0.6836,     -3.2812,      6.8438,     -4.4375,     -0.6992,     -7.0938,     -1.8359,      1.9844,     -0.2812,      9.1250,     -7.2188]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.0010,      0.0613,      0.0198,      0.0198,      0.0236,     -0.0762,     -0.0251,     -0.0498,      0.0117,     -0.0415,      0.0747,      0.1201,      0.0369,      0.0781,      0.0054,     -0.0771],
        [    -0.3242,      0.3945,      0.0162,     -0.5820,     -1.1641,     -1.9375,      1.0078,      0.6562,     -0.7031,      0.2139,      0.1875,      0.1709,      1.6484,      0.1406,      1.1875,     -0.8125],
        [     1.0859,      0.8984,      0.3203,     -1.4531,     -0.2070,     -2.2969,      0.4648,      0.4961,     -1.3828,     -0.2305,      1.0625,      1.1250,      1.9297,     -1.5547,      1.6562,     -0.5898],
        [    -0.3770,      1.2344,      1.0391,      0.1113,     -0.2832,     -1.9062,      0.6797,      1.5938,     -1.4531,     -1.2031,      0.7656,      0.8867,      1.4297,      0.5898,      1.0938,     -0.5000],
        [    -0.6328,      0.8750,      0.0742,     -0.9961,     -2.2188,     -1.2969,      1.0234,      1.4141,     -2.0000,      0.2871,      1.9922,      0.8594,      0.5664,      0.8516,      1.1875,     -1.0781],
        [    -0.8281,      0.5781,     -0.4688,     -1.2500,     -1.7031,     -1.5234,      1.0859,      0.4238,     -1.0000,     -1.3516,      1.5391,      1.9375,      0.9609,     -1.1250,      2.5781,     -1.9922],
        [    -0.5703,      1.3906,     -0.4023,     -2.1875,     -1.2578,     -1.6484,     -0.0762,      1.0938,      0.3945,     -1.0000,      0.6914,      1.5156,      0.0291,      0.3945,      0.7930,     -1.1641],
        [    -1.6094,     -0.0177,     -0.0403,     -0.1924,     -1.7188,     -1.6406,      0.3281,      0.0894,     -0.0654,      0.9414,      1.4609,      0.1445,      2.5000,     -3.9844,      1.7109,     -0.6445],
        [    -0.6055,      1.0078,     -0.4277,     -1.1953,     -0.3398,     -0.5625,     -0.5781,     -0.3281,     -0.4922,     -0.4590,      0.7305,      0.5195,     -0.1079,     -1.8828,      0.1260,      0.1396],
        [    -0.6250,      0.1602,     -0.0136,     -0.5664,     -1.2656,     -1.2344,     -0.5000,      0.2988,     -0.8711,     -1.2344,      0.3555,      0.0947,      1.7031,      0.0073,      1.7500,     -1.3359]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.7344,     31.0000,      8.7500,     10.2500,     14.8125,    -54.7500,    -13.2500,    -25.0000,      5.5938,    -22.0000,     54.5000,     68.0000,     19.7500,     17.6250,      2.9375,    -39.5000],
        [   -16.5000,     13.4375,      0.4844,    -20.5000,    -49.5000,    -94.0000,     35.7500,     22.1250,    -22.7500,      7.5938,      9.2500,      6.4688,     59.5000,      2.1406,     43.5000,    -28.5000],
        [    50.2500,     27.8750,      8.6875,    -46.2500,     -8.0625,   -101.5000,     15.0625,     15.2500,    -40.7500,     -7.4688,     47.5000,     39.0000,     63.5000,    -21.6250,     55.2500,    -18.7500],
        [   -17.2500,     38.0000,     28.0000,      3.5312,    -10.9375,    -83.5000,     22.0000,     48.7500,    -42.5000,    -39.0000,     34.0000,     30.5000,     46.7500,      8.1875,     36.2500,    -15.8125],
        [   -28.2500,     26.2500,      1.9531,    -30.7500,    -83.0000,    -55.2500,     32.0000,     42.0000,    -56.7500,      9.0000,     86.5000,     28.7500,     18.0000,     11.4375,     38.2500,    -33.0000],
        [   -38.5000,     18.0000,    -12.7500,    -40.0000,    -66.5000,    -67.5000,     35.2500,     13.0625,    -29.5000,    -44.0000,     69.0000,     67.0000,     31.7500,    -15.6250,     86.0000,    -63.2500],
        [   -25.0000,     40.7500,    -10.3750,    -66.0000,    -46.0000,    -69.0000,     -2.3438,     31.7500,     11.0000,    -30.5000,     29.3750,     49.5000,      0.9062,      5.1875,     24.8750,    -35.0000],
        [   -67.5000,     -0.5000,     -1.0000,     -5.5938,    -60.5000,    -66.0000,      9.6250,      2.5000,     -1.7500,     27.6250,     59.5000,      4.5625,     74.5000,    -50.2500,     51.7500,    -18.6250],
        [   -25.7500,     29.1250,    -10.7500,    -35.2500,    -12.1875,    -23.0000,    -17.2500,     -9.3750,    -13.3750,    -13.8125,     30.2500,     16.6250,     -3.2812,    -24.2500,      3.8594,      4.0938],
        [   -24.3750,      4.1875,     -0.3125,    -15.1875,    -41.5000,    -46.0000,    -13.6250,      7.7500,    -21.7500,    -33.7500,     13.3750,      2.7500,     47.2500,      0.0859,     49.0000,    -35.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -2.0938,     11.7500,     -3.3125,      7.5625,      4.8438,     -0.9922,      9.3125,     -4.7500,     -3.2969,      1.2891,      0.4316,      3.7500,      3.4375,      0.5156,     -7.6562,     -0.8438],
        [    -1.5391,     -4.8438,     23.1250,     -1.8203,     -6.8125,      0.5625,      3.3594,      0.1748,     -6.3438,      0.1699,    -40.5000,      0.3809,     -1.9688,     -0.3301,      7.3438,     -2.8125],
        [     7.5312,      8.4375,      6.8438,      8.0625,     12.7500,      1.9844,      0.0179,     -0.3105,      6.7188,      1.4453,     -3.9375,     -2.8594,      2.2812,      0.0425,     11.6250,      8.0000],
        [    -9.3750,      3.0625,     -3.0156,     13.5625,      3.3281,     -0.9141,     -2.9844,      7.9375,     -6.2812,      8.3750,     -2.6562,      5.3438,      9.0000,     -0.1406,      8.0625,    -13.1875],
        [   -40.0000,     -3.9844,     -2.0938,    -10.6875,    -17.7500,     -8.6250,      4.9375,     19.7500,     -0.0654,     13.0000,      9.1875,     -5.2812,      0.3203,      0.5430,     17.0000,     -6.7500],
        [   -36.0000,      0.3789,     -5.7500,     10.0000,     -7.5000,     -0.1543,      3.8594,     -7.0938,     -5.3125,     11.2500,     -0.1729,     -4.2500,     10.8125,      0.0071,      4.9688,      1.8359],
        [   -24.0000,     -0.2246,     -0.4023,      1.9453,    -15.0625,     -8.1250,     -1.8359,      9.1250,     -1.9766,     -1.5547,     -4.1875,     -7.8750,      8.1250,     -0.3418,    -11.3125,    -16.2500],
        [   -64.0000,     -0.6797,    -14.4375,    -19.6250,    -22.0000,     -7.5000,     18.0000,      3.7500,      8.0625,     13.0000,    -10.4375,      4.4062,     -8.1875,     -1.6172,     -4.1562,      8.1875],
        [    -6.3750,    -11.9375,      3.6094,     -6.0000,    -12.0625,     -7.5000,    -40.7500,      1.1719,     -4.0938,     16.7500,     -1.6016,    -22.6250,      4.6250,     -1.0078,    -14.0000,     -1.7891],
        [   -21.8750,     -3.1094,      6.4688,     11.5625,      5.4062,     -1.8750,      1.2656,     -0.8359,     -8.5000,     12.1250,    -13.0000,      5.6250,      6.0312,     -0.2471,     -9.0625,     -9.1250]]
-------------------------
name='positions layer 15'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 15'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -2.0938,     11.7500,     -3.3125,      7.5625,      4.8438,     -0.9922,      9.3125,     -4.7500,     -3.2969,      1.2891,      0.4316,      3.7500,      3.4375,      0.5156,     -7.6562,     -0.8438],
        [    -1.5391,     -4.8438,     23.1250,     -1.8203,     -6.8125,      0.5625,      3.3594,      0.1748,     -6.3438,      0.1699,    -40.5000,      0.3809,     -1.9688,     -0.3301,      7.3438,     -2.8125],
        [     7.5312,      8.4375,      6.8438,      8.0625,     12.7500,      1.9844,      0.0179,     -0.3105,      6.7188,      1.4453,     -3.9375,     -2.8594,      2.2812,      0.0425,     11.6250,      8.0000],
        [    -9.3750,      3.0625,     -3.0156,     13.5625,      3.3281,     -0.9141,     -2.9844,      7.9375,     -6.2812,      8.3750,     -2.6562,      5.3438,      9.0000,     -0.1406,      8.0625,    -13.1875],
        [   -40.0000,     -3.9844,     -2.0938,    -10.6875,    -17.7500,     -8.6250,      4.9375,     19.7500,     -0.0654,     13.0000,      9.1875,     -5.2812,      0.3203,      0.5430,     17.0000,     -6.7500],
        [   -36.0000,      0.3789,     -5.7500,     10.0000,     -7.5000,     -0.1543,      3.8594,     -7.0938,     -5.3125,     11.2500,     -0.1729,     -4.2500,     10.8125,      0.0071,      4.9688,      1.8359],
        [   -24.0000,     -0.2246,     -0.4023,      1.9453,    -15.0625,     -8.1250,     -1.8359,      9.1250,     -1.9766,     -1.5547,     -4.1875,     -7.8750,      8.1250,     -0.3418,    -11.3125,    -16.2500],
        [   -64.0000,     -0.6797,    -14.4375,    -19.6250,    -22.0000,     -7.5000,     18.0000,      3.7500,      8.0625,     13.0000,    -10.4375,      4.4062,     -8.1875,     -1.6172,     -4.1562,      8.1875],
        [    -6.3750,    -11.9375,      3.6094,     -6.0000,    -12.0625,     -7.5000,    -40.7500,      1.1719,     -4.0938,     16.7500,     -1.6016,    -22.6250,      4.6250,     -1.0078,    -14.0000,     -1.7891],
        [   -21.8750,     -3.1094,      6.4688,     11.5625,      5.4062,     -1.8750,      1.2656,     -0.8359,     -8.5000,     12.1250,    -13.0000,      5.6250,      6.0312,     -0.2471,     -9.0625,     -9.1250]]
name='residual layer 15'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.7344,     31.0000,      8.7500,     10.2500,     14.8125,    -54.7500,    -13.2500,    -25.0000,      5.5938,    -22.0000,     54.5000,     68.0000,     19.7500,     17.6250,      2.9375,    -39.5000],
        [   -16.5000,     13.4375,      0.4844,    -20.5000,    -49.5000,    -94.0000,     35.7500,     22.1250,    -22.7500,      7.5938,      9.2500,      6.4688,     59.5000,      2.1406,     43.5000,    -28.5000],
        [    50.2500,     27.8750,      8.6875,    -46.2500,     -8.0625,   -101.5000,     15.0625,     15.2500,    -40.7500,     -7.4688,     47.5000,     39.0000,     63.5000,    -21.6250,     55.2500,    -18.7500],
        [   -17.2500,     38.0000,     28.0000,      3.5312,    -10.9375,    -83.5000,     22.0000,     48.7500,    -42.5000,    -39.0000,     34.0000,     30.5000,     46.7500,      8.1875,     36.2500,    -15.8125],
        [   -28.2500,     26.2500,      1.9531,    -30.7500,    -83.0000,    -55.2500,     32.0000,     42.0000,    -56.7500,      9.0000,     86.5000,     28.7500,     18.0000,     11.4375,     38.2500,    -33.0000],
        [   -38.5000,     18.0000,    -12.7500,    -40.0000,    -66.5000,    -67.5000,     35.2500,     13.0625,    -29.5000,    -44.0000,     69.0000,     67.0000,     31.7500,    -15.6250,     86.0000,    -63.2500],
        [   -25.0000,     40.7500,    -10.3750,    -66.0000,    -46.0000,    -69.0000,     -2.3438,     31.7500,     11.0000,    -30.5000,     29.3750,     49.5000,      0.9062,      5.1875,     24.8750,    -35.0000],
        [   -67.5000,     -0.5000,     -1.0000,     -5.5938,    -60.5000,    -66.0000,      9.6250,      2.5000,     -1.7500,     27.6250,     59.5000,      4.5625,     74.5000,    -50.2500,     51.7500,    -18.6250],
        [   -25.7500,     29.1250,    -10.7500,    -35.2500,    -12.1875,    -23.0000,    -17.2500,     -9.3750,    -13.3750,    -13.8125,     30.2500,     16.6250,     -3.2812,    -24.2500,      3.8594,      4.0938],
        [   -24.3750,      4.1875,     -0.3125,    -15.1875,    -41.5000,    -46.0000,    -13.6250,      7.7500,    -21.7500,    -33.7500,     13.3750,      2.7500,     47.2500,      0.0859,     49.0000,    -35.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0060,  0.1030,  0.0121,  0.0386,  0.0417, -0.1348, -0.0089, -0.0679,  0.0057, -0.0483,  0.1025,  0.1416,  0.0776,  0.0708, -0.0102, -0.0859],
        [-0.4883,  0.2676,  0.6797, -0.6250, -1.5547, -2.9062,  1.1328,  0.6602, -0.9297,  0.2344, -0.7578,  0.1738,  2.4844,  0.0913,  1.4219, -0.8633],
        [ 1.5625,  1.1328,  0.4492, -1.0625,  0.1289, -3.0781,  0.4375,  0.4395, -1.0859, -0.1816,  1.0469,  0.9180,  2.8125, -1.0859,  1.8672, -0.2949],
        [-0.7812,  1.3828,  0.7773,  0.5156, -0.2266, -2.8281,  0.5977,  1.8047, -1.6875, -1.0000,  0.8203,  0.9883,  2.5938,  0.4395,  1.3359, -0.8633],
        [-1.8672,  0.6992, -0.0041, -1.1719, -2.7969, -2.0000,  1.0781,  1.8359, -1.8281,  0.6680,  2.3281,  0.6016,  0.7969,  0.6055,  1.5547, -1.1016],
        [-2.1094,  0.5977, -0.5547, -0.8750, -2.1250, -2.1875,  1.1797,  0.1826, -1.1562, -1.0312,  1.7266,  1.6641,  1.9062, -0.8203,  2.6406, -1.7656],
        [-1.4609,  1.3828, -0.3398, -1.9609, -1.8438, -2.6250, -0.1328,  1.3281,  0.3164, -1.0547,  0.6680,  1.1641,  0.4277,  0.2676,  0.4160, -1.5547],
        [-4.0312, -0.0417, -0.5039, -0.8008, -2.5781, -2.5781,  0.9062,  0.2100,  0.2285,  1.3906,  1.3438,  0.2598,  3.2500, -2.9688,  1.5078, -0.3242],
        [-0.8828,  0.5430, -0.2090, -1.1719, -0.6758, -0.9570, -1.7031, -0.2451, -0.5664,  0.0898,  0.6992, -0.1543,  0.0586, -1.2891, -0.2871,  0.0645],
        [-1.4375,  0.0386,  0.2031, -0.1157, -1.1406, -1.7031, -0.4102,  0.2344, -1.1094, -0.7461,  0.0104,  0.2441,  2.6406, -0.0093,  1.2812, -1.4141]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -2.8281,   42.7500,    5.4375,   17.7500,   19.6250,  -55.7500,   -3.9375,  -29.7500,    2.2969,  -20.7500,   55.0000,   72.0000,   23.2500,   18.1250,   -4.7188,  -40.2500],
        [ -18.0000,    8.6250,   23.6250,  -22.3750,  -56.2500,  -93.5000,   39.0000,   22.2500,  -29.1250,    7.7500,  -31.2500,    6.8438,   57.5000,    1.8125,   50.7500,  -31.2500],
        [  57.7500,   36.2500,   15.5000,  -38.2500,    4.6875,  -99.5000,   15.0625,   14.9375,  -34.0000,   -6.0312,   43.5000,   36.2500,   66.0000,  -21.6250,   67.0000,  -10.7500],
        [ -26.6250,   41.0000,   25.0000,   17.1250,   -7.6250,  -84.5000,   19.0000,   56.7500,  -48.7500,  -30.6250,   31.3750,   35.7500,   55.7500,    8.0625,   44.2500,  -29.0000],
        [ -68.0000,   22.2500,   -0.1406,  -41.5000, -101.0000,  -64.0000,   37.0000,   61.7500,  -56.7500,   22.0000,   95.5000,   23.5000,   18.3750,   12.0000,   55.2500,  -39.7500],
        [ -74.5000,   18.3750,  -18.5000,  -30.0000,  -74.0000,  -67.5000,   39.0000,    5.9688,  -34.7500,  -32.7500,   69.0000,   62.7500,   42.5000,  -15.6250,   91.0000,  -61.5000],
        [ -49.0000,   40.5000,  -10.7500,  -64.0000,  -61.0000,  -77.0000,   -4.1875,   41.0000,    9.0000,  -32.0000,   25.2500,   41.5000,    9.0000,    4.8438,   13.5625,  -51.2500],
        [-132.0000,   -1.1797,  -15.4375,  -25.2500,  -82.5000,  -73.5000,   27.6250,    6.2500,    6.3125,   40.5000,   49.0000,    9.0000,   66.5000,  -51.7500,   47.5000,  -10.4375],
        [ -32.0000,   17.2500,   -7.1250,  -41.2500,  -24.2500,  -30.5000,  -58.0000,   -8.1875,  -17.5000,    2.9375,   28.6250,   -6.0000,    1.3438,  -25.2500,  -10.1250,    2.3125],
        [ -46.2500,    1.0781,    6.1562,   -3.6250,  -36.0000,  -48.0000,  -12.3750,    6.9062,  -30.2500,  -21.6250,    0.3750,    8.3750,   53.2500,   -0.1611,   40.0000,  -45.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.9062,      0.4727,      1.3672,      0.7656,      0.3105,      0.5742,      0.0261,      0.5352,     -0.4551,     -0.5000,     -0.1079,      0.9414,      0.2910,     -0.1177,      0.3379,      0.1768],
        [    -1.4609,     -4.6562,      3.5938,      8.4375,      5.8438,      2.3281,     -5.7188,     -0.6562,      3.8906,     -3.6562,     -1.3047,     -2.8750,     -6.5000,     -0.6875,      5.5312,      3.7500],
        [    -2.9375,     -4.9688,      0.8945,      6.9062,      5.0625,      0.3398,    -12.0625,      2.4688,      5.5312,     -9.1875,     -3.6094,      1.5078,     -2.6562,     -0.7070,      3.2969,      6.6250],
        [    -2.9688,     -1.3359,      7.5312,      4.6250,      1.4844,     -0.0938,    -12.5000,     -1.6875,      2.0781,     -4.9688,      7.4688,      4.2500,     -4.2812,     -0.1147,      9.6875,      1.4766],
        [   -36.7500,     -2.1406,     16.2500,     15.2500,      2.2969,      5.5625,     -2.1562,     -3.8438,     -1.4062,     -9.8750,     -9.5000,     14.0625,     -2.4844,     -0.3301,     10.3125,     -1.6562],
        [   -10.8750,      8.5000,     15.7500,     32.0000,     -7.5312,      5.7188,     -0.9453,      6.2500,     -2.2500,    -11.0000,     -1.8047,     12.3750,     -0.6641,     -0.3730,     11.8125,     -5.9688],
        [   -14.5625,     -4.4375,     26.1250,     12.2500,     -5.4062,     -5.1250,     -1.1172,      2.2656,     -3.5625,    -20.0000,     -9.3125,     11.8125,      2.9219,     -0.0586,     12.5000,      1.0078],
        [   -46.5000,     -6.5938,     12.0000,     -2.8281,      9.0625,     -6.3750,      3.5156,      3.4531,      6.1875,     -7.6562,     10.5000,     12.8750,     -0.0184,      0.6367,     10.1875,      0.7930],
        [   -45.7500,      1.2734,      7.2812,     12.0625,      4.5625,     -3.8281,      3.9219,     -1.4844,     -2.2344,     -6.5000,    -14.3750,      8.0625,      2.8125,      0.1650,     19.0000,     -7.5000],
        [   -21.0000,      8.6875,     14.5625,     17.8750,     -9.8750,     -7.0312,    -13.8750,     -2.0625,      0.4844,      7.5625,      0.4316,     10.6875,      7.6875,      0.1934,     10.0625,     -0.6328]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0022,      0.0859,      0.0138,      0.0354,      0.0347,     -0.0903,     -0.0064,     -0.0576,      0.0041,     -0.0437,      0.0630,      0.1299,      0.0522,      0.0869,     -0.0081,     -0.0845],
        [    -0.2969,      0.1128,      0.7852,     -0.3770,     -1.2500,     -2.1250,      0.7812,      0.6055,     -0.8008,      0.1196,     -0.5352,      0.1006,      1.6094,      0.0776,      1.4766,     -0.8281],
        [     0.8359,      0.8906,      0.4785,     -0.8594,      0.2432,     -2.3281,      0.0708,      0.4902,     -0.9102,     -0.4492,      0.6562,      0.9609,      2.0156,     -1.5469,      1.8516,     -0.1250],
        [    -0.4570,      1.1406,      0.9570,      0.6016,     -0.1553,     -2.0000,      0.1553,      1.5703,     -1.5078,     -1.0625,      0.6484,      1.0312,      1.6562,      0.5586,      1.4297,     -0.8438],
        [    -1.5391,      0.5547,      0.4512,     -0.6914,     -2.3594,     -1.3281,      0.7891,      1.5703,     -1.7969,      0.3457,      1.3672,      0.9219,      0.4863,      0.7773,      1.6641,     -1.2031],
        [    -1.4297,      0.8398,     -0.0879,      0.0603,     -2.2188,     -1.5938,      0.9844,      0.3770,     -1.3047,     -1.4219,      1.2188,      2.1094,      1.4609,     -1.2188,      2.9844,     -2.2500],
        [    -1.0469,      1.1094,      0.4844,     -1.5312,     -1.7812,     -2.0938,     -0.1357,      1.3203,      0.1885,     -1.6562,      0.2852,      1.4688,      0.4121,      0.3574,      0.7461,     -1.6484],
        [    -2.7500,     -0.2236,     -0.1011,     -0.7773,     -1.8438,     -1.8984,      0.7422,      0.2773,      0.4043,      0.9844,      0.9961,      0.5625,      2.1562,     -3.5781,      1.5312,     -0.2949],
        [    -1.1562,      0.5117,      0.0044,     -0.7734,     -0.4746,     -0.7812,     -1.2344,     -0.2656,     -0.6094,     -0.1021,      0.2275,      0.0510,      0.1279,     -1.6875,      0.2266,     -0.1523],
        [    -1.0859,      0.2949,      0.6367,      0.4121,     -1.2109,     -1.3672,     -0.6562,      0.1455,     -1.0078,     -0.4395,      0.0142,      0.5156,      2.0469,      0.0024,      1.3984,     -1.4609]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     2.0781,     43.2500,      6.8125,     18.5000,     19.8750,    -55.2500,     -3.9062,    -29.2500,      1.8438,    -21.2500,     55.0000,     73.0000,     23.5000,     18.0000,     -4.3750,    -40.0000],
        [   -19.5000,      3.9688,     27.2500,    -13.9375,    -50.5000,    -91.0000,     33.2500,     21.6250,    -25.2500,      4.0938,    -32.5000,      3.9688,     51.0000,      1.1250,     56.2500,    -27.5000],
        [    54.7500,     31.2500,     16.3750,    -31.3750,      9.7500,    -99.0000,      3.0000,     17.3750,    -28.5000,    -15.2500,     40.0000,     37.7500,     63.2500,    -22.3750,     70.5000,     -4.1250],
        [   -29.6250,     39.7500,     32.5000,     21.7500,     -6.1250,    -84.5000,      6.5000,     55.0000,    -46.7500,    -35.5000,     38.7500,     40.0000,     51.5000,      7.9375,     54.0000,    -27.5000],
        [  -105.0000,     20.1250,     16.1250,    -26.2500,    -98.5000,    -58.5000,     34.7500,     58.0000,    -58.2500,     12.1250,     86.0000,     37.5000,     15.8750,     11.6875,     65.5000,    -41.5000],
        [   -85.5000,     26.8750,     -2.7500,      2.0000,    -81.5000,    -61.7500,     38.0000,     12.2500,    -37.0000,    -43.7500,     67.0000,     75.0000,     41.7500,    -16.0000,    103.0000,    -67.5000],
        [   -63.5000,     36.0000,     15.3750,    -51.7500,    -66.5000,    -82.0000,     -5.3125,     43.2500,      5.4375,    -52.0000,     15.9375,     53.2500,     11.9375,      4.7812,     26.0000,    -50.2500],
        [  -178.0000,     -7.7812,     -3.4375,    -28.1250,    -73.5000,    -80.0000,     31.1250,      9.6875,     12.5000,     32.7500,     59.5000,     21.8750,     66.5000,    -51.0000,     57.7500,     -9.6250],
        [   -78.0000,     18.5000,      0.1562,    -29.2500,    -19.7500,    -34.2500,    -54.0000,     -9.6875,    -19.7500,     -3.5625,     14.2500,      2.0625,      4.1562,    -25.1250,      8.8750,     -5.1875],
        [   -67.0000,      9.7500,     20.7500,     14.2500,    -46.0000,    -55.0000,    -26.2500,      4.8438,    -29.7500,    -14.0625,      0.8047,     19.0000,     61.0000,      0.0322,     50.0000,    -45.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.8125,     18.3750,     25.0000,    -13.3750,      9.6875,     -0.2246,     19.0000,     27.6250,     10.9375,    -55.7500,   -131.0000,    -39.5000,      4.8438,      2.7031,     21.5000,      9.7500],
        [   -66.0000,     -5.2812,    -12.2500,    -14.3125,     -8.1250,      8.4375,     -7.1875,    -21.3750,    -10.6875,    -15.1250,    -11.9375,    -20.8750,     -1.2344,      0.6328,    -20.1250,     -4.6562],
        [   -17.0000,     -3.9219,     10.6250,    -10.5625,     -4.9062,      7.5938,    -16.1250,      9.7500,    -10.5000,    -11.2500,     36.0000,      1.7266,     -4.3750,      0.8750,      3.6719,      2.4062],
        [   -21.2500,     11.3125,      1.1172,      0.0036,      0.4199,      6.0312,     -7.6875,     18.0000,      9.1250,    -10.1250,    -16.0000,      0.3359,      6.3125,      0.8945,      0.4746,      5.5938],
        [    28.1250,      9.3750,    -22.2500,    -11.0000,    -12.3750,     13.4375,     -4.3750,     17.5000,     -7.6875,     -7.0000,    -19.8750,    -15.6250,     -3.4531,      0.0019,     14.6875,     -0.6250],
        [   -26.5000,     -9.9375,     34.5000,     -4.0625,    -16.2500,     12.8125,     45.0000,     17.6250,      8.8750,     32.7500,     45.0000,     10.8750,     -1.3281,      1.6016,     65.5000,      5.3750],
        [    53.7500,     18.7500,     53.7500,    -20.8750,     -6.6250,      0.5195,    -28.8750,     35.7500,     16.0000,    -10.7500,     32.2500,     -2.1562,      0.5117,      0.8828,     30.1250,     -0.4707],
        [   -32.0000,      1.6406,     -0.8867,     -4.0000,    -19.0000,      4.9375,    -18.5000,     32.2500,     10.1875,     -4.0000,      5.7188,     32.2500,     -9.0000,      0.2012,     12.7500,     10.1875],
        [    47.7500,     14.6250,    -10.4375,      7.6250,     -1.6172,     -6.1562,    -24.7500,     23.2500,      0.2852,      5.3438,    -15.4375,      0.6055,      0.4082,     -0.1572,    -42.5000,      3.6719],
        [   -27.5000,     -4.3438,     -5.0938,    -15.8125,     16.8750,      0.0206,     11.4375,     10.5000,     -4.0000,      3.2188,     75.0000,    -14.7500,      2.1250,     -0.4863,     -1.3828,     -8.5625]]
-------------------------
name='positions layer 16'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 16'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     4.8125,     18.3750,     25.0000,    -13.3750,      9.6875,     -0.2246,     19.0000,     27.6250,     10.9375,    -55.7500,   -131.0000,    -39.5000,      4.8438,      2.7031,     21.5000,      9.7500],
        [   -66.0000,     -5.2812,    -12.2500,    -14.3125,     -8.1250,      8.4375,     -7.1875,    -21.3750,    -10.6875,    -15.1250,    -11.9375,    -20.8750,     -1.2344,      0.6328,    -20.1250,     -4.6562],
        [   -17.0000,     -3.9219,     10.6250,    -10.5625,     -4.9062,      7.5938,    -16.1250,      9.7500,    -10.5000,    -11.2500,     36.0000,      1.7266,     -4.3750,      0.8750,      3.6719,      2.4062],
        [   -21.2500,     11.3125,      1.1172,      0.0036,      0.4199,      6.0312,     -7.6875,     18.0000,      9.1250,    -10.1250,    -16.0000,      0.3359,      6.3125,      0.8945,      0.4746,      5.5938],
        [    28.1250,      9.3750,    -22.2500,    -11.0000,    -12.3750,     13.4375,     -4.3750,     17.5000,     -7.6875,     -7.0000,    -19.8750,    -15.6250,     -3.4531,      0.0019,     14.6875,     -0.6250],
        [   -26.5000,     -9.9375,     34.5000,     -4.0625,    -16.2500,     12.8125,     45.0000,     17.6250,      8.8750,     32.7500,     45.0000,     10.8750,     -1.3281,      1.6016,     65.5000,      5.3750],
        [    53.7500,     18.7500,     53.7500,    -20.8750,     -6.6250,      0.5195,    -28.8750,     35.7500,     16.0000,    -10.7500,     32.2500,     -2.1562,      0.5117,      0.8828,     30.1250,     -0.4707],
        [   -32.0000,      1.6406,     -0.8867,     -4.0000,    -19.0000,      4.9375,    -18.5000,     32.2500,     10.1875,     -4.0000,      5.7188,     32.2500,     -9.0000,      0.2012,     12.7500,     10.1875],
        [    47.7500,     14.6250,    -10.4375,      7.6250,     -1.6172,     -6.1562,    -24.7500,     23.2500,      0.2852,      5.3438,    -15.4375,      0.6055,      0.4082,     -0.1572,    -42.5000,      3.6719],
        [   -27.5000,     -4.3438,     -5.0938,    -15.8125,     16.8750,      0.0206,     11.4375,     10.5000,     -4.0000,      3.2188,     75.0000,    -14.7500,      2.1250,     -0.4863,     -1.3828,     -8.5625]]
name='residual layer 16'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     2.0781,     43.2500,      6.8125,     18.5000,     19.8750,    -55.2500,     -3.9062,    -29.2500,      1.8438,    -21.2500,     55.0000,     73.0000,     23.5000,     18.0000,     -4.3750,    -40.0000],
        [   -19.5000,      3.9688,     27.2500,    -13.9375,    -50.5000,    -91.0000,     33.2500,     21.6250,    -25.2500,      4.0938,    -32.5000,      3.9688,     51.0000,      1.1250,     56.2500,    -27.5000],
        [    54.7500,     31.2500,     16.3750,    -31.3750,      9.7500,    -99.0000,      3.0000,     17.3750,    -28.5000,    -15.2500,     40.0000,     37.7500,     63.2500,    -22.3750,     70.5000,     -4.1250],
        [   -29.6250,     39.7500,     32.5000,     21.7500,     -6.1250,    -84.5000,      6.5000,     55.0000,    -46.7500,    -35.5000,     38.7500,     40.0000,     51.5000,      7.9375,     54.0000,    -27.5000],
        [  -105.0000,     20.1250,     16.1250,    -26.2500,    -98.5000,    -58.5000,     34.7500,     58.0000,    -58.2500,     12.1250,     86.0000,     37.5000,     15.8750,     11.6875,     65.5000,    -41.5000],
        [   -85.5000,     26.8750,     -2.7500,      2.0000,    -81.5000,    -61.7500,     38.0000,     12.2500,    -37.0000,    -43.7500,     67.0000,     75.0000,     41.7500,    -16.0000,    103.0000,    -67.5000],
        [   -63.5000,     36.0000,     15.3750,    -51.7500,    -66.5000,    -82.0000,     -5.3125,     43.2500,      5.4375,    -52.0000,     15.9375,     53.2500,     11.9375,      4.7812,     26.0000,    -50.2500],
        [  -178.0000,     -7.7812,     -3.4375,    -28.1250,    -73.5000,    -80.0000,     31.1250,      9.6875,     12.5000,     32.7500,     59.5000,     21.8750,     66.5000,    -51.0000,     57.7500,     -9.6250],
        [   -78.0000,     18.5000,      0.1562,    -29.2500,    -19.7500,    -34.2500,    -54.0000,     -9.6875,    -19.7500,     -3.5625,     14.2500,      2.0625,      4.1562,    -25.1250,      8.8750,     -5.1875],
        [   -67.0000,      9.7500,     20.7500,     14.2500,    -46.0000,    -55.0000,    -26.2500,      4.8438,    -29.7500,    -14.0625,      0.8047,     19.0000,     61.0000,      0.0322,     50.0000,    -45.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.0101,  0.1533,  0.0630,  0.0083,  0.0559, -0.1377,  0.0270, -0.0032,  0.0275, -0.1699, -0.1230,  0.0640,  0.0776,  0.0610,  0.0322, -0.0625],
        [-1.1562, -0.0299,  0.2734, -0.4199, -1.0234, -1.8906,  0.4297,  0.0046, -0.7070, -0.2246, -0.6641, -0.2949,  1.2656,  0.0476,  0.6211, -0.6094],
        [ 0.4297,  0.5273,  0.4141, -0.5273,  0.0708, -1.7734, -0.1816,  0.4199, -0.6484, -0.4570,  0.9531,  0.5820,  1.2656, -0.4941,  1.0781, -0.0276],
        [-0.6250,  1.0547,  0.5586,  0.2949, -0.0903, -1.6406, -0.0178,  1.2188, -0.6758, -0.8477,  0.3086,  0.6445,  1.3359,  0.2188,  0.8516, -0.3789],
        [-0.9609,  0.6250, -0.1035, -0.5156, -1.7891, -0.9609,  0.4648,  1.2812, -1.2031,  0.0972,  0.9141,  0.3555,  0.2930,  0.2969,  1.2812, -0.7422],
        [-1.4375,  0.3652,  0.5508, -0.0292, -1.6172, -1.0703,  1.2969,  0.5195, -0.5273, -0.2129,  1.5859,  1.4297,  0.9727, -0.3730,  2.7656, -1.1172],
        [-0.1318,  1.2500,  1.2656, -1.0781, -1.2734, -1.8672, -0.5625,  1.4531,  0.4219, -1.2734,  0.7188,  0.8945,  0.3164,  0.1533,  0.9648, -0.9648],
        [-2.9062, -0.1436, -0.0806, -0.4922, -1.6562, -1.7734,  0.2129,  0.7930,  0.4590,  0.6016,  0.9961,  0.9766,  1.5000, -1.4141,  1.2422,  0.0109],
        [-0.4082,  0.7500, -0.1865, -0.3203, -0.3691, -0.9219, -1.2891,  0.2480, -0.3828,  0.0361, -0.0176,  0.0464,  0.1147, -0.6875, -0.5742, -0.0286],
        [-1.4531,  0.1396,  0.3223, -0.0264, -0.5742, -1.4297, -0.2773,  0.3184, -0.7539, -0.2500,  1.2812,  0.0845,  1.8125, -0.0140,  0.9492, -1.1719]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   6.8750,   61.5000,   31.7500,    5.1250,   29.5000,  -55.5000,   15.1250,   -1.6250,   12.7500,  -77.0000,  -76.0000,   33.5000,   28.3750,   20.7500,   17.1250,  -30.2500],
        [ -85.5000,   -1.3125,   15.0000,  -28.2500,  -58.5000,  -82.5000,   26.0000,    0.2500,  -36.0000,  -11.0000,  -44.5000,  -16.8750,   49.7500,    1.7578,   36.0000,  -32.2500],
        [  37.7500,   27.3750,   27.0000,  -42.0000,    4.8438,  -91.5000,  -13.1250,   27.1250,  -39.0000,  -26.5000,   76.0000,   39.5000,   59.0000,  -21.5000,   74.0000,   -1.7188],
        [ -51.0000,   51.0000,   33.5000,   21.7500,   -5.7188,  -78.5000,   -1.1875,   73.0000,  -37.5000,  -45.5000,   22.7500,   40.2500,   57.7500,    8.8125,   54.5000,  -21.8750],
        [ -77.0000,   29.5000,   -6.1250,  -37.2500, -111.0000,  -45.0000,   30.3750,   75.5000,  -66.0000,    5.1250,   66.0000,   21.8750,   12.4375,   11.6875,   80.0000,  -42.0000],
        [-112.0000,   17.0000,   31.7500,   -2.0625,  -98.0000,  -49.0000,   83.0000,   29.8750,  -28.1250,  -11.0000,  112.0000,   86.0000,   40.5000,  -14.3750,  168.0000,  -62.0000],
        [  -9.7500,   54.7500,   69.0000,  -72.5000,  -73.0000,  -81.5000,  -34.2500,   79.0000,   21.5000,  -62.7500,   48.2500,   51.0000,   12.4375,    5.6562,   56.0000,  -50.7500],
        [-210.0000,   -6.1250,   -4.3125,  -32.0000,  -92.5000,  -75.0000,   12.6250,   42.0000,   22.7500,   28.7500,   65.0000,   54.0000,   57.5000,  -50.7500,   70.5000,    0.5625],
        [ -30.2500,   33.0000,  -10.2500,  -21.6250,  -21.3750,  -40.5000,  -79.0000,   13.5625,  -19.5000,    1.7812,   -1.1875,    2.6719,    4.5625,  -25.2500,  -33.5000,   -1.5156],
        [ -94.5000,    5.4062,   15.6250,   -1.5625,  -29.1250,  -55.0000,  -14.8125,   15.3750,  -33.7500,  -10.8750,   76.0000,    4.2500,   63.0000,   -0.4531,   48.5000,  -54.2500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -6.1562,     -4.6875,      6.1562,      8.1250,      1.1250,     -3.1562,     -2.8438,     -4.0625,     -0.9805,     -2.1250,     22.7500,     12.0000,      9.3125,     -0.1069,     -4.3438,      8.6250],
        [    -0.2891,      1.8281,      8.0000,     -1.8359,      3.0625,      1.0625,      1.9141,     -0.1118,     -0.0442,      6.1875,     25.7500,      4.0312,      1.0078,      0.5117,     15.7500,      2.9062],
        [    14.8125,     -0.1738,     10.8750,     -3.1406,      2.9844,     -2.4219,    -17.7500,      7.6250,      5.1562,      0.3750,     13.3750,      7.0312,      3.6406,      0.0432,     10.2500,      0.5312],
        [    15.8125,     -2.1406,      1.4922,     -4.1250,      0.3555,     -1.1406,    -18.1250,     12.7500,      6.0938,      4.3125,     16.1250,      2.8750,      1.6016,      0.0918,     17.0000,     -3.2188],
        [    -9.3125,      5.9688,     -0.4316,     -7.1250,      6.1562,      5.5938,    -13.3125,      6.3125,      6.3125,      5.0938,     18.8750,     -2.8750,      2.3281,      0.0630,     13.6250,     -1.4453],
        [   -10.2500,      5.5000,      7.2188,     -6.2188,     -4.9062,      2.8281,    -14.5000,     19.8750,      6.6562,      7.2188,     26.2500,      0.7812,     -1.2500,      0.0588,      2.2188,      4.3750],
        [    -8.0625,     -2.4219,      1.6250,     -5.8438,     -7.4375,     -1.0078,    -16.5000,     25.2500,     -2.0312,      8.1875,     22.0000,     15.6250,      2.4531,      0.0532,     -0.9141,      1.3359],
        [   -22.5000,     12.5625,      8.0000,    -14.4375,      7.0625,      1.6328,    -22.0000,      9.0000,      8.9375,      8.2500,     32.7500,      3.1719,      2.3281,      0.0898,      4.6875,      2.1250],
        [   -44.5000,     19.1250,    -10.8750,    -28.7500,     -3.5938,      0.5352,      9.8125,      9.6875,      4.7500,      7.9062,     40.0000,      6.5000,      1.8516,      0.0786,     -4.1875,     -2.5469],
        [   -28.2500,      8.9375,     -3.3281,    -20.0000,     -0.1494,      0.4727,    -36.7500,      7.9375,      0.8047,      7.1562,     29.6250,     15.4375,      1.1094,     -0.1113,      5.1250,      1.2891]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.0006,      0.1177,      0.0747,      0.0243,      0.0615,     -0.1157,      0.0192,     -0.0109,      0.0302,     -0.1738,     -0.0586,      0.0879,      0.0923,      0.1641,      0.0226,     -0.0508],
        [    -0.7148,      0.0099,      0.4219,     -0.5117,     -1.0312,     -1.4922,      0.4043,      0.0024,     -0.8516,     -0.0977,     -0.1904,     -0.2285,      1.1484,      0.1670,      0.8438,     -0.6367],
        [     0.4863,      0.5781,      0.7656,     -0.8516,      0.1611,     -1.8984,     -0.4941,      0.6797,     -0.8867,     -0.5898,      1.0078,      0.9141,      1.5703,     -1.7422,      1.5156,     -0.0287],
        [    -0.3418,      1.0859,      0.7422,      0.3477,     -0.1152,     -1.6875,     -0.3242,      1.7656,     -0.8633,     -0.9727,      0.4590,      0.8906,      1.5625,      0.7617,      1.3594,     -0.6328],
        [    -0.8203,      0.7773,     -0.1367,     -0.8594,     -2.2188,     -0.8203,      0.2812,      1.6484,     -1.6094,      0.2363,      0.9844,      0.3848,      0.3828,      0.9844,      1.7422,     -1.0703],
        [    -1.0391,      0.4395,      0.7227,     -0.1426,     -1.9453,     -0.8555,      1.0078,      0.8945,     -0.5156,     -0.0781,      1.4219,      1.5703,      0.9062,     -1.0703,      2.8125,     -1.2734],
        [    -0.1719,      1.1641,      1.4922,     -1.5469,     -1.7266,     -1.7500,     -0.8516,      2.1406,      0.5352,     -1.2812,      0.8281,      1.3750,      0.3906,      0.4863,      1.0391,     -1.2422],
        [    -2.3281,      0.1484,      0.0806,     -0.9453,     -1.9062,     -1.6172,     -0.1631,      1.0859,      0.8984,      0.9062,      1.1875,      1.2266,      1.6250,     -4.4688,      1.4766,      0.0703],
        [    -0.7031,      1.1328,     -0.4375,     -0.9648,     -0.5234,     -0.8242,     -1.1328,      0.4668,     -0.3945,      0.2227,      0.4453,      0.1855,      0.1650,     -2.0938,     -0.6953,     -0.1001],
        [    -1.2266,      0.3301,      0.2695,     -0.4395,     -0.6484,     -1.1875,     -0.8945,      0.4961,     -0.9336,     -0.0908,      1.2812,      0.4199,      1.7422,     -0.0496,      1.0469,     -1.3828]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.7188,     56.7500,     38.0000,     13.2500,     30.6250,    -58.7500,     12.2500,     -5.6875,     11.7500,    -79.0000,    -53.2500,     45.5000,     37.7500,     20.6250,     12.7500,    -21.6250],
        [   -86.0000,      0.5156,     23.0000,    -30.1250,    -55.5000,    -81.5000,     27.8750,      0.1387,    -36.0000,     -4.8125,    -18.7500,    -12.8750,     50.7500,      2.2656,     51.7500,    -29.3750],
        [    52.5000,     27.2500,     38.0000,    -45.2500,      7.8125,    -94.0000,    -30.8750,     34.7500,    -33.7500,    -26.1250,     89.5000,     46.5000,     62.7500,    -21.5000,     84.0000,     -1.1875],
        [   -35.2500,     48.7500,     35.0000,     17.6250,     -5.3750,    -79.5000,    -19.2500,     86.0000,    -31.3750,    -41.2500,     39.0000,     43.0000,     59.2500,      8.8750,     71.5000,    -25.1250],
        [   -86.5000,     35.5000,     -6.5625,    -44.5000,   -105.0000,    -39.5000,     17.0000,     82.0000,    -59.7500,     10.2500,     85.0000,     19.0000,     14.7500,     11.7500,     93.5000,    -43.5000],
        [  -122.0000,     22.5000,     39.0000,     -8.2500,   -103.0000,    -46.2500,     68.5000,     49.7500,    -21.5000,     -3.7812,    138.0000,     87.0000,     39.2500,    -14.3125,    170.0000,    -57.5000],
        [   -17.7500,     52.2500,     70.5000,    -78.5000,    -80.5000,    -82.5000,    -50.7500,    104.0000,     19.5000,    -54.5000,     70.0000,     66.5000,     14.8750,      5.7188,     55.0000,    -49.5000],
        [  -232.0000,      6.4375,      3.6875,    -46.5000,    -85.5000,    -73.5000,     -9.3750,     51.0000,     31.7500,     37.0000,     98.0000,     57.2500,     59.7500,    -50.7500,     75.0000,      2.6875],
        [   -75.0000,     52.0000,    -21.1250,    -50.5000,    -25.0000,    -40.0000,    -69.0000,     23.2500,    -14.7500,      9.6875,     38.7500,      9.1875,      6.4062,    -25.1250,    -37.7500,     -4.0625],
        [  -123.0000,     14.3750,     12.3125,    -21.5000,    -29.2500,    -54.5000,    -51.5000,     23.2500,    -33.0000,     -3.7188,    105.5000,     19.7500,     64.0000,     -0.5625,     53.5000,    -53.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    55.5000,     -4.0938,     29.6250,      9.6250,     16.7500,      0.9258,     27.0000,     13.2500,      9.3750,      2.9219,    -11.8125,    -12.4375,     -4.2188,      0.2539,    -17.6250,      0.9922],
        [     3.7031,    -12.8125,    -16.6250,      0.8125,     -6.4688,     -7.3750,      9.1875,     -9.1875,     -3.3281,     11.6250,      9.5000,      6.7188,      0.2441,      0.3809,    -31.5000,     11.6875],
        [    23.6250,      5.1875,    -33.7500,     -2.5781,     -3.8438,      7.0312,    -46.2500,     -3.4219,     -1.3047,     -0.5977,     22.7500,    -21.8750,     -2.5312,     -0.6953,     -9.9375,     28.5000],
        [   -46.5000,      5.1875,     43.2500,    -24.3750,     -5.5938,      2.6250,    -33.2500,     -5.5312,     19.0000,     -9.7500,    -36.0000,     19.0000,      6.6875,     -0.4785,      3.7031,     12.8125],
        [   -19.8750,    -36.7500,      2.8906,     14.5625,    -15.1875,     -0.1914,    -31.0000,     -7.9688,     -3.6250,     -0.4199,     47.0000,     21.1250,     -1.6016,     -0.3184,     36.0000,     12.5000],
        [   -21.7500,     29.6250,     -0.8164,      0.6367,     -8.3750,     -3.0625,     10.2500,     -6.8438,      0.0422,     14.5625,     42.5000,     15.1250,     -4.8125,      1.4453,     22.0000,      5.9688],
        [   -71.0000,    -21.5000,      3.0312,      7.8750,     -8.5625,     -5.5000,     30.3750,      5.4688,      7.7500,     10.9375,     50.2500,      1.9062,      6.5625,      0.1035,    -12.8125,      7.6875],
        [    -0.1543,    -30.7500,     12.8125,    -22.1250,    -12.3750,     -1.3438,     33.7500,     11.8750,     15.3125,     29.2500,    -11.4375,     21.3750,     -6.0938,     -0.5625,     -8.8125,      4.9688],
        [    -5.9062,     -6.1875,     -4.0625,    -18.1250,    -24.1250,     -9.4375,    -35.7500,      7.9062,     -3.4062,     -3.5938,    -66.5000,      6.9375,     -1.3438,     -0.8398,     -6.4688,     12.0625],
        [   -80.0000,     11.2500,     45.5000,    -28.2500,      3.4062,     -9.2500,     26.7500,    -14.8750,    -22.3750,     -3.7812,    125.5000,     -3.3906,     10.6250,     -0.1699,    -21.7500,     -5.8438]]
-------------------------
name='positions layer 17'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 17'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    55.5000,     -4.0938,     29.6250,      9.6250,     16.7500,      0.9258,     27.0000,     13.2500,      9.3750,      2.9219,    -11.8125,    -12.4375,     -4.2188,      0.2539,    -17.6250,      0.9922],
        [     3.7031,    -12.8125,    -16.6250,      0.8125,     -6.4688,     -7.3750,      9.1875,     -9.1875,     -3.3281,     11.6250,      9.5000,      6.7188,      0.2441,      0.3809,    -31.5000,     11.6875],
        [    23.6250,      5.1875,    -33.7500,     -2.5781,     -3.8438,      7.0312,    -46.2500,     -3.4219,     -1.3047,     -0.5977,     22.7500,    -21.8750,     -2.5312,     -0.6953,     -9.9375,     28.5000],
        [   -46.5000,      5.1875,     43.2500,    -24.3750,     -5.5938,      2.6250,    -33.2500,     -5.5312,     19.0000,     -9.7500,    -36.0000,     19.0000,      6.6875,     -0.4785,      3.7031,     12.8125],
        [   -19.8750,    -36.7500,      2.8906,     14.5625,    -15.1875,     -0.1914,    -31.0000,     -7.9688,     -3.6250,     -0.4199,     47.0000,     21.1250,     -1.6016,     -0.3184,     36.0000,     12.5000],
        [   -21.7500,     29.6250,     -0.8164,      0.6367,     -8.3750,     -3.0625,     10.2500,     -6.8438,      0.0422,     14.5625,     42.5000,     15.1250,     -4.8125,      1.4453,     22.0000,      5.9688],
        [   -71.0000,    -21.5000,      3.0312,      7.8750,     -8.5625,     -5.5000,     30.3750,      5.4688,      7.7500,     10.9375,     50.2500,      1.9062,      6.5625,      0.1035,    -12.8125,      7.6875],
        [    -0.1543,    -30.7500,     12.8125,    -22.1250,    -12.3750,     -1.3438,     33.7500,     11.8750,     15.3125,     29.2500,    -11.4375,     21.3750,     -6.0938,     -0.5625,     -8.8125,      4.9688],
        [    -5.9062,     -6.1875,     -4.0625,    -18.1250,    -24.1250,     -9.4375,    -35.7500,      7.9062,     -3.4062,     -3.5938,    -66.5000,      6.9375,     -1.3438,     -0.8398,     -6.4688,     12.0625],
        [   -80.0000,     11.2500,     45.5000,    -28.2500,      3.4062,     -9.2500,     26.7500,    -14.8750,    -22.3750,     -3.7812,    125.5000,     -3.3906,     10.6250,     -0.1699,    -21.7500,     -5.8438]]
name='residual layer 17'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.7188,     56.7500,     38.0000,     13.2500,     30.6250,    -58.7500,     12.2500,     -5.6875,     11.7500,    -79.0000,    -53.2500,     45.5000,     37.7500,     20.6250,     12.7500,    -21.6250],
        [   -86.0000,      0.5156,     23.0000,    -30.1250,    -55.5000,    -81.5000,     27.8750,      0.1387,    -36.0000,     -4.8125,    -18.7500,    -12.8750,     50.7500,      2.2656,     51.7500,    -29.3750],
        [    52.5000,     27.2500,     38.0000,    -45.2500,      7.8125,    -94.0000,    -30.8750,     34.7500,    -33.7500,    -26.1250,     89.5000,     46.5000,     62.7500,    -21.5000,     84.0000,     -1.1875],
        [   -35.2500,     48.7500,     35.0000,     17.6250,     -5.3750,    -79.5000,    -19.2500,     86.0000,    -31.3750,    -41.2500,     39.0000,     43.0000,     59.2500,      8.8750,     71.5000,    -25.1250],
        [   -86.5000,     35.5000,     -6.5625,    -44.5000,   -105.0000,    -39.5000,     17.0000,     82.0000,    -59.7500,     10.2500,     85.0000,     19.0000,     14.7500,     11.7500,     93.5000,    -43.5000],
        [  -122.0000,     22.5000,     39.0000,     -8.2500,   -103.0000,    -46.2500,     68.5000,     49.7500,    -21.5000,     -3.7812,    138.0000,     87.0000,     39.2500,    -14.3125,    170.0000,    -57.5000],
        [   -17.7500,     52.2500,     70.5000,    -78.5000,    -80.5000,    -82.5000,    -50.7500,    104.0000,     19.5000,    -54.5000,     70.0000,     66.5000,     14.8750,      5.7188,     55.0000,    -49.5000],
        [  -232.0000,      6.4375,      3.6875,    -46.5000,    -85.5000,    -73.5000,     -9.3750,     51.0000,     31.7500,     37.0000,     98.0000,     57.2500,     59.7500,    -50.7500,     75.0000,      2.6875],
        [   -75.0000,     52.0000,    -21.1250,    -50.5000,    -25.0000,    -40.0000,    -69.0000,     23.2500,    -14.7500,      9.6875,     38.7500,      9.1875,      6.4062,    -25.1250,    -37.7500,     -4.0625],
        [  -123.0000,     14.3750,     12.3125,    -21.5000,    -29.2500,    -54.5000,    -51.5000,     23.2500,    -33.0000,     -3.7188,    105.5000,     19.7500,     64.0000,     -0.5625,     53.5000,    -53.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 0.9219,  1.3047,  1.4453,  0.5195,  1.1484, -1.7500,  0.7852,  0.1709,  0.6055, -1.8828, -1.2031,  0.7461,  1.4141,  1.2500, -0.1055, -0.5117],
        [-1.0078, -0.2275,  0.1016, -0.5000, -1.1250, -2.0156,  0.5547, -0.1523, -0.8477,  0.1260, -0.1279, -0.1035,  1.6016,  0.1177,  0.3281, -0.3281],
        [ 0.9023,  0.5781,  0.0654, -0.7812,  0.0693, -1.8906, -1.1094,  0.5078, -0.7266, -0.4785,  1.4922,  0.3984,  1.8281, -0.9531,  1.1562,  0.4883],
        [-0.9922,  0.9844,  1.2344, -0.1128, -0.1963, -1.7109, -0.7734,  1.3359, -0.2637, -0.9297,  0.0408,  1.0312,  2.0469,  0.3691,  1.1953, -0.2256],
        [-1.3125, -0.0231, -0.0586, -0.5078, -2.1719, -0.8984, -0.2100,  1.2422, -1.3594,  0.1816,  1.8203,  0.6758,  0.4121,  0.5078,  2.0938, -0.5742],
        [-1.8281,  0.9922,  0.6289, -0.1328, -2.0938, -1.1484,  1.2109,  0.7461, -0.4766,  0.2061,  2.5781,  1.7812,  1.1172, -0.5938,  3.2031, -0.9883],
        [-1.1719,  0.6133,  1.2656, -1.2891, -1.7344, -2.1406, -0.3281,  1.9766,  0.6328, -0.8672,  1.7891,  1.2344,  0.7266,  0.2793,  0.7344, -0.8359],
        [-3.2188, -0.5078,  0.2988, -1.3125, -2.0000, -1.9141,  0.4141,  1.1953,  1.1484,  1.3828,  1.3516,  1.5000,  1.9141, -2.5781,  1.2109,  0.1602],
        [-1.0156,  0.8672, -0.4121, -1.1953, -0.9102, -1.1406, -1.6094,  0.5391, -0.4004,  0.1152, -0.3926,  0.2773,  0.1631, -1.1875, -0.7344,  0.1523],
        [-2.7500,  0.5234,  1.0156, -0.9297, -0.5156, -1.5859, -0.4082,  0.1553, -1.3125, -0.1523,  3.5156,  0.3047,  2.5938, -0.0359,  0.5664, -1.2031]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  56.2500,   52.7500,   67.5000,   22.8750,   47.5000,  -57.7500,   39.2500,    7.5625,   21.1250,  -76.0000,  -65.0000,   33.0000,   33.5000,   20.8750,   -4.8750,  -20.6250],
        [ -82.5000,  -12.3125,    6.3750,  -29.2500,  -62.0000,  -89.0000,   37.0000,   -9.0625,  -39.2500,    6.8125,   -9.2500,   -6.1562,   51.0000,    2.6406,   20.2500,  -17.7500],
        [  76.0000,   32.5000,    4.2500,  -47.7500,    3.9688,  -87.0000,  -77.0000,   31.3750,  -35.0000,  -26.7500,  112.0000,   24.6250,   60.2500,  -22.2500,   74.0000,   27.2500],
        [ -82.0000,   54.0000,   78.0000,   -6.7500,  -11.0000,  -77.0000,  -52.5000,   80.5000,  -12.3750,  -51.0000,    3.0000,   62.0000,   66.0000,    8.3750,   75.0000,  -12.3125],
        [-106.5000,   -1.2500,   -3.6719,  -30.0000, -120.0000,  -39.7500,  -14.0000,   74.0000,  -63.5000,    9.8125,  132.0000,   40.0000,   13.1250,   11.4375,  130.0000,  -31.0000],
        [-144.0000,   52.0000,   38.2500,   -7.6250, -111.5000,  -49.2500,   79.0000,   43.0000,  -21.5000,   10.7500,  180.0000,  102.0000,   34.5000,  -12.8750,  192.0000,  -51.5000],
        [ -89.0000,   30.7500,   73.5000,  -70.5000,  -89.0000,  -88.0000,  -20.3750,  109.5000,   27.2500,  -43.5000,  120.0000,   68.5000,   21.5000,    5.8125,   42.2500,  -41.7500],
        [-232.0000,  -24.2500,   16.5000,  -68.5000,  -98.0000,  -75.0000,   24.3750,   63.0000,   47.0000,   66.0000,   86.5000,   78.5000,   53.7500,  -51.2500,   66.0000,    7.6562],
        [ -81.0000,   45.7500,  -25.2500,  -68.5000,  -49.0000,  -49.5000, -105.0000,   31.1250,  -18.1250,    6.0938,  -27.7500,   16.1250,    5.0625,  -26.0000,  -44.2500,    8.0000],
        [-203.0000,   25.6250,   57.7500,  -49.7500,  -25.8750,  -63.7500,  -24.7500,    8.3750,  -55.5000,   -7.5000,  231.0000,   16.3750,   74.5000,   -0.7344,   31.7500,  -58.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -1.0234,      5.6250,     -2.1562,     12.1875,      3.3906,      0.8242,      9.0625,      8.8125,      1.8828,      2.5781,    -14.2500,      0.3652,     -4.1562,      0.3086,     10.9375,     -7.0312],
        [   -37.5000,     -1.9922,     13.2500,      3.0469,     12.9375,      2.6719,    -35.2500,     -2.2656,      6.1250,     -1.3594,     10.5000,    -11.6875,     -0.1592,     -0.2891,     -4.4375,     10.0000],
        [    -6.1875,     -5.8438,      2.5625,    -17.3750,     -4.4062,      2.3750,     -9.5625,     -4.2812,     -7.0625,    -12.0000,     -3.4062,     -4.7500,      5.4062,     -0.2695,    -14.1250,      4.1250],
        [   -16.3750,     11.9375,    -12.2500,    -20.6250,     -7.2812,     -1.6875,     -2.7344,      3.2031,    -14.6250,     -6.9062,     15.8125,    -10.5000,      3.4688,     -0.0752,     -0.7578,     -0.5703],
        [    11.3750,    -14.1875,     -1.7422,     -3.2188,     10.7500,      6.5625,    -17.2500,      0.6250,     -3.8281,     -6.9375,     26.5000,     13.8750,     -1.1016,     -0.0879,     16.6250,      3.1250],
        [    -3.2656,     -2.2656,     25.3750,     -9.6875,     -2.5625,     -2.4844,    -13.2500,      4.4062,     -6.4062,     -6.2188,     42.7500,    -15.5000,      2.0781,      0.2109,      0.4785,      6.0312],
        [   -37.0000,    -12.0625,      5.1250,     10.8125,      7.6250,     -7.8750,      2.1562,     22.1250,     -9.0625,     -5.9375,      9.3750,    -15.1250,      2.1406,     -0.0175,     36.0000,      3.5469],
        [   -23.0000,     -0.5000,     26.8750,    -27.3750,    -11.1875,     -9.8750,      8.7500,     34.2500,    -12.1875,     -1.2188,     84.5000,    -40.0000,      1.5938,      0.0845,     31.2500,     -5.1562],
        [   -42.7500,     -8.3125,      5.6875,    -14.3125,     -6.2188,     -2.5625,      9.3125,     10.0000,     -4.5312,      3.9844,     37.2500,      2.5781,      1.1797,     -0.4277,      5.8438,    -10.0625],
        [   -27.8750,      4.2500,     -2.4062,      4.0938,      4.3438,      0.3438,    -43.0000,      1.5547,     -2.5781,     22.8750,     40.0000,    -14.2500,     -3.0469,      0.7461,    -16.3750,      2.5781]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     0.4785,      1.2188,      1.2734,      0.6367,      1.0938,     -1.3984,      0.7539,      0.3457,      0.6133,     -1.7031,     -0.7734,      0.6055,      0.9648,      2.2031,      0.1045,     -0.6992],
        [    -0.8633,     -0.2471,      0.3184,     -0.3945,     -0.8750,     -1.7656,      0.0227,     -0.1992,     -0.7344,      0.1055,      0.0101,     -0.2676,      1.3828,      0.2041,      0.2266,     -0.1631],
        [     0.5117,      0.4688,      0.1123,     -1.0000,     -0.0079,     -1.7656,     -1.1406,      0.4824,     -0.9492,     -0.7617,      0.8984,      0.3027,      1.8125,     -1.9922,      0.8750,      0.6680],
        [    -0.7422,      1.1953,      1.1172,     -0.4316,     -0.3418,     -1.6875,     -0.7539,      1.5391,     -0.6250,     -1.1641,      0.1602,      0.8086,      1.9844,      0.7539,      1.1172,     -0.2832],
        [    -0.6836,     -0.2676,     -0.0879,     -0.5039,     -1.9531,     -0.6836,     -0.4043,      1.3125,     -1.4922,      0.0554,      1.2891,      0.8086,      0.3281,      0.9844,      2.1094,     -0.5859],
        [    -1.1875,      0.9648,      1.1562,     -0.2930,     -2.2812,     -1.1875,      0.9609,      0.9336,     -0.6953,      0.0981,      2.0312,      1.4609,      1.1250,     -1.2344,      3.1094,     -1.0781],
        [    -0.9453,      0.3379,      1.3359,     -0.9414,     -1.5156,     -2.0469,     -0.2471,      2.4219,      0.4219,     -0.9922,      1.1016,      0.8359,      0.6719,      0.5234,      1.1797,     -0.8398],
        [    -1.9531,     -0.4570,      0.7500,     -1.5469,     -2.0781,     -1.8516,      0.4590,      1.8203,      0.8203,      1.3359,      1.4844,      0.6172,      1.6094,     -4.7500,      1.4922,      0.0562],
        [    -0.9414,      0.6875,     -0.3359,     -1.3203,     -1.0391,     -1.1250,     -1.3125,      0.7656,     -0.5312,      0.2051,      0.0815,      0.2969,      0.1797,     -2.4375,     -0.5820,     -0.0459],
        [    -1.6016,      0.4980,      0.8672,     -0.6680,     -0.3711,     -1.2578,     -0.8477,      0.1680,     -1.2422,      0.2871,      2.1250,      0.0309,      1.8828,      0.0010,      0.2139,     -1.1406]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    55.2500,     58.5000,     65.5000,     35.0000,     51.0000,    -57.0000,     48.2500,     16.3750,     23.0000,    -73.5000,    -79.0000,     33.2500,     29.3750,     21.1250,      6.0625,    -27.6250],
        [  -120.0000,    -14.3125,     19.6250,    -26.2500,    -49.0000,    -86.5000,      1.7500,    -11.3125,    -33.0000,      5.4375,      1.2500,    -17.8750,     50.7500,      2.3438,     15.8125,     -7.7500],
        [    70.0000,     26.6250,      6.8125,    -65.0000,     -0.4375,    -84.5000,    -86.5000,     27.1250,    -42.0000,    -38.7500,    108.5000,     19.8750,     65.5000,    -22.5000,     60.0000,     31.3750],
        [   -98.5000,     66.0000,     66.0000,    -27.3750,    -18.2500,    -78.5000,    -55.2500,     83.5000,    -27.0000,    -58.0000,     18.7500,     51.5000,     69.5000,      8.3125,     74.0000,    -12.8750],
        [   -95.0000,    -15.4375,     -5.4062,    -33.2500,   -109.0000,    -33.2500,    -31.2500,     74.5000,    -67.5000,      2.8750,    158.0000,     54.0000,     12.0000,     11.3750,    147.0000,    -27.8750],
        [  -147.0000,     49.7500,     63.5000,    -17.2500,   -114.0000,    -51.7500,     66.0000,     47.5000,    -27.8750,      4.5312,    223.0000,     86.5000,     36.5000,    -12.6875,    192.0000,    -45.5000],
        [  -126.0000,     18.7500,     78.5000,    -59.7500,    -81.5000,    -96.0000,    -18.2500,    132.0000,     18.2500,    -49.5000,    129.0000,     53.5000,     23.6250,      5.7812,     78.0000,    -38.2500],
        [  -255.0000,    -24.7500,     43.5000,    -96.0000,   -109.0000,    -85.0000,     33.0000,     97.0000,     34.7500,     65.0000,    171.0000,     38.5000,     55.2500,    -51.2500,     97.0000,      2.5000],
        [  -124.0000,     37.5000,    -19.5000,    -83.0000,    -55.2500,    -52.0000,    -95.5000,     41.0000,    -22.6250,     10.0625,      9.5000,     18.7500,      6.2500,    -26.3750,    -38.5000,     -2.0625],
        [  -231.0000,     29.8750,     55.2500,    -45.7500,    -21.5000,    -63.5000,    -68.0000,      9.9375,    -58.0000,     15.3750,    272.0000,      2.1250,     71.5000,      0.0117,     15.3750,    -56.2500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   -59.7500,     22.3750,      4.6562,     62.0000,     -4.0938,     -0.4609,    -42.2500,     25.5000,      3.4531,    -28.5000,     21.7500,      6.3125,    -10.1250,      1.5781,    -17.7500,     13.0000],
        [   -99.0000,    -29.5000,     -8.5000,    -21.8750,      3.1094,    -24.2500,   -107.5000,    -27.0000,     19.0000,    -23.2500,    103.0000,     38.7500,      1.8750,     -0.0447,     42.7500,      1.0703],
        [   -46.0000,    -30.0000,      4.5938,      4.2500,     21.8750,    -16.2500,   -101.5000,     19.1250,      7.6562,     -4.7188,    121.0000,      7.6250,     10.5000,     -0.6562,    -39.0000,      9.8125],
        [   -83.0000,     16.6250,     73.5000,     43.7500,     -3.8438,     -1.2031,     -8.0625,    -52.5000,     29.3750,     36.5000,     50.7500,      3.6875,     13.2500,      0.8164,     60.0000,      3.9844],
        [   -62.5000,      7.0312,     -2.2812,     32.2500,    -10.3750,     -2.7344,     34.2500,     48.2500,    -11.9375,      9.7500,    186.0000,    -24.0000,      3.6094,      0.8203,      1.7891,     17.7500],
        [    -7.5312,    -22.3750,     27.2500,      7.2188,    -10.2500,     -4.1250,     11.6875,     56.7500,     30.5000,     -5.5312,    139.0000,    -42.5000,      5.1250,      1.7656,     47.0000,     32.2500],
        [   -86.0000,     -0.0374,     12.1875,      3.8281,    -43.7500,      2.1562,     41.7500,      4.3125,     -6.1250,     -5.1875,     72.0000,    -18.7500,      9.8750,     -2.0938,     35.5000,      7.5000],
        [    60.5000,    -35.2500,     -5.5000,     29.2500,    -13.6250,    -29.5000,     -7.0938,     42.0000,     28.0000,     12.8750,     88.5000,     -8.2500,     11.5625,     -3.7500,     42.2500,     -9.1250],
        [   -48.2500,     -6.7812,    -11.1875,     57.0000,    -16.7500,     -0.6836,    -65.5000,     16.2500,     18.6250,      1.7969,     72.5000,    -37.7500,     -0.4941,      0.7500,     21.6250,      0.6367],
        [   -92.0000,      3.4375,     13.9375,    -39.7500,    -12.3750,     -4.3750,    -62.7500,    -15.1875,    -15.3125,     -3.9531,     82.5000,    -26.8750,     -4.8438,     -1.9062,     -0.2500,    -25.3750]]
-------------------------
name='positions layer 18'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 18'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   -59.7500,     22.3750,      4.6562,     62.0000,     -4.0938,     -0.4609,    -42.2500,     25.5000,      3.4531,    -28.5000,     21.7500,      6.3125,    -10.1250,      1.5781,    -17.7500,     13.0000],
        [   -99.0000,    -29.5000,     -8.5000,    -21.8750,      3.1094,    -24.2500,   -107.5000,    -27.0000,     19.0000,    -23.2500,    103.0000,     38.7500,      1.8750,     -0.0447,     42.7500,      1.0703],
        [   -46.0000,    -30.0000,      4.5938,      4.2500,     21.8750,    -16.2500,   -101.5000,     19.1250,      7.6562,     -4.7188,    121.0000,      7.6250,     10.5000,     -0.6562,    -39.0000,      9.8125],
        [   -83.0000,     16.6250,     73.5000,     43.7500,     -3.8438,     -1.2031,     -8.0625,    -52.5000,     29.3750,     36.5000,     50.7500,      3.6875,     13.2500,      0.8164,     60.0000,      3.9844],
        [   -62.5000,      7.0312,     -2.2812,     32.2500,    -10.3750,     -2.7344,     34.2500,     48.2500,    -11.9375,      9.7500,    186.0000,    -24.0000,      3.6094,      0.8203,      1.7891,     17.7500],
        [    -7.5312,    -22.3750,     27.2500,      7.2188,    -10.2500,     -4.1250,     11.6875,     56.7500,     30.5000,     -5.5312,    139.0000,    -42.5000,      5.1250,      1.7656,     47.0000,     32.2500],
        [   -86.0000,     -0.0374,     12.1875,      3.8281,    -43.7500,      2.1562,     41.7500,      4.3125,     -6.1250,     -5.1875,     72.0000,    -18.7500,      9.8750,     -2.0938,     35.5000,      7.5000],
        [    60.5000,    -35.2500,     -5.5000,     29.2500,    -13.6250,    -29.5000,     -7.0938,     42.0000,     28.0000,     12.8750,     88.5000,     -8.2500,     11.5625,     -3.7500,     42.2500,     -9.1250],
        [   -48.2500,     -6.7812,    -11.1875,     57.0000,    -16.7500,     -0.6836,    -65.5000,     16.2500,     18.6250,      1.7969,     72.5000,    -37.7500,     -0.4941,      0.7500,     21.6250,      0.6367],
        [   -92.0000,      3.4375,     13.9375,    -39.7500,    -12.3750,     -4.3750,    -62.7500,    -15.1875,    -15.3125,     -3.9531,     82.5000,    -26.8750,     -4.8438,     -1.9062,     -0.2500,    -25.3750]]
name='residual layer 18'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    55.2500,     58.5000,     65.5000,     35.0000,     51.0000,    -57.0000,     48.2500,     16.3750,     23.0000,    -73.5000,    -79.0000,     33.2500,     29.3750,     21.1250,      6.0625,    -27.6250],
        [  -120.0000,    -14.3125,     19.6250,    -26.2500,    -49.0000,    -86.5000,      1.7500,    -11.3125,    -33.0000,      5.4375,      1.2500,    -17.8750,     50.7500,      2.3438,     15.8125,     -7.7500],
        [    70.0000,     26.6250,      6.8125,    -65.0000,     -0.4375,    -84.5000,    -86.5000,     27.1250,    -42.0000,    -38.7500,    108.5000,     19.8750,     65.5000,    -22.5000,     60.0000,     31.3750],
        [   -98.5000,     66.0000,     66.0000,    -27.3750,    -18.2500,    -78.5000,    -55.2500,     83.5000,    -27.0000,    -58.0000,     18.7500,     51.5000,     69.5000,      8.3125,     74.0000,    -12.8750],
        [   -95.0000,    -15.4375,     -5.4062,    -33.2500,   -109.0000,    -33.2500,    -31.2500,     74.5000,    -67.5000,      2.8750,    158.0000,     54.0000,     12.0000,     11.3750,    147.0000,    -27.8750],
        [  -147.0000,     49.7500,     63.5000,    -17.2500,   -114.0000,    -51.7500,     66.0000,     47.5000,    -27.8750,      4.5312,    223.0000,     86.5000,     36.5000,    -12.6875,    192.0000,    -45.5000],
        [  -126.0000,     18.7500,     78.5000,    -59.7500,    -81.5000,    -96.0000,    -18.2500,    132.0000,     18.2500,    -49.5000,    129.0000,     53.5000,     23.6250,      5.7812,     78.0000,    -38.2500],
        [  -255.0000,    -24.7500,     43.5000,    -96.0000,   -109.0000,    -85.0000,     33.0000,     97.0000,     34.7500,     65.0000,    171.0000,     38.5000,     55.2500,    -51.2500,     97.0000,      2.5000],
        [  -124.0000,     37.5000,    -19.5000,    -83.0000,    -55.2500,    -52.0000,    -95.5000,     41.0000,    -22.6250,     10.0625,      9.5000,     18.7500,      6.2500,    -26.3750,    -38.5000,     -2.0625],
        [  -231.0000,     29.8750,     55.2500,    -45.7500,    -21.5000,    -63.5000,    -68.0000,      9.9375,    -58.0000,     15.3750,    272.0000,      2.1250,     71.5000,      0.0117,     15.3750,    -56.2500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0294,  1.0625,  0.7070,  0.9922,  0.6406, -1.0391,  0.0591,  0.4766,  0.4082, -1.3984, -0.5117,  0.4668,  0.3750,  0.7461, -0.1216, -0.1924],
        [-1.2578, -0.5039,  0.0981, -0.4336, -0.5508, -1.7656, -0.9180, -0.3828, -0.1904, -0.2148,  0.8164,  0.2168,  0.8984,  0.0659,  0.5312, -0.0771],
        [ 0.1338, -0.0378,  0.0977, -0.5312,  0.2500, -1.5547, -1.5859,  0.4492, -0.4512, -0.5078,  1.7500,  0.2754,  1.2578, -0.6445,  0.1855,  0.4629],
        [-1.0156,  0.9336,  1.2031,  0.1436, -0.2578, -1.2344, -0.5352,  0.3027,  0.0315, -0.2520,  0.5312,  0.5547,  1.3750,  0.2559,  1.1953, -0.1001],
        [-0.8594, -0.0923, -0.0645, -0.0085, -1.3594, -0.5430,  0.0247,  1.1641, -1.0234,  0.1445,  2.5781,  0.2949,  0.2520,  0.3340,  1.2891, -0.1108],
        [-0.8633,  0.3086,  0.7734, -0.0884, -1.4531, -0.8633,  0.6562,  1.0156,  0.0347, -0.0117,  2.7656,  0.4414,  0.6875, -0.3047,  2.1250, -0.1484],
        [-1.1953,  0.2119,  0.7852, -0.4941, -1.4766, -1.4688,  0.1992,  1.3438,  0.1611, -0.6445,  1.5547,  0.3535,  0.5625,  0.1045,  1.0156, -0.3496],
        [-1.1328, -0.7031,  0.3398, -0.6094, -1.4922, -1.8516,  0.2266,  1.4141,  0.8633,  0.9531,  2.0625,  0.3184,  1.1562, -1.6016,  1.2891, -0.0776],
        [-1.0938,  0.3906, -0.2988, -0.2598, -0.9531, -0.9258, -1.5469,  0.6328, -0.0601,  0.1582,  0.7109, -0.2168,  0.1084, -0.8164, -0.1699, -0.0182],
        [-2.0938,  0.4336,  0.6836, -0.8672, -0.4570, -1.2188, -1.2734, -0.0593, -1.1172,  0.1553,  3.1406, -0.2871,  1.2734, -0.0615,  0.1553, -1.0625]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -4.5000,   81.0000,   70.0000,   97.0000,   47.0000,  -57.5000,    6.0000,   42.0000,   26.5000, -102.0000,  -57.2500,   39.5000,   19.2500,   22.7500,  -11.6875,  -14.6250],
        [-219.0000,  -43.7500,   11.1250,  -48.0000,  -46.0000, -111.0000, -106.0000,  -38.2500,  -14.0000,  -17.7500,  104.0000,   20.8750,   52.5000,    2.2969,   58.5000,   -6.6875],
        [  24.0000,   -3.3750,   11.3750,  -60.7500,   21.5000, -101.0000, -188.0000,   46.2500,  -34.2500,  -43.5000,  230.0000,   27.5000,   76.0000,  -23.1250,   21.0000,   41.2500],
        [-182.0000,   82.5000,  140.0000,   16.3750,  -22.1250,  -79.5000,  -63.2500,   31.0000,    2.3750,  -21.5000,   69.5000,   55.2500,   83.0000,    9.1250,  134.0000,   -8.8750],
        [-158.0000,   -8.3750,   -7.6875,   -1.0000, -119.5000,  -36.0000,    3.0000,  123.0000,  -79.5000,   12.6250,  344.0000,   30.0000,   15.6250,   12.1875,  149.0000,  -10.1250],
        [-155.0000,   27.3750,   91.0000,  -10.0000, -124.0000,  -56.0000,   77.5000,  104.0000,    2.6250,   -1.0000,  362.0000,   44.0000,   41.5000,  -10.9375,  239.0000,  -13.2500],
        [-212.0000,   18.7500,   90.5000,  -56.0000, -125.0000,  -94.0000,   23.5000,  136.0000,   12.1250,  -54.7500,  201.0000,   34.7500,   33.5000,    3.6875,  113.5000,  -30.7500],
        [-194.0000,  -60.0000,   38.0000,  -67.0000, -122.5000, -114.5000,   25.8750,  139.0000,   62.7500,   78.0000,  260.0000,   30.2500,   67.0000,  -55.0000,  139.0000,   -6.6250],
        [-172.0000,   30.7500,  -30.7500,  -26.0000,  -72.0000,  -52.7500, -161.0000,   57.2500,   -4.0000,   11.8750,   82.0000,  -19.0000,    5.7500,  -25.6250,  -16.8750,   -1.4219],
        [-324.0000,   33.2500,   69.0000,  -85.5000,  -34.0000,  -68.0000, -131.0000,   -5.2500,  -73.5000,   11.4375,  354.0000,  -24.7500,   66.5000,   -1.8906,   15.1250,  -81.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[     3.3438,      3.7188,      7.5625,      6.5625,     11.6875,     -3.6094,     25.2500,     11.3125,     -0.7031,      2.7188,     -8.3750,     -7.6250,    -12.9375,      0.2334,     19.2500,      3.7969],
        [    -2.1562,      9.4375,      4.4375,     10.6875,      8.7500,     -3.6250,     14.0625,     -0.9141,      2.9688,     -0.8516,      9.1250,      0.4043,    -14.2500,     -0.2969,     10.1875,      0.7305],
        [   -33.0000,      0.2910,    -12.3125,     13.0625,     12.1875,     -7.1562,      7.6250,    -19.1250,     15.3750,     -0.2539,      5.8750,    -14.5625,    -10.7500,      0.8516,      4.3438,      6.3750],
        [   -57.2500,      7.7500,      1.7500,      0.9727,     16.7500,    -10.0625,      7.0000,     -6.3125,      9.7500,      5.9375,     32.0000,      3.9688,     -6.0312,      0.1099,      9.9375,     -0.2715],
        [   -29.3750,    -13.5000,      9.0000,     -2.1250,     30.6250,     -1.1250,     26.2500,    -31.5000,      2.0781,    -13.8750,     41.2500,    -12.5000,     -4.0312,      0.0757,     14.8125,      5.4062],
        [    15.0000,     -7.7812,      3.8750,    -22.0000,      4.8125,      1.0547,      9.7500,    -25.8750,     -1.3203,     13.3750,     31.6250,     -5.4375,     -5.2812,      0.3281,     17.5000,     -7.7188],
        [    -5.9688,     11.9375,    -11.4375,    -22.1250,    -21.5000,    -11.6250,     20.6250,    -15.5000,      5.5625,     24.7500,    -16.7500,     -1.3281,     -7.6875,      0.0449,     34.2500,    -14.4375],
        [    10.6875,     -4.6250,    -12.5625,    -35.0000,    -18.0000,      8.0625,     14.8750,    -21.5000,      4.5312,     16.0000,     36.2500,     18.2500,    -15.8125,      0.2773,     12.3125,    -12.5000],
        [   -49.0000,      0.9805,      8.5625,     -8.3125,     -0.9062,     -3.0156,     15.0000,    -45.7500,     -0.5742,     -7.3750,     33.7500,      2.3906,      1.8828,      0.5938,     28.7500,    -17.0000],
        [   -19.5000,      7.1562,     14.9375,     -0.9297,     10.8125,     -3.1875,     -4.1875,    -20.6250,      4.0625,     -4.0312,     15.2500,      3.0000,     -8.2500,     -0.1318,     30.0000,    -15.2500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.0070,  1.3594,  1.1719,  1.4531,  1.0703, -1.3047,  0.3438,  0.8633,  0.5547, -1.8047, -0.4531,  0.4492,  0.1484,  2.0000,  0.0977, -0.2168],
        [-0.9570, -0.3965,  0.1699, -0.3770, -0.4902, -1.7578, -0.7266, -0.4551, -0.1709, -0.2432,  0.5586,  0.2158,  0.6445,  0.1260,  0.6406, -0.0859],
        [-0.0432, -0.0396, -0.0114, -0.5352,  0.4902, -1.8359, -1.5859,  0.3516, -0.3242, -0.6367,  1.2969,  0.1455,  1.2266, -1.5547,  0.2617,  0.7578],
        [-1.0078,  1.0156,  1.5078,  0.1709, -0.0688, -1.3359, -0.4336,  0.2812,  0.1826, -0.1982,  0.4883,  0.5859,  1.2656,  0.5664,  1.3047, -0.1279],
        [-0.8945, -0.2793,  0.0159, -0.0349, -1.2969, -0.6289,  0.2559,  1.1875, -1.3281, -0.0181,  2.1094,  0.1973,  0.2158,  0.8516,  1.6953, -0.0747],
        [-0.7305,  0.2734,  1.2422, -0.3887, -1.8828, -1.0078,  0.8281,  1.0938,  0.0243,  0.1943,  2.3438,  0.4707,  0.7383, -0.8008,  2.8750, -0.3613],
        [-1.1094,  0.4180,  1.0156, -0.9297, -2.2656, -1.8984,  0.4102,  1.6562,  0.3223, -0.4609,  1.0703,  0.4004,  0.5117,  0.2754,  1.6172, -0.7656],
        [-0.9375, -0.8828,  0.3281, -1.2188, -2.1875, -1.9219,  0.3828,  1.6250,  1.2344,  1.4609,  1.7422,  0.5820,  1.0234, -4.0625,  1.6719, -0.3262],
        [-1.2500,  0.4785, -0.3164, -0.4531, -1.2500, -1.1172, -1.5078,  0.1748, -0.0928,  0.0767,  0.7500, -0.2207,  0.1680, -2.0469,  0.1445, -0.3457],
        [-1.6797,  0.5312,  1.0312, -0.9883, -0.3438, -1.2266, -1.2109, -0.3418, -1.2188,  0.1099,  2.0625, -0.2500,  1.1094, -0.1436,  0.4746, -1.5703]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -1.1562,   84.5000,   77.5000,  103.5000,   58.7500,  -61.0000,   31.2500,   53.2500,   25.7500,  -99.5000,  -65.5000,   31.8750,    6.3125,   23.0000,    7.5625,  -10.8125],
        [-221.0000,  -34.2500,   15.5625,  -37.2500,  -37.2500, -114.5000,  -92.0000,  -39.2500,  -11.0000,  -18.6250,  113.0000,   21.2500,   38.2500,    2.0000,   68.5000,   -5.9688],
        [  -9.0000,   -3.0781,   -0.9375,  -47.7500,   33.7500, -108.0000, -180.0000,   27.1250,  -18.8750,  -43.7500,  236.0000,   12.9375,   65.0000,  -22.2500,   25.3750,   47.5000],
        [-239.0000,   90.0000,  142.0000,   17.3750,   -5.3750,  -89.5000,  -56.2500,   24.7500,   12.1250,  -15.5625,  101.5000,   59.2500,   77.0000,    9.2500,  144.0000,   -9.1250],
        [-187.0000,  -21.8750,    1.3125,   -3.1250,  -89.0000,  -37.0000,   29.2500,   91.5000,  -77.5000,   -1.2500,  386.0000,   17.5000,   11.6250,   12.2500,  164.0000,   -4.7188],
        [-140.0000,   19.6250,   95.0000,  -32.0000, -119.0000,  -55.0000,   87.0000,   78.0000,    1.3047,   12.3750,  394.0000,   38.5000,   36.2500,  -10.6250,  256.0000,  -21.0000],
        [-218.0000,   30.7500,   79.0000,  -78.0000, -146.0000, -105.5000,   44.0000,  120.5000,   17.7500,  -30.0000,  184.0000,   33.5000,   25.7500,    3.7344,  148.0000,  -45.2500],
        [-183.0000,  -64.5000,   25.5000, -102.0000, -140.0000, -106.5000,   40.7500,  117.5000,   67.5000,   94.0000,  296.0000,   48.5000,   51.2500,  -54.7500,  151.0000,  -19.1250],
        [-221.0000,   31.7500,  -22.2500,  -34.2500,  -73.0000,  -55.7500, -146.0000,   11.5000,   -4.5625,    4.5000,  116.0000,  -16.6250,    7.6250,  -25.0000,   11.8750,  -18.3750],
        [-344.0000,   40.5000,   84.0000,  -86.5000,  -23.2500,  -71.0000, -135.0000,  -25.8750,  -69.5000,    7.4062,  370.0000,  -21.7500,   58.2500,   -2.0156,   45.0000,  -97.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -169.0000,      5.2500,    -13.5000,     31.1250,     35.2500,     -8.3750,    -54.5000,    -16.0000,      6.7500,     -3.8281,    -36.0000,     -2.1250,    -11.7500,      0.9961,     82.5000,     39.7500],
        [   -63.2500,      2.2812,    -47.7500,     15.0625,    -13.8750,      5.6875,      2.8906,      5.5938,     13.5000,     -6.6562,     74.5000,     22.5000,     -6.0938,      0.7656,     22.0000,     19.6250],
        [   -75.0000,     13.1250,      4.1562,    -10.7500,     22.8750,     -5.5625,      3.2344,      5.1562,     -7.7500,    -12.6875,     78.5000,     52.5000,    -14.2500,      0.7227,    -17.0000,     19.5000],
        [   -74.5000,    -10.0625,     65.0000,    -23.0000,    -34.2500,      2.4219,    -36.0000,     53.0000,     21.7500,      2.1250,     70.5000,     10.8125,     -5.0000,      0.5117,     -8.4375,      0.6523],
        [    34.7500,    -18.6250,     10.5000,    -22.0000,      1.6250,     -8.8750,     18.2500,      6.7188,    -13.8125,      6.4375,     53.2500,     -1.8438,     -7.3125,      0.2041,    -27.2500,     -9.8125],
        [   -41.2500,     14.3750,      8.8750,     -1.3359,     -7.2500,     -8.3125,      5.8125,     16.3750,    -16.0000,    -15.6875,     73.5000,    -19.1250,     -2.7188,      0.3398,     11.1250,     24.0000],
        [    49.0000,     14.6250,     34.5000,     17.0000,    -32.7500,    -12.6250,     12.3750,    -31.5000,     27.6250,     39.7500,    -93.0000,     51.5000,     23.1250,      2.5469,     50.7500,     39.5000],
        [   240.0000,     34.7500,     -1.0625,     59.5000,    -14.3125,     15.6875,     73.0000,     -7.4688,     16.3750,     77.5000,    -66.0000,    -43.0000,     11.4375,      1.5781,    -10.6875,     -0.9258],
        [   -88.5000,    -23.1250,    -92.5000,    -27.7500,     22.3750,     -2.9062,    -22.3750,     44.2500,     -5.3125,    -24.6250,     17.5000,      8.5000,     -7.5000,     -0.2275,     17.3750,    -12.4375],
        [   -95.0000,     38.0000,     60.5000,    -74.5000,    -37.7500,     -9.8125,    -27.5000,    -32.5000,     25.6250,      4.6250,     10.5000,     16.1250,     12.1250,     -0.0193,      0.9258,    -13.2500]]
-------------------------
name='positions layer 19'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 19'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -169.0000,      5.2500,    -13.5000,     31.1250,     35.2500,     -8.3750,    -54.5000,    -16.0000,      6.7500,     -3.8281,    -36.0000,     -2.1250,    -11.7500,      0.9961,     82.5000,     39.7500],
        [   -63.2500,      2.2812,    -47.7500,     15.0625,    -13.8750,      5.6875,      2.8906,      5.5938,     13.5000,     -6.6562,     74.5000,     22.5000,     -6.0938,      0.7656,     22.0000,     19.6250],
        [   -75.0000,     13.1250,      4.1562,    -10.7500,     22.8750,     -5.5625,      3.2344,      5.1562,     -7.7500,    -12.6875,     78.5000,     52.5000,    -14.2500,      0.7227,    -17.0000,     19.5000],
        [   -74.5000,    -10.0625,     65.0000,    -23.0000,    -34.2500,      2.4219,    -36.0000,     53.0000,     21.7500,      2.1250,     70.5000,     10.8125,     -5.0000,      0.5117,     -8.4375,      0.6523],
        [    34.7500,    -18.6250,     10.5000,    -22.0000,      1.6250,     -8.8750,     18.2500,      6.7188,    -13.8125,      6.4375,     53.2500,     -1.8438,     -7.3125,      0.2041,    -27.2500,     -9.8125],
        [   -41.2500,     14.3750,      8.8750,     -1.3359,     -7.2500,     -8.3125,      5.8125,     16.3750,    -16.0000,    -15.6875,     73.5000,    -19.1250,     -2.7188,      0.3398,     11.1250,     24.0000],
        [    49.0000,     14.6250,     34.5000,     17.0000,    -32.7500,    -12.6250,     12.3750,    -31.5000,     27.6250,     39.7500,    -93.0000,     51.5000,     23.1250,      2.5469,     50.7500,     39.5000],
        [   240.0000,     34.7500,     -1.0625,     59.5000,    -14.3125,     15.6875,     73.0000,     -7.4688,     16.3750,     77.5000,    -66.0000,    -43.0000,     11.4375,      1.5781,    -10.6875,     -0.9258],
        [   -88.5000,    -23.1250,    -92.5000,    -27.7500,     22.3750,     -2.9062,    -22.3750,     44.2500,     -5.3125,    -24.6250,     17.5000,      8.5000,     -7.5000,     -0.2275,     17.3750,    -12.4375],
        [   -95.0000,     38.0000,     60.5000,    -74.5000,    -37.7500,     -9.8125,    -27.5000,    -32.5000,     25.6250,      4.6250,     10.5000,     16.1250,     12.1250,     -0.0193,      0.9258,    -13.2500]]
name='residual layer 19'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -1.1562,   84.5000,   77.5000,  103.5000,   58.7500,  -61.0000,   31.2500,   53.2500,   25.7500,  -99.5000,  -65.5000,   31.8750,    6.3125,   23.0000,    7.5625,  -10.8125],
        [-221.0000,  -34.2500,   15.5625,  -37.2500,  -37.2500, -114.5000,  -92.0000,  -39.2500,  -11.0000,  -18.6250,  113.0000,   21.2500,   38.2500,    2.0000,   68.5000,   -5.9688],
        [  -9.0000,   -3.0781,   -0.9375,  -47.7500,   33.7500, -108.0000, -180.0000,   27.1250,  -18.8750,  -43.7500,  236.0000,   12.9375,   65.0000,  -22.2500,   25.3750,   47.5000],
        [-239.0000,   90.0000,  142.0000,   17.3750,   -5.3750,  -89.5000,  -56.2500,   24.7500,   12.1250,  -15.5625,  101.5000,   59.2500,   77.0000,    9.2500,  144.0000,   -9.1250],
        [-187.0000,  -21.8750,    1.3125,   -3.1250,  -89.0000,  -37.0000,   29.2500,   91.5000,  -77.5000,   -1.2500,  386.0000,   17.5000,   11.6250,   12.2500,  164.0000,   -4.7188],
        [-140.0000,   19.6250,   95.0000,  -32.0000, -119.0000,  -55.0000,   87.0000,   78.0000,    1.3047,   12.3750,  394.0000,   38.5000,   36.2500,  -10.6250,  256.0000,  -21.0000],
        [-218.0000,   30.7500,   79.0000,  -78.0000, -146.0000, -105.5000,   44.0000,  120.5000,   17.7500,  -30.0000,  184.0000,   33.5000,   25.7500,    3.7344,  148.0000,  -45.2500],
        [-183.0000,  -64.5000,   25.5000, -102.0000, -140.0000, -106.5000,   40.7500,  117.5000,   67.5000,   94.0000,  296.0000,   48.5000,   51.2500,  -54.7500,  151.0000,  -19.1250],
        [-221.0000,   31.7500,  -22.2500,  -34.2500,  -73.0000,  -55.7500, -146.0000,   11.5000,   -4.5625,    4.5000,  116.0000,  -16.6250,    7.6250,  -25.0000,   11.8750,  -18.3750],
        [-344.0000,   40.5000,   84.0000,  -86.5000,  -23.2500,  -71.0000, -135.0000,  -25.8750,  -69.5000,    7.4062,  370.0000,  -21.7500,   58.2500,   -2.0156,   45.0000,  -97.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -1.2734,      0.9609,      0.6133,      1.3125,      1.1406,     -1.1250,     -0.2061,      0.3848,      0.4336,     -1.1953,     -0.7617,      0.2773,     -0.1133,      0.9297,      0.8320,      0.3281],
        [    -1.7266,     -0.2754,     -0.2500,     -0.1748,     -0.5039,     -1.4297,     -0.6367,     -0.2812,      0.0269,     -0.2363,      1.1406,      0.3320,      0.5430,      0.0869,      0.6758,      0.1260],
        [    -0.5312,      0.0908,      0.0261,     -0.4824,      0.5820,     -1.5547,     -1.3203,      0.2812,     -0.2988,     -0.5508,      1.9922,      0.5156,      0.8906,     -0.7070,      0.0649,      0.6445],
        [    -1.8125,      0.6562,      1.5312,     -0.0422,     -0.3711,     -1.0859,     -0.6250,      0.6172,      0.3457,     -0.1191,      0.9922,      0.5039,      1.1484,      0.2910,      0.9609,     -0.0742],
        [    -1.0312,     -0.3887,      0.1021,     -0.2207,     -0.9531,     -0.6680,      0.3789,      0.9102,     -1.0938,      0.0540,      2.9688,      0.1328,      0.0811,      0.4336,      1.1406,     -0.1484],
        [    -1.3125,      0.3496,      0.9648,     -0.3125,     -1.4844,     -0.9922,      0.7891,      0.9375,     -0.1875,     -0.0369,      3.3750,      0.1748,      0.6719,     -0.3848,      2.3750,      0.0330],
        [    -1.2422,      0.4727,      1.0703,     -0.5820,     -2.1250,     -1.8750,      0.4883,      0.9023,      0.5898,      0.1104,      0.6719,      0.7773,      0.9961,      0.2383,      1.8047,     -0.0640],
        [     0.3770,     -0.2793,      0.2070,     -0.3633,     -1.6484,     -1.2969,      0.8867,      1.0000,      0.9805,      1.7422,      1.5156,      0.0452,      1.1484,     -1.8125,      1.1406,     -0.2002],
        [    -2.1250,      0.0840,     -1.0156,     -0.5547,     -0.5664,     -0.8750,     -1.3672,      0.5273,     -0.1206,     -0.2129,      0.9219,     -0.0698,      0.0024,     -0.8984,      0.2471,     -0.3223],
        [    -2.9062,      0.7383,      1.2266,     -1.3906,     -0.6562,     -1.1641,     -1.2734,     -0.5312,     -0.5156,      0.1226,      2.5312,     -0.0466,      1.2969,     -0.0698,      0.3730,     -1.1016]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -170.0000,     90.0000,     64.0000,    135.0000,     94.0000,    -69.5000,    -23.2500,     37.2500,     32.5000,   -103.5000,   -101.5000,     29.7500,     -5.4375,     24.0000,     90.0000,     29.0000],
        [  -284.0000,    -32.0000,    -32.2500,    -22.2500,    -51.0000,   -109.0000,    -89.0000,    -33.7500,      2.5000,    -25.2500,    188.0000,     43.7500,     32.2500,      2.7656,     90.5000,     13.6250],
        [   -84.0000,     10.0625,      3.2188,    -58.5000,     56.5000,   -113.5000,   -177.0000,     32.2500,    -26.6250,    -56.5000,    314.0000,     65.5000,     50.7500,    -21.5000,      8.3750,     67.0000],
        [  -314.0000,     80.0000,    207.0000,     -5.6250,    -39.5000,    -87.0000,    -92.0000,     78.0000,     34.0000,    -13.4375,    172.0000,     70.0000,     72.0000,      9.7500,    136.0000,     -8.5000],
        [  -152.0000,    -40.5000,     11.8125,    -25.1250,    -87.5000,    -46.0000,     47.5000,     98.0000,    -91.5000,      5.1875,    440.0000,     15.6250,      4.3125,     12.4375,    137.0000,    -14.5000],
        [  -181.0000,     34.0000,    104.0000,    -33.2500,   -126.0000,    -63.2500,     93.0000,     94.5000,    -14.6875,     -3.3125,    468.0000,     19.3750,     33.5000,    -10.3125,    268.0000,      3.0000],
        [  -169.0000,     45.5000,    113.5000,    -61.0000,   -179.0000,   -118.0000,     56.5000,     89.0000,     45.5000,      9.7500,     91.0000,     85.0000,     49.0000,      6.2812,    199.0000,     -5.7500],
        [    57.0000,    -29.7500,     24.5000,    -42.5000,   -154.0000,    -91.0000,    114.0000,    110.0000,     84.0000,    172.0000,    230.0000,      5.5000,     62.7500,    -53.2500,    140.0000,    -20.0000],
        [  -310.0000,      8.6250,   -115.0000,    -62.0000,    -50.5000,    -58.7500,   -168.0000,     55.7500,     -9.8750,    -20.1250,    134.0000,     -8.1250,      0.1250,    -25.2500,     29.2500,    -30.7500],
        [  -440.0000,     78.5000,    144.0000,   -161.0000,    -61.0000,    -81.0000,   -162.0000,    -58.5000,    -44.0000,     12.0000,    380.0000,     -5.6250,     70.5000,     -2.0312,     46.0000,   -110.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   -43.2500,      8.5000,     14.0625,     15.8125,      6.4062,      1.7500,     32.2500,     -4.1875,     -5.7188,     -2.0000,     24.2500,     -8.8125,    -14.6875,      1.8750,     -4.0312,    -18.3750],
        [    27.1250,      4.5938,      3.2500,     -0.7383,    -11.6875,      1.7109,     14.6250,     13.1875,     -0.9336,    -18.1250,      5.7812,      8.4375,      1.4062,      1.0391,      1.4219,    -14.2500],
        [   -72.0000,     14.3750,     -2.8125,    -16.8750,     -7.4062,     -1.0391,     15.8125,    -15.9375,    -12.5000,    -16.8750,     11.0000,     -6.2812,      0.3379,      0.9570,    -23.7500,      6.9375],
        [   -42.2500,     10.2500,    -16.6250,    -14.8750,      5.0938,      2.5000,     -7.7500,      8.8750,     -3.1719,    -19.6250,     70.0000,     -3.7969,     -3.4062,     -0.4277,      0.1582,     13.5625],
        [     4.3125,     14.6875,     -4.0312,    -16.1250,     -2.2031,      2.8750,    -12.1250,    -15.8750,     15.6250,    -18.0000,     28.1250,     26.8750,     -3.5156,      0.8125,     52.7500,     -2.6875],
        [   -37.2500,      1.2266,      8.1875,    -34.2500,    -11.3750,      5.3750,    -54.5000,     -2.6562,      4.1250,     -4.6562,     82.5000,      6.6562,      7.7188,      1.1953,     22.5000,    -14.9375],
        [    24.3750,      5.6562,     -6.5000,    -19.6250,     -5.8438,      3.0156,    -16.1250,     -8.2500,     -3.6094,     -8.3750,     15.5000,      4.8438,      2.8906,     -0.6523,      5.3125,     -4.5938],
        [     5.1562,     10.0625,    -30.7500,     -1.3438,    -23.2500,      6.2812,    -26.3750,     -6.8438,      4.2188,    -37.2500,     37.0000,      5.5625,      2.5938,      0.8281,     49.5000,     11.3125],
        [    24.1250,     23.8750,     46.0000,     -8.8125,    -18.1250,     -1.1250,    -32.0000,     26.5000,      6.6562,    -27.1250,     61.2500,      1.0703,      2.5938,      0.7578,     64.5000,    -27.7500],
        [   -34.7500,     20.7500,     -8.0000,      0.0059,    -15.3125,     -8.5000,    -51.7500,     28.0000,     -5.0000,     -6.7500,     26.8750,     28.8750,     -4.4688,      0.3027,     33.7500,     -6.5625]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.9492,  1.1328,  0.8320,  1.5469,  1.4062, -1.1406,  0.0684,  0.3730,  0.4160, -1.4141, -0.3750,  0.2178, -0.4062,  1.6797,  0.7656,  0.1494],
        [-1.0234, -0.2832, -0.2773, -0.2119, -0.7891, -1.6172, -0.5078, -0.2090,  0.0219, -0.5195,  0.8438,  0.4863,  0.6055,  0.2217,  0.7305, -0.0079],
        [-0.6562,  0.2656,  0.0041, -0.7305,  0.6484, -1.8281, -1.1562,  0.1738, -0.5742, -0.9258,  1.4922,  0.5820,  0.9727, -1.2578, -0.1289,  0.9844],
        [-1.3203,  0.8594,  1.6797, -0.1748, -0.3984, -1.1797, -0.6250,  0.8125,  0.3984, -0.3672,  0.9766,  0.5703,  1.1406,  0.5039,  1.0000,  0.0591],
        [-0.6523, -0.2930,  0.0820, -0.4160, -1.2422, -0.7148,  0.2656,  0.9180, -1.1641, -0.1689,  2.2500,  0.4355,  0.0157,  0.8516,  1.6641, -0.2383],
        [-1.0391,  0.4316,  1.2734, -0.7383, -2.0469, -1.0391,  0.3105,  1.1094, -0.1748, -0.1138,  2.8594,  0.2891,  0.8867, -0.6328,  2.7500, -0.1797],
        [-0.6719,  0.6133,  1.1875, -0.8594, -2.6875, -2.0156,  0.3184,  0.9453,  0.6758,  0.0190,  0.5391,  0.9727,  1.0859,  0.3809,  1.8828, -0.1514],
        [ 0.2617, -0.2148, -0.0630, -0.4258, -2.3594, -1.3594,  0.6289,  1.1094,  1.3047,  1.7109,  1.2344,  0.1089,  1.2422, -3.2500,  1.5938, -0.1157],
        [-1.2500,  0.3672, -0.7227, -0.7109, -0.9375, -0.9844, -1.4844,  0.9102, -0.0491, -0.6172,  0.9258, -0.0723,  0.0535, -1.5625,  0.8125, -0.8047],
        [-1.9766,  1.0703,  1.3516, -1.5391, -1.0000, -1.4141, -1.5156, -0.3223, -0.7148,  0.0654,  1.8438,  0.2256,  1.2422, -0.1050,  0.6602, -1.5312]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -213.0000,     98.5000,     78.0000,    151.0000,    100.5000,    -68.0000,      9.0000,     33.0000,     26.7500,   -105.5000,    -77.0000,     21.0000,    -20.1250,     25.8750,     86.0000,     10.6250],
        [  -256.0000,    -27.3750,    -29.0000,    -23.0000,    -62.7500,   -107.5000,    -74.5000,    -20.5000,      1.5625,    -43.5000,    194.0000,     52.2500,     33.7500,      3.8125,     92.0000,     -0.6250],
        [  -156.0000,     24.5000,      0.4062,    -75.5000,     49.0000,   -114.5000,   -161.0000,     16.2500,    -39.0000,    -73.5000,    324.0000,     59.2500,     51.0000,    -20.5000,    -15.3750,     74.0000],
        [  -356.0000,     90.0000,    190.0000,    -20.5000,    -34.5000,    -84.5000,   -100.0000,     87.0000,     30.8750,    -33.0000,    242.0000,     66.0000,     68.5000,      9.3125,    136.0000,      5.0625],
        [  -148.0000,    -25.7500,      7.7812,    -41.2500,    -89.5000,    -43.0000,     35.5000,     82.0000,    -76.0000,    -12.8125,    468.0000,     42.5000,      0.7969,     13.2500,    190.0000,    -17.2500],
        [  -218.0000,     35.2500,    112.0000,    -67.5000,   -137.0000,    -58.0000,     38.5000,     92.0000,    -10.5625,     -7.9688,    552.0000,     26.0000,     41.2500,     -9.1250,    290.0000,    -11.9375],
        [  -145.0000,     51.2500,    107.0000,    -80.5000,   -185.0000,   -115.0000,     40.5000,     81.0000,     42.0000,      1.3750,    106.5000,     90.0000,     52.0000,      5.6250,    204.0000,    -10.3750],
        [    62.2500,    -19.7500,     -6.2500,    -43.7500,   -177.0000,    -84.5000,     87.5000,    103.0000,     88.0000,    135.0000,    268.0000,     11.0625,     65.5000,    -52.5000,    190.0000,     -8.6875],
        [  -286.0000,     32.5000,    -69.0000,    -71.0000,    -68.5000,    -60.0000,   -200.0000,     82.0000,     -3.2188,    -47.2500,    195.0000,     -7.0625,      2.7188,    -24.5000,     94.0000,    -58.5000],
        [  -474.0000,     99.0000,    136.0000,   -161.0000,    -76.5000,    -89.5000,   -214.0000,    -30.5000,    -49.0000,      5.2500,    406.0000,     23.2500,     66.0000,     -1.7266,     80.0000,   -116.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    35.2500,     24.6250,      8.2500,     -8.6875,      4.0312,      7.5000,      0.6602,     37.5000,     15.5000,     29.6250,      0.8008,     -9.2500,      5.1250,      1.8828,     70.0000,    -18.6250],
        [  -112.5000,     -3.2500,     10.1875,    137.0000,    -17.0000,    -38.7500,    -25.5000,      1.5469,     85.0000,     16.2500,     25.3750,    120.5000,    -10.2500,     -2.0938,    -33.2500,    -10.3125],
        [    71.5000,    -17.3750,     20.0000,    173.0000,    -47.2500,    -14.5625,   -143.0000,     97.0000,     92.0000,    -28.7500,     16.2500,     49.5000,    -11.5000,     -0.4961,    187.0000,     -2.8438],
        [  -183.0000,     15.6250,     77.5000,     -1.4375,     -4.0625,      5.0312,   -156.0000,    115.5000,     62.2500,     -1.0703,    -71.0000,     70.0000,      1.5156,     -3.5312,     90.5000,     88.0000],
        [  -129.0000,    -74.0000,     61.5000,     33.5000,      2.1875,     10.2500,     49.0000,     63.7500,     32.5000,     -2.8438,     23.7500,    -30.7500,    -12.8750,      1.7031,     -8.8125,      2.0625],
        [    91.0000,    -74.5000,     21.3750,     46.2500,      0.5312,     -5.7188,    201.0000,    134.0000,     21.6250,     45.0000,    -14.0625,     56.5000,      5.5000,      2.7656,    -89.0000,    111.5000],
        [    -6.4062,     47.5000,    -19.7500,     93.5000,    -30.8750,     -1.7891,   -179.0000,     29.7500,    -35.5000,     25.8750,      2.0000,    119.5000,     10.3125,     -0.5156,     23.0000,    -13.7500],
        [    93.0000,     11.9375,    134.0000,     65.5000,     23.7500,     11.3125,    -80.5000,     31.1250,    -31.8750,     -8.9375,     36.0000,    -52.2500,     -4.0938,     -2.4531,    111.0000,     55.7500],
        [  -101.0000,      6.4062,     27.1250,     94.0000,      2.6250,      4.3750,    -50.0000,     58.5000,    -17.1250,    -39.5000,     75.5000,    -66.5000,     -9.0625,      0.0771,     22.3750,     -7.3438],
        [   -15.8125,     -9.6250,     65.5000,     66.0000,     11.6250,    -35.2500,    -47.0000,    -17.3750,     29.1250,      6.2812,    -28.6250,    -59.2500,      8.6250,      0.7383,    -64.0000,     -4.4062]]
-------------------------
name='positions layer 20'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 20'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    35.2500,     24.6250,      8.2500,     -8.6875,      4.0312,      7.5000,      0.6602,     37.5000,     15.5000,     29.6250,      0.8008,     -9.2500,      5.1250,      1.8828,     70.0000,    -18.6250],
        [  -112.5000,     -3.2500,     10.1875,    137.0000,    -17.0000,    -38.7500,    -25.5000,      1.5469,     85.0000,     16.2500,     25.3750,    120.5000,    -10.2500,     -2.0938,    -33.2500,    -10.3125],
        [    71.5000,    -17.3750,     20.0000,    173.0000,    -47.2500,    -14.5625,   -143.0000,     97.0000,     92.0000,    -28.7500,     16.2500,     49.5000,    -11.5000,     -0.4961,    187.0000,     -2.8438],
        [  -183.0000,     15.6250,     77.5000,     -1.4375,     -4.0625,      5.0312,   -156.0000,    115.5000,     62.2500,     -1.0703,    -71.0000,     70.0000,      1.5156,     -3.5312,     90.5000,     88.0000],
        [  -129.0000,    -74.0000,     61.5000,     33.5000,      2.1875,     10.2500,     49.0000,     63.7500,     32.5000,     -2.8438,     23.7500,    -30.7500,    -12.8750,      1.7031,     -8.8125,      2.0625],
        [    91.0000,    -74.5000,     21.3750,     46.2500,      0.5312,     -5.7188,    201.0000,    134.0000,     21.6250,     45.0000,    -14.0625,     56.5000,      5.5000,      2.7656,    -89.0000,    111.5000],
        [    -6.4062,     47.5000,    -19.7500,     93.5000,    -30.8750,     -1.7891,   -179.0000,     29.7500,    -35.5000,     25.8750,      2.0000,    119.5000,     10.3125,     -0.5156,     23.0000,    -13.7500],
        [    93.0000,     11.9375,    134.0000,     65.5000,     23.7500,     11.3125,    -80.5000,     31.1250,    -31.8750,     -8.9375,     36.0000,    -52.2500,     -4.0938,     -2.4531,    111.0000,     55.7500],
        [  -101.0000,      6.4062,     27.1250,     94.0000,      2.6250,      4.3750,    -50.0000,     58.5000,    -17.1250,    -39.5000,     75.5000,    -66.5000,     -9.0625,      0.0771,     22.3750,     -7.3438],
        [   -15.8125,     -9.6250,     65.5000,     66.0000,     11.6250,    -35.2500,    -47.0000,    -17.3750,     29.1250,      6.2812,    -28.6250,    -59.2500,      8.6250,      0.7383,    -64.0000,     -4.4062]]
name='residual layer 20'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -213.0000,     98.5000,     78.0000,    151.0000,    100.5000,    -68.0000,      9.0000,     33.0000,     26.7500,   -105.5000,    -77.0000,     21.0000,    -20.1250,     25.8750,     86.0000,     10.6250],
        [  -256.0000,    -27.3750,    -29.0000,    -23.0000,    -62.7500,   -107.5000,    -74.5000,    -20.5000,      1.5625,    -43.5000,    194.0000,     52.2500,     33.7500,      3.8125,     92.0000,     -0.6250],
        [  -156.0000,     24.5000,      0.4062,    -75.5000,     49.0000,   -114.5000,   -161.0000,     16.2500,    -39.0000,    -73.5000,    324.0000,     59.2500,     51.0000,    -20.5000,    -15.3750,     74.0000],
        [  -356.0000,     90.0000,    190.0000,    -20.5000,    -34.5000,    -84.5000,   -100.0000,     87.0000,     30.8750,    -33.0000,    242.0000,     66.0000,     68.5000,      9.3125,    136.0000,      5.0625],
        [  -148.0000,    -25.7500,      7.7812,    -41.2500,    -89.5000,    -43.0000,     35.5000,     82.0000,    -76.0000,    -12.8125,    468.0000,     42.5000,      0.7969,     13.2500,    190.0000,    -17.2500],
        [  -218.0000,     35.2500,    112.0000,    -67.5000,   -137.0000,    -58.0000,     38.5000,     92.0000,    -10.5625,     -7.9688,    552.0000,     26.0000,     41.2500,     -9.1250,    290.0000,    -11.9375],
        [  -145.0000,     51.2500,    107.0000,    -80.5000,   -185.0000,   -115.0000,     40.5000,     81.0000,     42.0000,      1.3750,    106.5000,     90.0000,     52.0000,      5.6250,    204.0000,    -10.3750],
        [    62.2500,    -19.7500,     -6.2500,    -43.7500,   -177.0000,    -84.5000,     87.5000,    103.0000,     88.0000,    135.0000,    268.0000,     11.0625,     65.5000,    -52.5000,    190.0000,     -8.6875],
        [  -286.0000,     32.5000,    -69.0000,    -71.0000,    -68.5000,    -60.0000,   -200.0000,     82.0000,     -3.2188,    -47.2500,    195.0000,     -7.0625,      2.7188,    -24.5000,     94.0000,    -58.5000],
        [  -474.0000,     99.0000,    136.0000,   -161.0000,    -76.5000,    -89.5000,   -214.0000,    -30.5000,    -49.0000,      5.2500,    406.0000,     23.2500,     66.0000,     -1.7266,     80.0000,   -116.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.8125,  0.8516,  0.4688,  0.8281,  0.7656, -0.6484,  0.0444,  0.4648,  0.3672, -0.4863, -0.3438,  0.0688, -0.1816,  0.7344,  0.8359, -0.0535],
        [-1.3438, -0.1680, -0.0811,  0.5273, -0.4629, -1.2422, -0.3652, -0.0991,  0.5977, -0.1387,  0.7891,  0.8008,  0.2256,  0.0361,  0.2500, -0.0583],
        [-0.3125,  0.0398,  0.0898,  0.4590,  0.0104, -1.1172, -1.1250,  0.6016,  0.3711, -0.5312,  1.2422,  0.5117,  0.3848, -0.4492,  0.7422,  0.3848],
        [-1.8906,  0.5586,  1.1094, -0.0981, -0.2168, -0.6562, -0.9023,  1.0156,  0.6211, -0.1670,  0.5938,  0.6055,  0.6484,  0.1172,  0.9297,  0.4805],
        [-1.2344, -0.6719,  0.3652, -0.0442, -0.6211, -0.3418,  0.3770,  0.9336, -0.3691, -0.0977,  2.1719,  0.0669, -0.1416,  0.3867,  0.9453, -0.0996],
        [-0.5273, -0.2451,  0.6523, -0.1123, -0.9023, -0.6172,  0.9922,  1.3438,  0.0869,  0.2148,  2.1875,  0.4355,  0.5117, -0.1514,  0.9766,  0.6055],
        [-0.6406,  0.6289,  0.4375,  0.0698, -1.4531, -1.1562, -0.5859,  0.6680,  0.0522,  0.1611,  0.4512,  1.1250,  0.6953,  0.1240,  1.1172, -0.1484],
        [ 0.5781, -0.0437,  0.5625,  0.1030, -0.9062, -0.6367,  0.0261,  0.7148,  0.3965,  0.6562,  1.1094, -0.1953,  0.6016, -1.1797,  1.3125,  0.2559],
        [-1.7812,  0.2715, -0.2285,  0.1348, -0.4844, -0.5977, -1.1562,  0.9297, -0.1777, -0.5586,  1.2266, -0.4297, -0.0767, -0.6484,  0.6250, -0.4434],
        [-1.8516,  0.5117,  0.8984, -0.4570, -0.3906, -1.1016, -0.9883, -0.2598, -0.1426,  0.0608,  1.4062, -0.1729,  0.7422, -0.0215,  0.0708, -0.6680]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-178.0000,  123.0000,   86.0000,  142.0000,  104.5000,  -60.5000,    9.6875,   70.5000,   42.2500,  -76.0000,  -76.0000,   11.7500,  -15.0000,   27.7500,  156.0000,   -8.0000],
        [-368.0000,  -30.6250,  -18.7500,  114.0000,  -80.0000, -146.0000, -100.0000,  -19.0000,   86.5000,  -27.2500,  219.0000,  173.0000,   23.5000,    1.7188,   58.7500,  -10.9375],
        [ -84.5000,    7.1250,   20.3750,   97.5000,    1.7500, -129.0000, -304.0000,  113.0000,   53.0000, -102.0000,  340.0000,  109.0000,   39.5000,  -21.0000,  172.0000,   71.0000],
        [-540.0000,  105.5000,  268.0000,  -22.0000,  -38.5000,  -79.5000, -256.0000,  202.0000,   93.0000,  -34.0000,  171.0000,  136.0000,   70.0000,    5.7812,  226.0000,   93.0000],
        [-276.0000, -100.0000,   69.5000,   -7.7500,  -87.5000,  -32.7500,   84.5000,  146.0000,  -43.5000,  -15.6250,  492.0000,   11.7500,  -12.0625,   14.9375,  181.0000,  -15.1875],
        [-127.0000,  -39.2500,  133.0000,  -21.2500, -136.0000,  -63.7500,  240.0000,  226.0000,   11.0625,   37.0000,  536.0000,   82.5000,   46.7500,   -6.3750,  201.0000,   99.5000],
        [-151.0000,   99.0000,   87.0000,   13.0000, -216.0000, -117.0000, -138.0000,  111.0000,    6.5000,   27.2500,  108.5000,  210.0000,   62.2500,    5.1250,  227.0000,  -24.1250],
        [ 155.0000,   -7.8125,  128.0000,   21.7500, -153.0000,  -73.0000,    7.0000,  134.0000,   56.0000,  126.0000,  304.0000,  -41.2500,   61.5000,  -55.0000,  300.0000,   47.0000],
        [-388.0000,   39.0000,  -42.0000,   23.0000,  -66.0000,  -55.5000, -250.0000,  140.0000,  -20.3750,  -87.0000,  270.0000,  -73.5000,   -6.3438,  -24.3750,  116.5000,  -66.0000],
        [-490.0000,   89.5000,  202.0000,  -95.0000,  -65.0000, -125.0000, -260.0000,  -48.0000,  -19.8750,   11.5000,  378.0000,  -36.0000,   74.5000,   -0.9883,   16.0000, -121.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ -2.9062,   1.3516, -15.8750,  -6.0938,   2.8594,  -3.0625, -13.1250,  12.1875,  -3.2344,   4.7812,   5.8125,  -6.2188,  -5.7188,  -0.7891,   6.5938,  -4.4062],
        [ -2.2031,  20.0000, -19.6250,  -8.5625, -10.6250,  -1.1016, -30.0000,   6.3438,  -4.1562,  14.9375,   8.5625,   5.7188,  -1.5156,  -0.7539,  19.0000,  -9.3125],
        [ 20.8750,   8.8125, -52.2500, -14.1250, -16.5000,  -3.2500, -27.5000,  17.7500, -23.0000,  13.7500,  27.5000,  18.1250,  -4.1562,  -0.2910,  14.3750,   6.9688],
        [ 17.1250,   6.3750, -31.1250, -26.7500, -11.7500,  -6.1562, -13.1875,  20.1250,  -6.2500,   6.3125,  31.1250,   3.2344,  -4.9375,  -0.6133,  30.1250,   2.9531],
        [ 24.2500,  24.0000, -42.7500, -26.3750, -14.2500,  -3.1875, -10.3750,  -0.6445, -20.0000,  -6.1562,  35.5000,   1.6250,  -3.1250,  -0.4922,  30.0000,  13.0000],
        [  8.3125,  15.5000, -33.7500, -26.7500,  -8.3750,   3.2344, -45.2500, -10.8125, -11.0000,  -8.2500,  30.7500,   9.5000,  -6.5625,  -1.0156,  43.7500,  -3.1406],
        [ 10.3750,  -9.1250, -20.2500, -25.8750,  -4.9688,   0.3730,   6.5312,   9.2500, -12.6875,  -5.9062,   3.1562,  -6.2188,  -5.5000,  -0.7773,  12.9375,   1.4375],
        [ 18.5000, -11.5000, -15.3750, -41.2500, -10.9375,  -8.3750,  17.6250,  -0.8242, -22.5000, -16.3750,  13.1250, -15.8125,  -7.2812,  -0.8398,  27.5000,  -2.4531],
        [ 29.5000,  17.1250, -26.0000, -11.3750,  -1.3438,  -8.1250, -38.7500, -16.6250, -20.5000, -12.9375,  23.2500,  29.6250,  -3.1406,  -0.5078,  20.6250,   4.2188],
        [  3.6875,  32.2500, -29.1250, -14.3750, -18.3750,  -2.5781, -69.0000, -21.5000, -25.7500, -14.7500,  23.6250,  14.6875,  -2.3906,  -1.2969,  47.5000, -12.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -0.7227,      1.1250,      0.5703,      1.0938,      1.1719,     -0.9648,     -0.0205,      0.7812,      0.4707,     -0.7227,     -0.3027,      0.0471,     -0.3340,      1.2266,      1.0938,     -0.1348],
        [    -1.2188,     -0.0796,     -0.2578,      0.6992,     -0.8164,     -1.8516,     -0.6406,     -0.0986,      0.8203,     -0.1035,      0.8125,      1.2578,      0.2949,      0.0366,      0.4336,     -0.1826],
        [    -0.2314,      0.1309,     -0.2354,      0.6133,     -0.1465,     -1.8359,     -1.7969,      1.1250,      0.3320,     -0.8203,      1.4531,      0.9883,      0.5234,     -0.8906,      1.1484,      0.7734],
        [    -1.8125,      0.8789,      1.6875,     -0.3418,     -0.4785,     -1.1406,     -1.3984,      1.8281,      0.9141,     -0.2461,      0.7617,      1.0312,      0.9180,      0.2070,      1.5000,      0.9141],
        [    -1.2969,     -0.8867,      0.2812,     -0.3555,     -1.4297,     -0.7070,      0.5703,      1.7734,     -0.9883,     -0.2852,      2.9375,      0.1475,     -0.3184,      0.8555,      1.8359,     -0.0308],
        [    -0.5664,     -0.2559,      0.9688,     -0.4629,     -1.8906,     -1.1094,      1.3906,      2.4375,      0.0009,      0.3496,      2.9375,      0.9414,      0.7773,     -0.4043,      1.9844,      1.2578],
        [    -0.6172,      0.8945,      0.5977,     -0.1143,     -2.6406,     -1.9531,     -0.8633,      1.2500,     -0.0825,      0.2393,      0.5312,      1.9141,      1.0156,      0.2188,      1.7812,     -0.2715],
        [     0.6953,     -0.1748,      0.9180,     -0.1572,     -1.7969,     -1.2422,      0.1465,      1.2656,      0.4062,      1.1172,      1.3750,     -0.4883,      0.8828,     -2.5625,      2.2188,      0.4863],
        [    -1.4766,      0.5234,     -0.5703,      0.0967,     -0.7617,     -1.0078,     -1.7812,      1.2031,     -0.5117,     -1.0547,      1.3203,     -0.3867,     -0.1592,     -1.1797,      0.9609,     -0.6992],
        [    -1.4688,      0.8320,      1.0625,     -0.6680,     -0.6875,     -1.4688,     -1.4844,     -0.4961,     -0.4180,     -0.0251,      1.3203,     -0.1377,      0.8828,     -0.0791,      0.3242,     -1.0938]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -181.0000,    124.5000,     70.0000,    136.0000,    107.5000,    -63.5000,     -3.4375,     82.5000,     39.0000,    -71.0000,    -70.0000,      5.5312,    -20.7500,     27.0000,    163.0000,    -12.3750],
        [  -370.0000,    -10.6250,    -38.5000,    105.5000,    -90.5000,   -147.0000,   -130.0000,    -12.6250,     82.5000,    -12.3125,    228.0000,    179.0000,     22.0000,      0.9648,     78.0000,    -20.2500],
        [   -63.5000,     15.9375,    -31.8750,     83.5000,    -14.7500,   -132.0000,   -332.0000,    131.0000,     30.0000,    -88.0000,    368.0000,    127.0000,     35.2500,    -21.2500,    186.0000,     78.0000],
        [  -524.0000,    112.0000,    237.0000,    -48.7500,    -50.2500,    -85.5000,   -270.0000,    222.0000,     87.0000,    -27.7500,    202.0000,    139.0000,     65.0000,      5.1562,    256.0000,     96.0000],
        [  -252.0000,    -76.0000,     26.7500,    -34.0000,   -102.0000,    -36.0000,     74.0000,    145.0000,    -63.5000,    -21.7500,    528.0000,     13.3750,    -15.1875,     14.4375,    211.0000,     -2.1875],
        [  -118.5000,    -23.7500,     99.0000,    -48.0000,   -144.0000,    -60.5000,    195.0000,    215.0000,      0.0625,     28.7500,    568.0000,     92.0000,     40.2500,     -7.3750,    245.0000,     96.5000],
        [  -141.0000,     90.0000,     67.0000,    -12.8750,   -221.0000,   -116.5000,   -131.0000,    120.0000,     -6.1875,     21.3750,    111.5000,    204.0000,     56.7500,      4.3438,    240.0000,    -22.7500],
        [   174.0000,    -19.2500,    112.5000,    -19.5000,   -164.0000,    -81.5000,     24.6250,    133.0000,     33.5000,    109.5000,    318.0000,    -57.0000,     54.2500,    -55.7500,    328.0000,     44.5000],
        [  -358.0000,     56.0000,    -68.0000,     11.6250,    -67.5000,    -63.5000,   -288.0000,    123.5000,    -41.0000,   -100.0000,    294.0000,    -44.0000,     -9.5000,    -24.8750,    137.0000,    -61.7500],
        [  -486.0000,    122.0000,    173.0000,   -109.5000,    -83.5000,   -127.5000,   -328.0000,    -69.5000,    -45.5000,     -3.2500,    402.0000,    -21.2500,     72.0000,     -2.2812,     63.5000,   -133.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   -29.1250,     42.5000,    100.5000,    -15.3125,      3.6094,     -3.5938,     12.3125,    -23.3750,      6.7812,     31.8750,    -61.0000,    -60.2500,     -4.7500,     -0.3535,     18.0000,    -18.5000],
        [   -43.2500,    -88.0000,     25.2500,     45.5000,    -41.7500,     -6.4688,    -50.5000,    -30.3750,     31.6250,    -66.0000,     -5.7188,    -20.3750,     -0.2480,      0.7812,     57.5000,     44.2500],
        [   -71.0000,      1.0938,   -163.0000,   -209.0000,    -96.0000,     26.8750,     -1.8516,     72.5000,     44.2500,   -122.5000,    174.0000,    -73.0000,     10.1875,      2.1719,     48.5000,    122.5000],
        [   -83.5000,     18.6250,    -99.5000,     44.5000,    -46.2500,    -11.0625,   -153.0000,     15.6250,    -55.0000,     20.6250,     21.1250,      0.7227,    -19.2500,      1.8672,    -95.5000,    -10.8125],
        [  -119.0000,    -84.0000,   -209.0000,    104.0000,    -10.0000,     21.3750,    129.0000,    130.0000,     20.6250,     55.2500,      7.9062,    -19.2500,     24.5000,      1.6406,    141.0000,    134.0000],
        [   -76.0000,    -98.5000,    129.0000,    -69.5000,    -35.5000,    -38.2500,     -3.3750,    262.0000,      9.1250,     -8.5625,     35.2500,    165.0000,     11.5000,      0.9727,    163.0000,    -52.0000],
        [  -268.0000,     29.2500,    114.0000,     16.8750,    -98.5000,    -25.2500,   -112.5000,    -92.0000,     64.0000,    -76.5000,     58.2500,    -60.2500,     47.2500,     -0.2373,      7.9375,     57.5000],
        [    29.2500,      3.4375,     16.5000,     63.5000,    -31.8750,      5.3438,     25.6250,    212.0000,    148.0000,     29.1250,    116.5000,   -177.0000,      3.6562,     -4.0312,     97.5000,     25.1250],
        [  -133.0000,     47.5000,     -9.0625,    -46.5000,     15.0625,     25.6250,     16.3750,     44.5000,    -74.0000,    -64.5000,    107.0000,    -26.1250,    -22.6250,      1.9297,     44.5000,     43.2500],
        [   -77.0000,     76.0000,    -15.0000,    -18.5000,    -22.8750,      5.5000,     -4.7500,      6.1562,     17.7500,     15.6875,      7.0625,    -11.1250,     -1.8906,     -0.8516,     45.5000,     44.2500]]
-------------------------
name='positions layer 21'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 21'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   -29.1250,     42.5000,    100.5000,    -15.3125,      3.6094,     -3.5938,     12.3125,    -23.3750,      6.7812,     31.8750,    -61.0000,    -60.2500,     -4.7500,     -0.3535,     18.0000,    -18.5000],
        [   -43.2500,    -88.0000,     25.2500,     45.5000,    -41.7500,     -6.4688,    -50.5000,    -30.3750,     31.6250,    -66.0000,     -5.7188,    -20.3750,     -0.2480,      0.7812,     57.5000,     44.2500],
        [   -71.0000,      1.0938,   -163.0000,   -209.0000,    -96.0000,     26.8750,     -1.8516,     72.5000,     44.2500,   -122.5000,    174.0000,    -73.0000,     10.1875,      2.1719,     48.5000,    122.5000],
        [   -83.5000,     18.6250,    -99.5000,     44.5000,    -46.2500,    -11.0625,   -153.0000,     15.6250,    -55.0000,     20.6250,     21.1250,      0.7227,    -19.2500,      1.8672,    -95.5000,    -10.8125],
        [  -119.0000,    -84.0000,   -209.0000,    104.0000,    -10.0000,     21.3750,    129.0000,    130.0000,     20.6250,     55.2500,      7.9062,    -19.2500,     24.5000,      1.6406,    141.0000,    134.0000],
        [   -76.0000,    -98.5000,    129.0000,    -69.5000,    -35.5000,    -38.2500,     -3.3750,    262.0000,      9.1250,     -8.5625,     35.2500,    165.0000,     11.5000,      0.9727,    163.0000,    -52.0000],
        [  -268.0000,     29.2500,    114.0000,     16.8750,    -98.5000,    -25.2500,   -112.5000,    -92.0000,     64.0000,    -76.5000,     58.2500,    -60.2500,     47.2500,     -0.2373,      7.9375,     57.5000],
        [    29.2500,      3.4375,     16.5000,     63.5000,    -31.8750,      5.3438,     25.6250,    212.0000,    148.0000,     29.1250,    116.5000,   -177.0000,      3.6562,     -4.0312,     97.5000,     25.1250],
        [  -133.0000,     47.5000,     -9.0625,    -46.5000,     15.0625,     25.6250,     16.3750,     44.5000,    -74.0000,    -64.5000,    107.0000,    -26.1250,    -22.6250,      1.9297,     44.5000,     43.2500],
        [   -77.0000,     76.0000,    -15.0000,    -18.5000,    -22.8750,      5.5000,     -4.7500,      6.1562,     17.7500,     15.6875,      7.0625,    -11.1250,     -1.8906,     -0.8516,     45.5000,     44.2500]]
name='residual layer 21'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -181.0000,    124.5000,     70.0000,    136.0000,    107.5000,    -63.5000,     -3.4375,     82.5000,     39.0000,    -71.0000,    -70.0000,      5.5312,    -20.7500,     27.0000,    163.0000,    -12.3750],
        [  -370.0000,    -10.6250,    -38.5000,    105.5000,    -90.5000,   -147.0000,   -130.0000,    -12.6250,     82.5000,    -12.3125,    228.0000,    179.0000,     22.0000,      0.9648,     78.0000,    -20.2500],
        [   -63.5000,     15.9375,    -31.8750,     83.5000,    -14.7500,   -132.0000,   -332.0000,    131.0000,     30.0000,    -88.0000,    368.0000,    127.0000,     35.2500,    -21.2500,    186.0000,     78.0000],
        [  -524.0000,    112.0000,    237.0000,    -48.7500,    -50.2500,    -85.5000,   -270.0000,    222.0000,     87.0000,    -27.7500,    202.0000,    139.0000,     65.0000,      5.1562,    256.0000,     96.0000],
        [  -252.0000,    -76.0000,     26.7500,    -34.0000,   -102.0000,    -36.0000,     74.0000,    145.0000,    -63.5000,    -21.7500,    528.0000,     13.3750,    -15.1875,     14.4375,    211.0000,     -2.1875],
        [  -118.5000,    -23.7500,     99.0000,    -48.0000,   -144.0000,    -60.5000,    195.0000,    215.0000,      0.0625,     28.7500,    568.0000,     92.0000,     40.2500,     -7.3750,    245.0000,     96.5000],
        [  -141.0000,     90.0000,     67.0000,    -12.8750,   -221.0000,   -116.5000,   -131.0000,    120.0000,     -6.1875,     21.3750,    111.5000,    204.0000,     56.7500,      4.3438,    240.0000,    -22.7500],
        [   174.0000,    -19.2500,    112.5000,    -19.5000,   -164.0000,    -81.5000,     24.6250,    133.0000,     33.5000,    109.5000,    318.0000,    -57.0000,     54.2500,    -55.7500,    328.0000,     44.5000],
        [  -358.0000,     56.0000,    -68.0000,     11.6250,    -67.5000,    -63.5000,   -288.0000,    123.5000,    -41.0000,   -100.0000,    294.0000,    -44.0000,     -9.5000,    -24.8750,    137.0000,    -61.7500],
        [  -486.0000,    122.0000,    173.0000,   -109.5000,    -83.5000,   -127.5000,   -328.0000,    -69.5000,    -45.5000,     -3.2500,    402.0000,    -21.2500,     72.0000,     -2.2812,     63.5000,   -133.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-1.1094,  1.0938,  1.0000,  0.6992,  0.8281, -0.7305,  0.0474,  0.4023,  0.3516, -0.2754, -0.7461, -0.3398, -0.3555,  0.7695,  0.9727, -0.2109],
        [-1.8438, -0.5430, -0.0659,  0.7422, -0.8320, -1.4141, -0.8164, -0.2461,  0.7383, -0.4648,  1.0703,  0.8281,  0.2559,  0.0425,  0.6133,  0.1387],
        [-0.5938,  0.0923, -0.9531, -0.6055, -0.6836, -0.9531, -1.4844,  1.1484,  0.4727, -1.2344,  2.5625,  0.2773,  0.5273, -0.4590,  1.0469,  1.1406],
        [-2.6250,  0.6953,  0.6562, -0.0201, -0.5859, -0.8594, -1.8438,  1.3203,  0.2002, -0.0408,  1.0391,  0.7070,  0.5195,  0.1650,  0.7031,  0.4727],
        [-2.0781, -1.1016, -1.1328,  0.4297, -0.8828, -0.1680,  1.1484,  1.9688, -0.3457,  0.2490,  3.2344, -0.0383,  0.1367,  0.4902,  1.9922,  0.9531],
        [-1.0078, -0.7773,  1.3047, -0.6641, -1.2969, -1.0469,  0.9961,  3.1406,  0.0684,  0.1377,  3.3594,  1.5469,  0.6992, -0.1797,  2.1250,  0.2949],
        [-2.2031,  0.7891,  1.0781,  0.0236, -2.4219, -1.5703, -1.3203,  0.1934,  0.4512, -0.3926,  0.9805,  0.9023,  1.4688,  0.1206,  1.3516,  0.2402],
        [ 0.9922, -0.0947,  0.6953,  0.2354, -1.3359, -0.7617,  0.2480,  2.1562,  1.2812,  0.8984,  2.2812, -1.3281,  0.7422, -1.5859,  2.0938,  0.4375],
        [-2.5625,  0.6641, -0.4453, -0.1992, -0.3848, -0.4062, -1.4297,  1.1250, -0.8672, -1.1328,  2.2500, -0.4258, -0.4414, -0.6523,  0.9570, -0.1235],
        [-2.5938,  1.1172,  0.8047, -0.6445, -0.6836, -1.1562, -1.5391, -0.3730, -0.1846,  0.0757,  2.0156, -0.1738,  0.8477, -0.0786,  0.5078, -0.5234]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-210.0000,  167.0000,  170.0000,  120.5000,  111.0000,  -67.0000,    8.8750,   59.0000,   45.7500,  -39.0000, -131.0000,  -54.7500,  -25.5000,   26.6250,  181.0000,  -30.8750],
        [-414.0000,  -98.5000,  -13.2500,  151.0000, -132.0000, -153.0000, -180.0000,  -43.0000,  114.0000,  -78.5000,  222.0000,  159.0000,   21.7500,    1.7500,  136.0000,   24.0000],
        [-134.0000,   17.0000, -195.0000, -125.5000, -111.0000, -105.0000, -334.0000,  204.0000,   74.0000, -210.0000,  544.0000,   54.0000,   45.5000,  -19.1250,  234.0000,  200.0000],
        [-608.0000,  131.0000,  138.0000,   -4.2500,  -96.5000,  -96.5000, -424.0000,  238.0000,   32.0000,   -7.1250,  223.0000,  140.0000,   45.7500,    7.0312,  160.0000,   85.0000],
        [-372.0000, -160.0000, -182.0000,   70.0000, -112.0000,  -14.6250,  203.0000,  276.0000,  -43.0000,   33.5000,  536.0000,   -5.8750,    9.3125,   16.1250,  352.0000,  132.0000],
        [-194.0000, -122.0000,  228.0000, -117.5000, -180.0000,  -99.0000,  192.0000,  476.0000,    9.1875,   20.2500,  604.0000,  256.0000,   51.7500,   -6.4062,  408.0000,   44.5000],
        [-408.0000,  119.0000,  181.0000,    4.0000, -320.0000, -142.0000, -244.0000,   28.0000,   57.7500,  -55.0000,  170.0000,  144.0000,  104.0000,    4.0938,  248.0000,   34.7500],
        [ 203.0000,  -15.8125,  129.0000,   44.0000, -196.0000,  -76.0000,   50.2500,  344.0000,  182.0000,  139.0000,  434.0000, -234.0000,   58.0000,  -59.7500,  426.0000,   69.5000],
        [-492.0000,  103.5000,  -77.0000,  -35.0000,  -52.5000,  -38.0000, -272.0000,  168.0000, -115.0000, -164.0000,  400.0000,  -70.0000,  -32.0000,  -23.0000,  182.0000,  -18.5000],
        [-564.0000,  198.0000,  158.0000, -128.0000, -106.5000, -122.0000, -332.0000,  -63.2500,  -27.7500,   12.4375,  410.0000,  -32.5000,   70.0000,   -3.1250,  109.0000,  -89.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[   3.0625,   11.4375,   36.7500,  -12.3750,   -0.3633,  -18.1250,   38.2500,   19.1250,   11.5625,   38.2500,  -56.7500,   38.7500,   -9.5000,    2.8438,   37.0000,   -7.0312],
        [ -33.0000,    8.7500,   -5.6562,  -26.5000,    5.9375,    0.8828,    3.2031,    5.3438,   13.6250,  -36.2500,   14.8125,   12.1250,   -4.3438,   -0.5195,  -17.2500,  -46.0000],
        [ -31.2500,    7.4062,   10.1875,   18.1250,  -17.2500,    8.1875,  -11.8125,  -14.8750,   24.1250,  -15.8125,  -17.5000,    9.1875,   -6.0938,   -0.9688,  -15.3125,  -32.5000],
        [  -8.6250,    3.0781,  -34.5000,  -31.2500,    0.8555,    8.1250,   -6.1875,   10.1875,   13.1250,  -16.7500,   53.7500,   16.3750,   -4.2812,   -0.1631,  -10.0625,  -21.7500],
        [ -76.0000,   23.8750,   26.2500,  -16.5000,   -7.7500,    9.1875,   -6.1562,    5.8438,   28.6250,  -23.0000,   76.5000,   22.7500,  -17.2500,   -0.2227,    1.8281,  -59.0000],
        [-122.0000,  -19.7500,   25.5000,  -14.3125,  -22.6250,  -17.7500,   25.6250,   75.0000,    6.1875,  -37.7500,   99.5000,   52.5000,   10.0625,   -7.1250,  -11.7500,  -78.0000],
        [  -1.9688,   -9.5625,   38.7500,  -11.9375,   24.6250,   -2.2031,   53.2500,  -14.5000,    7.6875,   10.9375,  -30.3750,   38.7500,   -0.4141,   -3.0625,   38.2500,    7.0000],
        [  89.5000,   14.7500,   42.5000,    2.8125,   -5.5938,   -6.8750,  -13.8750,   15.0625,   17.2500,    5.6250,   38.5000,    9.6250,    4.8438,    4.1250,   69.0000,    0.9531],
        [ -34.2500,   -4.8438,    2.8125,  -18.7500,   -6.0625,    2.2969,   29.5000,   18.3750,    3.2188,   -4.1562,   42.7500,   30.1250,   -8.9375,   -2.0000,   29.1250,  -34.2500],
        [ -23.1250,   -0.5586,   21.3750,  -28.7500,   -8.5000,   -4.7188,   14.3750,    1.5156,    8.3750,   19.5000,   -2.3906,   26.7500,  -17.1250,   -0.6055,   -7.4375,  -32.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.7344,  1.1953,  1.2891,  0.6953,  0.8984, -0.9375,  0.2236,  0.5352,  0.5000, -0.0057, -0.7031, -0.0977, -0.4180,  0.9062,  1.1484, -0.3008],
        [-1.4375, -0.5430, -0.1069,  0.7266, -0.9297, -1.5156, -0.7617, -0.2344,  1.0078, -0.7930,  0.8008,  0.9492,  0.1885,  0.0344,  0.5664, -0.1582],
        [-0.4980,  0.1387, -0.9844, -0.5859, -0.8828, -0.9062, -1.3984,  1.1016,  0.7266, -1.4531,  1.6719,  0.3262,  0.4004, -0.5273,  0.9766,  1.1250],
        [-1.8125,  0.7461,  0.5352, -0.1885, -0.6445, -0.8086, -1.6875,  1.4141,  0.3281, -0.1504,  0.8594,  0.7891,  0.4121,  0.1758,  0.6562,  0.4180],
        [-1.5938, -0.9219, -0.9844,  0.3457, -0.9805, -0.0603,  0.9453,  1.9453, -0.1270,  0.0806,  2.3125,  0.1035, -0.0957,  0.4961,  1.8828,  0.5859],
        [-1.0000, -0.8516,  1.4219, -0.7617, -1.4844, -1.1562,  0.9297,  3.4062,  0.1201, -0.1196,  2.3750,  1.6953,  0.6641, -0.3750,  1.8828, -0.2402],
        [-1.4297,  0.7188,  1.3516, -0.0500, -2.3594, -1.5625, -0.8906,  0.0913,  0.5625, -0.3281,  0.5156,  1.1016,  1.2188,  0.0312,  1.4844,  0.3281],
        [ 0.9453, -0.0065,  0.9766,  0.2754, -1.4922, -0.8320,  0.1572,  2.2500,  1.5781,  1.0000,  1.6094, -1.2422,  0.6836, -1.5703,  2.3750,  0.5117],
        [-1.7734,  0.6250, -0.4395, -0.3301, -0.4531, -0.3730, -1.0938,  1.2188, -0.9297, -1.2188,  1.5781, -0.2324, -0.4668, -0.7344,  1.0625, -0.4004],
        [-1.7734,  1.1328,  0.9570, -0.8594, -0.8008, -1.1953, -1.2891, -0.3633, -0.1445,  0.2080,  1.3047, -0.0300,  0.5430, -0.0986,  0.4590, -0.8242]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-207.0000,  178.0000,  207.0000,  108.0000,  110.5000,  -85.0000,   47.0000,   78.0000,   57.2500,   -0.7500, -188.0000,  -16.0000,  -35.0000,   29.5000,  218.0000,  -38.0000],
        [-448.0000,  -90.0000,  -18.8750,  124.5000, -126.0000, -152.0000, -177.0000,  -37.7500,  127.5000, -115.0000,  237.0000,  171.0000,   17.3750,    1.2344,  119.0000,  -22.0000],
        [-165.0000,   24.3750, -185.0000, -107.5000, -128.0000,  -97.0000, -346.0000,  189.0000,   98.0000, -226.0000,  528.0000,   63.2500,   39.5000,  -20.1250,  219.0000,  168.0000],
        [-616.0000,  134.0000,  103.5000,  -35.5000,  -95.5000,  -88.5000, -430.0000,  248.0000,   45.0000,  -23.8750,  276.0000,  156.0000,   41.5000,    6.8750,  150.0000,   63.2500],
        [-448.0000, -136.0000, -156.0000,   53.5000, -120.0000,   -5.4375,  197.0000,  282.0000,  -14.3750,   10.5000,  612.0000,   16.8750,   -7.9375,   15.8750,  354.0000,   73.0000],
        [-316.0000, -142.0000,  254.0000, -132.0000, -203.0000, -117.0000,  218.0000,  552.0000,   15.3750,  -17.5000,  704.0000,  308.0000,   61.7500,  -13.5000,  396.0000,  -33.5000],
        [-410.0000,  109.5000,  220.0000,   -7.9375, -296.0000, -144.0000, -191.0000,   13.5000,   65.5000,  -44.0000,  140.0000,  183.0000,  103.5000,    1.0312,  286.0000,   41.7500],
        [ 292.0000,   -1.0625,  172.0000,   46.7500, -202.0000,  -83.0000,   36.5000,  360.0000,  199.0000,  145.0000,  472.0000, -224.0000,   62.7500,  -55.5000,  496.0000,   70.5000],
        [-528.0000,   98.5000,  -74.0000,  -53.7500,  -58.5000,  -35.7500, -242.0000,  186.0000, -112.0000, -168.0000,  442.0000,  -40.0000,  -41.0000,  -25.0000,  211.0000,  -52.7500],
        [-588.0000,  197.0000,  179.0000, -157.0000, -115.0000, -126.5000, -318.0000,  -61.7500,  -19.3750,   32.0000,  408.0000,   -5.7500,   53.0000,   -3.7344,  101.5000, -121.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  49.2500,   69.5000,  -63.5000,   94.5000,    0.8281,   -4.1875,   47.5000,  -49.5000,   50.0000,   36.0000,  -91.5000,  -21.6250,   27.7500,   -2.4219,  122.0000,   15.8125],
        [ -83.0000,    4.0312,   15.1875,    4.5000,  -39.5000,   10.1875,  -24.8750,   52.2500,  -65.0000,  -18.6250,    4.0625,   51.2500,    4.1250,    2.0312,   -4.0938,  -82.0000],
        [ -99.0000,  -62.5000,   25.3750, -138.0000,  -14.3125,   -3.5469,   54.5000,   87.5000, -119.0000,   37.0000,  -18.1250,   75.0000,   11.5625,    3.3125,  -78.0000,  -34.0000],
        [ -30.5000,   63.0000,  -92.5000,  -71.0000,   -8.6875,   22.0000,   60.7500,  -12.3750,  -39.7500,   43.2500,    5.5938,  -54.5000,   11.3125,    5.2188, -165.0000,  -76.0000],
        [ -56.2500, -101.5000,   91.5000,  -43.7500,  -50.7500,   32.5000,    9.8125,   36.5000,   16.7500,  -70.5000,   11.6250,   31.7500,   25.1250,    1.8828,  -93.0000,  -17.1250],
        [  49.0000,  -61.2500,  -65.0000,    8.8125,  -76.5000,   32.5000, -125.5000,   33.0000,   64.0000,   40.0000,  -54.2500,  -32.0000,  -29.6250,    3.1406,  -26.5000,  -66.0000],
        [  37.5000,  -56.0000,  -95.0000,   14.1250,  -90.5000,   -8.8750,   55.7500,  -67.0000,   17.8750,  -37.5000,   45.5000,   34.0000,  -18.1250,    2.0781,  -25.6250, -105.5000],
        [ 191.0000,  -53.7500,  -96.0000,   -9.0625,   -7.4375,   26.1250,   93.0000,  -83.5000,  -11.1250,   26.0000,  158.0000,  -92.5000,   -2.7500,   -2.6094, -162.0000,   -1.6016],
        [ -87.0000,   43.5000,   12.1875,   44.5000,  -23.3750,   23.6250,  -77.5000,   64.5000,  -50.5000,   82.5000,   34.2500,   70.5000,    3.9844,   -0.6328,  -32.0000,    2.3125],
        [ -80.5000,  144.0000,  -37.5000, -139.0000,  -51.0000,   -3.0000,  -37.5000,  -24.2500,   -7.4375,   40.5000,   87.5000,   20.0000,    3.3906,    0.4785,   83.0000,  -68.0000]]
-------------------------
name='positions layer 22'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 22'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  49.2500,   69.5000,  -63.5000,   94.5000,    0.8281,   -4.1875,   47.5000,  -49.5000,   50.0000,   36.0000,  -91.5000,  -21.6250,   27.7500,   -2.4219,  122.0000,   15.8125],
        [ -83.0000,    4.0312,   15.1875,    4.5000,  -39.5000,   10.1875,  -24.8750,   52.2500,  -65.0000,  -18.6250,    4.0625,   51.2500,    4.1250,    2.0312,   -4.0938,  -82.0000],
        [ -99.0000,  -62.5000,   25.3750, -138.0000,  -14.3125,   -3.5469,   54.5000,   87.5000, -119.0000,   37.0000,  -18.1250,   75.0000,   11.5625,    3.3125,  -78.0000,  -34.0000],
        [ -30.5000,   63.0000,  -92.5000,  -71.0000,   -8.6875,   22.0000,   60.7500,  -12.3750,  -39.7500,   43.2500,    5.5938,  -54.5000,   11.3125,    5.2188, -165.0000,  -76.0000],
        [ -56.2500, -101.5000,   91.5000,  -43.7500,  -50.7500,   32.5000,    9.8125,   36.5000,   16.7500,  -70.5000,   11.6250,   31.7500,   25.1250,    1.8828,  -93.0000,  -17.1250],
        [  49.0000,  -61.2500,  -65.0000,    8.8125,  -76.5000,   32.5000, -125.5000,   33.0000,   64.0000,   40.0000,  -54.2500,  -32.0000,  -29.6250,    3.1406,  -26.5000,  -66.0000],
        [  37.5000,  -56.0000,  -95.0000,   14.1250,  -90.5000,   -8.8750,   55.7500,  -67.0000,   17.8750,  -37.5000,   45.5000,   34.0000,  -18.1250,    2.0781,  -25.6250, -105.5000],
        [ 191.0000,  -53.7500,  -96.0000,   -9.0625,   -7.4375,   26.1250,   93.0000,  -83.5000,  -11.1250,   26.0000,  158.0000,  -92.5000,   -2.7500,   -2.6094, -162.0000,   -1.6016],
        [ -87.0000,   43.5000,   12.1875,   44.5000,  -23.3750,   23.6250,  -77.5000,   64.5000,  -50.5000,   82.5000,   34.2500,   70.5000,    3.9844,   -0.6328,  -32.0000,    2.3125],
        [ -80.5000,  144.0000,  -37.5000, -139.0000,  -51.0000,   -3.0000,  -37.5000,  -24.2500,   -7.4375,   40.5000,   87.5000,   20.0000,    3.3906,    0.4785,   83.0000,  -68.0000]]
name='residual layer 22'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-207.0000,  178.0000,  207.0000,  108.0000,  110.5000,  -85.0000,   47.0000,   78.0000,   57.2500,   -0.7500, -188.0000,  -16.0000,  -35.0000,   29.5000,  218.0000,  -38.0000],
        [-448.0000,  -90.0000,  -18.8750,  124.5000, -126.0000, -152.0000, -177.0000,  -37.7500,  127.5000, -115.0000,  237.0000,  171.0000,   17.3750,    1.2344,  119.0000,  -22.0000],
        [-165.0000,   24.3750, -185.0000, -107.5000, -128.0000,  -97.0000, -346.0000,  189.0000,   98.0000, -226.0000,  528.0000,   63.2500,   39.5000,  -20.1250,  219.0000,  168.0000],
        [-616.0000,  134.0000,  103.5000,  -35.5000,  -95.5000,  -88.5000, -430.0000,  248.0000,   45.0000,  -23.8750,  276.0000,  156.0000,   41.5000,    6.8750,  150.0000,   63.2500],
        [-448.0000, -136.0000, -156.0000,   53.5000, -120.0000,   -5.4375,  197.0000,  282.0000,  -14.3750,   10.5000,  612.0000,   16.8750,   -7.9375,   15.8750,  354.0000,   73.0000],
        [-316.0000, -142.0000,  254.0000, -132.0000, -203.0000, -117.0000,  218.0000,  552.0000,   15.3750,  -17.5000,  704.0000,  308.0000,   61.7500,  -13.5000,  396.0000,  -33.5000],
        [-410.0000,  109.5000,  220.0000,   -7.9375, -296.0000, -144.0000, -191.0000,   13.5000,   65.5000,  -44.0000,  140.0000,  183.0000,  103.5000,    1.0312,  286.0000,   41.7500],
        [ 292.0000,   -1.0625,  172.0000,   46.7500, -202.0000,  -83.0000,   36.5000,  360.0000,  199.0000,  145.0000,  472.0000, -224.0000,   62.7500,  -55.5000,  496.0000,   70.5000],
        [-528.0000,   98.5000,  -74.0000,  -53.7500,  -58.5000,  -35.7500, -242.0000,  186.0000, -112.0000, -168.0000,  442.0000,  -40.0000,  -41.0000,  -25.0000,  211.0000,  -52.7500],
        [-588.0000,  197.0000,  179.0000, -157.0000, -115.0000, -126.5000, -318.0000,  -61.7500,  -19.3750,   32.0000,  408.0000,   -5.7500,   53.0000,   -3.7344,  101.5000, -121.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.3828,  0.9375,  0.4082,  0.5742,  0.4531, -0.5898,  0.2715,  0.0962,  0.4434,  0.1133, -0.7891, -0.1206, -0.0547,  0.4414,  0.8945, -0.0708],
        [-1.2266, -0.3086, -0.0099,  0.3477, -0.6406, -0.8945, -0.5508,  0.0464,  0.2451, -0.4082,  0.6445,  0.6797,  0.1533,  0.0508,  0.2871, -0.3164],
        [-0.5742, -0.1279, -0.4062, -0.6172, -0.5195, -0.5977, -0.7461,  0.8359, -0.0776, -0.5430,  1.2812,  0.3984,  0.3438, -0.2461,  0.3320,  0.3809],
        [-1.4062,  0.6641,  0.0280, -0.2676, -0.3789, -0.3945, -0.9453,  0.7070,  0.0193,  0.0557,  0.7070,  0.2910,  0.3535,  0.1768, -0.0352, -0.0364],
        [-1.3906, -1.0156, -0.2070,  0.0311, -0.7891,  0.2021,  0.6719,  1.2188,  0.0111, -0.2178,  1.9922,  0.1768,  0.1465,  0.3281,  0.7773,  0.2021],
        [-0.6914, -0.8203,  0.5703, -0.3711, -1.2109, -0.5977,  0.2832,  2.0938,  0.3496,  0.0771,  1.9531,  0.9453,  0.2578, -0.1797,  1.0312, -0.3398],
        [-1.0938,  0.2451,  0.4297,  0.0212, -1.9062, -1.2344, -0.4727, -0.2188,  0.4180, -0.3184,  0.6367,  0.8438,  0.7812,  0.0615,  0.8320, -0.2471],
        [ 1.2891, -0.2275,  0.2363,  0.1162, -0.9414, -0.4141,  0.4082,  1.0234,  0.8516,  0.6016,  1.9531, -1.1172,  0.4941, -1.0391,  0.9609,  0.2422],
        [-1.6094,  0.5742, -0.1875, -0.0281, -0.3574, -0.0864, -0.9844,  0.9062, -0.7227, -0.2949,  1.4453,  0.1050, -0.2988, -0.4492,  0.5039, -0.1729],
        [-1.4766,  1.1797,  0.3672, -0.7617, -0.6172, -0.7812, -0.9336, -0.2637, -0.1016,  0.2129,  1.2734,  0.0417,  0.3867, -0.0486,  0.4434, -0.5508]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-158.0000,  248.0000,  144.0000,  202.0000,  111.5000,  -89.0000,   94.5000,   28.5000,  107.0000,   35.2500, -280.0000,  -37.5000,   -7.2500,   27.1250,  340.0000,  -22.2500],
        [-532.0000,  -86.0000,   -3.6875,  129.0000, -166.0000, -142.0000, -202.0000,   14.5000,   62.5000, -134.0000,  241.0000,  222.0000,   21.5000,    3.2656,  115.0000, -104.0000],
        [-264.0000,  -38.0000, -160.0000, -246.0000, -142.0000, -100.5000, -292.0000,  276.0000,  -21.0000, -189.0000,  510.0000,  138.0000,   51.0000,  -16.7500,  141.0000,  134.0000],
        [-648.0000,  197.0000,   11.0000, -106.5000, -104.0000,  -66.5000, -370.0000,  236.0000,    5.2500,   19.3750,  282.0000,  101.5000,   52.7500,   12.1250,  -15.0000,  -12.7500],
        [-504.0000, -238.0000,  -64.5000,    9.7500, -171.0000,   27.0000,  207.0000,  318.0000,    2.3750,  -60.0000,  624.0000,   48.5000,   17.2500,   17.7500,  260.0000,   56.0000],
        [-268.0000, -203.0000,  189.0000, -123.0000, -280.0000,  -84.5000,   92.5000,  584.0000,   79.5000,   22.5000,  648.0000,  276.0000,   32.0000,  -10.3750,  370.0000,  -99.5000],
        [-372.0000,   53.5000,  125.0000,    6.1875, -386.0000, -153.0000, -135.0000,  -53.5000,   83.5000,  -81.5000,  186.0000,  217.0000,   85.5000,    3.1094,  260.0000,  -63.7500],
        [ 484.0000,  -54.7500,   76.0000,   37.7500, -209.0000,  -57.0000,  130.0000,  276.0000,  188.0000,  171.0000,  632.0000, -316.0000,   60.0000,  -58.0000,  334.0000,   69.0000],
        [-616.0000,  142.0000,  -61.7500,   -9.2500,  -82.0000,  -12.1250, -320.0000,  250.0000, -162.0000,  -85.5000,  476.0000,   30.5000,  -37.0000,  -25.6250,  179.0000,  -50.5000],
        [-668.0000,  340.0000,  142.0000, -296.0000, -166.0000, -130.0000, -356.0000,  -86.0000,  -26.7500,   72.5000,  496.0000,   14.2500,   56.5000,   -3.2500,  184.0000, -189.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[ 11.7500,  -1.5156, -10.1875, -25.6250, -18.3750, -10.6250,  -1.5469,  -7.5312, -10.8750,   2.9531,  -7.8125,  -3.3750,  14.3125,  -8.8750, -13.3125,  -9.1875],
        [  1.5859,   2.1406,   0.0938, -25.5000, -19.7500, -25.2500,  33.2500,  10.1250,  -7.4062,  -9.3750,  -0.6367,   2.1406,  21.1250,  -4.1875, -15.1875, -17.5000],
        [  0.5781,  19.3750,  -0.3262, -16.2500, -17.1250, -23.3750,  14.6250,   7.3750,  -8.3125,  -4.2500,  17.3750,  -6.6562,  16.7500,  -2.1875,  14.6250, -16.8750],
        [ -9.4375,  13.9375, -14.0625, -25.2500, -19.1250, -25.5000, -10.6250,   2.9844,  -3.5938,   1.1328,   7.1875,  15.3125,  25.3750,  -7.7500,   9.5625, -10.9375],
        [ -9.3750,   7.8750,  -5.0938, -26.3750, -32.0000, -23.0000, -17.1250,  44.5000,  12.4375,  13.8125,  -0.6367,  48.5000,  10.1875,  -9.6875,  11.2500, -28.7500],
        [ -4.7812, -31.8750, -10.2500, -49.2500, -51.0000, -44.0000,   3.9062,  -7.5625, -17.7500,  -6.0000,  39.5000,  27.7500,   7.4688, -14.7500,  21.1250, -16.5000],
        [-53.7500, -11.6875,  31.3750,  16.1250,   4.9062, -31.5000,   7.3438,   1.6094,  30.7500,  11.5625, -25.5000,  19.1250,  20.1250,  -4.6562, -52.0000,  -8.8125],
        [-76.5000,  13.2500, -24.8750,   7.8438, -37.5000, -19.5000, -29.1250, -42.2500,  14.3750, -39.0000, -12.1250,   5.3750,   9.1875, -10.6250, -19.1250, -29.1250],
        [-33.5000, -42.2500,  -2.5781, -37.5000, -30.3750, -29.5000, -77.5000,  23.2500, -44.2500,  12.7500,   5.3125,  14.9375,  11.1875, -13.9375,  23.2500, -46.0000],
        [  9.1875,  -6.5312,  11.3125, -29.5000, -52.2500, -38.7500, -30.0000,  10.2500, -23.5000,  17.3750, -16.2500,  37.2500,  22.8750, -13.5000,  44.7500, -24.2500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-0.4199,  1.1797,  0.5820,  0.7695,  0.5469, -0.7500,  0.3359,  0.1016,  0.5703,  0.2021, -0.8984, -0.1816,  0.0579,  0.3301,  1.3125, -0.1719],
        [-1.3828, -0.3652, -0.0142,  0.4102, -0.9922, -1.1484, -0.5547,  0.1084,  0.2969, -0.6875,  0.6797,  0.9062,  0.3184, -0.0151,  0.3652, -0.6055],
        [-0.6289, -0.0747, -0.5820, -0.9570, -0.7773, -0.7812, -0.8398,  1.1484, -0.1455, -0.8555,  1.3672,  0.4863,  0.4629, -0.2852,  0.5234,  0.5352],
        [-1.7969,  0.9648, -0.0126, -0.5469, -0.6914, -0.6641, -1.3125,  1.1016,  0.0094,  0.1030,  0.8594,  0.4941,  0.6133,  0.0752, -0.0209, -0.1235],
        [-1.5703, -1.1797, -0.3242, -0.0771, -1.2734,  0.0325,  0.7305,  1.8750,  0.0947, -0.2617,  2.0781,  0.4590,  0.2412,  0.1562,  1.1719,  0.1602],
        [-0.7812, -1.1328,  0.7773, -0.7500, -1.9453, -0.9727,  0.3496,  2.7969,  0.3691,  0.0879,  2.1562,  1.3516,  0.3242, -0.4531,  1.5781, -0.6367],
        [-1.3203,  0.2178,  0.7344,  0.1050, -2.4375, -1.5078, -0.5000, -0.2715,  0.7383, -0.4004,  0.5430,  1.1328,  0.9414, -0.0303,  0.9102, -0.4297],
        [ 1.3828, -0.2354,  0.2617,  0.2363, -1.7109, -0.6836,  0.4316,  1.3359,  1.4297,  0.8242,  2.2812, -1.6250,  0.6719, -1.4688,  1.5000,  0.2598],
        [-1.6953,  0.4395, -0.2559, -0.1865, -0.6055, -0.2871, -1.3203,  1.2109, -1.1172, -0.3535,  1.3672,  0.1846, -0.1943, -0.6523,  0.7461, -0.4844],
        [-1.5938,  1.3594,  0.5664, -1.2031, -1.0859, -1.0859, -1.1875, -0.3125, -0.2539,  0.4023,  1.2656,  0.1943,  0.5547, -0.2559,  0.7812, -0.9922]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-146.0000,  246.0000,  134.0000,  176.0000,   93.0000,  -99.5000,   93.0000,   21.0000,   96.0000,   38.2500, -288.0000,  -41.0000,    7.0625,   18.2500,  326.0000,  -31.5000],
        [-532.0000,  -84.0000,   -3.5938,  103.5000, -186.0000, -167.0000, -169.0000,   24.6250,   55.0000, -143.0000,  240.0000,  224.0000,   42.5000,   -0.9219,  100.0000, -121.5000],
        [-264.0000,  -18.6250, -160.0000, -262.0000, -159.0000, -124.0000, -278.0000,  284.0000,  -29.2500, -193.0000,  528.0000,  131.0000,   68.0000,  -19.0000,  156.0000,  117.0000],
        [-656.0000,  211.0000,   -3.0625, -132.0000, -123.0000,  -92.0000, -380.0000,  239.0000,    1.6562,   20.5000,  290.0000,  117.0000,   78.0000,    4.3750,   -5.4375,  -23.7500],
        [-512.0000, -230.0000,  -69.5000,  -16.6250, -203.0000,    4.0000,  190.0000,  362.0000,   14.8125,  -46.2500,  624.0000,   97.0000,   27.5000,    8.0625,  272.0000,   27.2500],
        [-272.0000, -235.0000,  179.0000, -172.0000, -332.0000, -128.0000,   96.5000,  576.0000,   61.7500,   16.5000,  688.0000,  304.0000,   39.5000,  -25.1250,  392.0000, -116.0000],
        [-426.0000,   41.7500,  156.0000,   22.2500, -382.0000, -184.0000, -127.5000,  -52.0000,  114.0000,  -70.0000,  160.0000,  236.0000,  105.5000,   -1.5469,  208.0000,  -72.5000],
        [ 408.0000,  -41.5000,   51.0000,   45.5000, -246.0000,  -76.5000,  101.0000,  234.0000,  202.0000,  132.0000,  620.0000, -310.0000,   69.0000,  -68.5000,  314.0000,   40.0000],
        [-648.0000,  100.0000,  -64.5000,  -46.7500, -112.5000,  -41.5000, -398.0000,  274.0000, -206.0000,  -73.0000,  482.0000,   45.5000,  -25.7500,  -39.5000,  202.0000,  -96.5000],
        [-660.0000,  334.0000,  153.0000, -326.0000, -218.0000, -169.0000, -386.0000,  -76.0000,  -50.2500,   90.0000,  480.0000,   51.5000,   79.5000,  -16.7500,  229.0000, -213.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -7.0625,   48.7500,   10.7500,  -23.2500,  -17.6250,   -9.8750,    5.2812,  -87.5000,  -31.1250,   20.7500,   19.1250,   36.5000,  -18.0000,  -29.0000,    8.2500,   -1.0625],
        [  -5.6250,   38.7500,  -29.8750,  -18.2500,   19.0000,  -16.7500,   11.3750,   38.0000,  -16.2500,  -37.2500,  -22.7500,   17.3750,    2.7656,   -9.0625,  -14.2500,  -16.3750],
        [ -33.0000,  -33.0000,  -23.5000,  -69.0000,   42.2500,   13.0625,   16.7500,   54.7500,   24.6250,  -42.2500,   39.7500,  -62.7500,   73.0000, -130.0000,  -31.7500,  -45.7500],
        [  19.1250, -129.0000,  -79.5000,   31.2500,  -89.0000,   23.5000,   96.5000, -142.0000, -194.0000,  -91.5000,   74.5000, -219.0000,  -50.0000,  -67.5000,  -44.2500,  -94.5000],
        [ -79.5000,  -74.5000,  -10.3750,   15.6875,   -4.5938,   52.7500,   39.0000,   84.5000,   58.2500, -112.5000,  -39.5000,   14.8750,  -85.5000,   -1.6797,  -16.0000,  -28.0000],
        [ -13.8750,   57.7500,  -17.7500,  -43.5000,  139.0000,   35.0000,  -32.5000,  -26.7500,   33.7500,   10.3125,    5.4688,  -52.0000, -141.0000,   72.0000,   -8.8750,   -0.9609],
        [ -46.2500,  -33.0000,   -1.4453,  -58.7500,  113.5000,   24.2500,  -46.0000,  131.0000,  -17.7500, -142.0000,  -35.2500, -133.0000, -110.0000,   48.7500,   77.5000,   73.0000],
        [ -38.2500, -244.0000,  166.0000, -211.0000,  -92.0000,  -86.5000,  185.0000,  -26.7500, -193.0000,    9.5625,    7.9375,  292.0000,  -79.0000, -180.0000,  -60.0000,   41.5000],
        [ -50.5000,  -29.2500,   14.0625,   15.8750,   25.7500,   -1.0234,  -60.5000,  -29.1250,  -56.7500,   -9.3125,   -8.9375,  -79.0000,  -64.5000,  -19.7500,   25.0000,  -58.0000],
        [  -8.5000,  -83.0000,  -27.0000,  -19.0000,  -57.5000,  -28.7500,   18.1250,   66.5000,    4.5938, -126.5000,    0.8086,  -93.0000,  -55.0000,  -19.2500,  -46.7500,   -8.8750]]
-------------------------
name='positions layer 23'      | dtype=torch.int64     | shape=(10,)                | 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
name='x layer 23'              | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[  -7.0625,   48.7500,   10.7500,  -23.2500,  -17.6250,   -9.8750,    5.2812,  -87.5000,  -31.1250,   20.7500,   19.1250,   36.5000,  -18.0000,  -29.0000,    8.2500,   -1.0625],
        [  -5.6250,   38.7500,  -29.8750,  -18.2500,   19.0000,  -16.7500,   11.3750,   38.0000,  -16.2500,  -37.2500,  -22.7500,   17.3750,    2.7656,   -9.0625,  -14.2500,  -16.3750],
        [ -33.0000,  -33.0000,  -23.5000,  -69.0000,   42.2500,   13.0625,   16.7500,   54.7500,   24.6250,  -42.2500,   39.7500,  -62.7500,   73.0000, -130.0000,  -31.7500,  -45.7500],
        [  19.1250, -129.0000,  -79.5000,   31.2500,  -89.0000,   23.5000,   96.5000, -142.0000, -194.0000,  -91.5000,   74.5000, -219.0000,  -50.0000,  -67.5000,  -44.2500,  -94.5000],
        [ -79.5000,  -74.5000,  -10.3750,   15.6875,   -4.5938,   52.7500,   39.0000,   84.5000,   58.2500, -112.5000,  -39.5000,   14.8750,  -85.5000,   -1.6797,  -16.0000,  -28.0000],
        [ -13.8750,   57.7500,  -17.7500,  -43.5000,  139.0000,   35.0000,  -32.5000,  -26.7500,   33.7500,   10.3125,    5.4688,  -52.0000, -141.0000,   72.0000,   -8.8750,   -0.9609],
        [ -46.2500,  -33.0000,   -1.4453,  -58.7500,  113.5000,   24.2500,  -46.0000,  131.0000,  -17.7500, -142.0000,  -35.2500, -133.0000, -110.0000,   48.7500,   77.5000,   73.0000],
        [ -38.2500, -244.0000,  166.0000, -211.0000,  -92.0000,  -86.5000,  185.0000,  -26.7500, -193.0000,    9.5625,    7.9375,  292.0000,  -79.0000, -180.0000,  -60.0000,   41.5000],
        [ -50.5000,  -29.2500,   14.0625,   15.8750,   25.7500,   -1.0234,  -60.5000,  -29.1250,  -56.7500,   -9.3125,   -8.9375,  -79.0000,  -64.5000,  -19.7500,   25.0000,  -58.0000],
        [  -8.5000,  -83.0000,  -27.0000,  -19.0000,  -57.5000,  -28.7500,   18.1250,   66.5000,    4.5938, -126.5000,    0.8086,  -93.0000,  -55.0000,  -19.2500,  -46.7500,   -8.8750]]
name='residual layer 23'       | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[-146.0000,  246.0000,  134.0000,  176.0000,   93.0000,  -99.5000,   93.0000,   21.0000,   96.0000,   38.2500, -288.0000,  -41.0000,    7.0625,   18.2500,  326.0000,  -31.5000],
        [-532.0000,  -84.0000,   -3.5938,  103.5000, -186.0000, -167.0000, -169.0000,   24.6250,   55.0000, -143.0000,  240.0000,  224.0000,   42.5000,   -0.9219,  100.0000, -121.5000],
        [-264.0000,  -18.6250, -160.0000, -262.0000, -159.0000, -124.0000, -278.0000,  284.0000,  -29.2500, -193.0000,  528.0000,  131.0000,   68.0000,  -19.0000,  156.0000,  117.0000],
        [-656.0000,  211.0000,   -3.0625, -132.0000, -123.0000,  -92.0000, -380.0000,  239.0000,    1.6562,   20.5000,  290.0000,  117.0000,   78.0000,    4.3750,   -5.4375,  -23.7500],
        [-512.0000, -230.0000,  -69.5000,  -16.6250, -203.0000,    4.0000,  190.0000,  362.0000,   14.8125,  -46.2500,  624.0000,   97.0000,   27.5000,    8.0625,  272.0000,   27.2500],
        [-272.0000, -235.0000,  179.0000, -172.0000, -332.0000, -128.0000,   96.5000,  576.0000,   61.7500,   16.5000,  688.0000,  304.0000,   39.5000,  -25.1250,  392.0000, -116.0000],
        [-426.0000,   41.7500,  156.0000,   22.2500, -382.0000, -184.0000, -127.5000,  -52.0000,  114.0000,  -70.0000,  160.0000,  236.0000,  105.5000,   -1.5469,  208.0000,  -72.5000],
        [ 408.0000,  -41.5000,   51.0000,   45.5000, -246.0000,  -76.5000,  101.0000,  234.0000,  202.0000,  132.0000,  620.0000, -310.0000,   69.0000,  -68.5000,  314.0000,   40.0000],
        [-648.0000,  100.0000,  -64.5000,  -46.7500, -112.5000,  -41.5000, -398.0000,  274.0000, -206.0000,  -73.0000,  482.0000,   45.5000,  -25.7500,  -39.5000,  202.0000,  -96.5000],
        [-660.0000,  334.0000,  153.0000, -326.0000, -218.0000, -169.0000, -386.0000,  -76.0000,  -50.2500,   90.0000,  480.0000,   51.5000,   79.5000,  -16.7500,  229.0000, -213.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
name='x after forward'         | dtype=torch.bfloat16  | shape=(10, 2880)           | 
[[    -4.5312,      8.2500,      3.8438,      4.3438,      2.3125,     -4.6875,      3.0000,     -1.8125,      2.3125,      1.9062,     -8.8750,     -0.1279,     -0.4160,     -0.5000,     10.6875,     -1.0859],
        [   -13.4375,     -1.0703,     -0.7578,      2.0625,     -4.3438,     -6.6875,     -4.0938,      1.4453,      1.1797,     -4.9375,      6.0938,      5.8438,      1.4531,     -0.3945,      2.3281,     -3.9062],
        [    -6.2500,     -1.0312,     -3.4844,     -6.7188,     -2.5625,     -3.3906,     -5.6875,      6.5938,     -0.1187,     -5.4062,     13.3750,      1.3828,      3.8438,     -4.9688,      2.8438,      1.6953],
        [   -14.6875,      1.7891,     -1.7266,     -2.2500,     -5.0938,     -2.2969,     -6.7812,      2.0625,     -5.3750,     -1.7891,      9.3750,     -2.2656,      0.8320,     -2.3125,     -1.2422,     -3.0938],
        [   -15.3125,     -7.4375,     -1.8594,     -0.0232,     -5.5625,      2.1250,      6.1250,     10.6250,      2.2812,     -4.4688,     16.7500,      2.7969,     -1.9297,      0.2598,      7.1562,     -0.0219],
        [    -7.4375,     -4.3750,      3.7656,     -5.3750,     -5.2188,     -3.5156,      1.7188,     13.1875,      3.0156,      0.7617,     20.1250,      6.3125,     -3.3906,      1.9219,     10.8125,     -3.4375],
        [    -8.2500,      0.1455,      2.4375,     -0.6211,     -4.9062,     -4.0625,     -3.1406,      1.2812,      2.0469,     -4.0625,      2.4375,      1.7422,     -0.1016,      1.3125,      5.4375,      0.0099],
        [     7.4062,     -5.4062,      3.9219,     -3.2031,     -7.0312,     -4.7500,      5.9375,      3.8281,      0.2197,      3.0938,     14.0625,     -0.3496,     -0.2578,     -7.9062,      5.5312,      1.8516],
        [   -15.8750,      1.5312,     -1.0312,     -0.6797,     -2.0469,     -1.3984,    -10.8125,      5.1562,     -7.2812,     -2.0469,     12.0000,     -0.7344,     -2.6562,     -2.1250,      5.6250,     -4.0000],
        [   -14.4375,      5.1250,      2.4531,     -7.2188,     -6.1875,     -6.1875,     -8.1875,     -0.1904,     -1.1953,     -0.8594,     11.5625,     -0.8672,      0.6836,     -1.2266,      4.2812,     -5.4375]]
----------------------------------------------------------------------------------------------------
Compute logits:: hidden_states.shape = torch.Size([10, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([9], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
name='[COMPUTE_LOGITS] hidden states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-14.4375,   5.1250,   2.4531,  -7.2188,  -6.1875,  -6.1875,  -8.1875,  -0.1904,  -1.1953,  -0.8594,  11.5625,  -0.8672,   0.6836,  -1.2266,   4.2812,  -5.4375]]
----------------------------------------------------------------------------------------------------
name='[LOGITS] BEFORE self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-14.4375,   5.1250,   2.4531,  -7.2188,  -6.1875,  -6.1875,  -8.1875,  -0.1904,  -1.1953,  -0.8594,  11.5625,  -0.8672,   0.6836,  -1.2266,   4.2812,  -5.4375]]
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] x' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-14.4375,   5.1250,   2.4531,  -7.2188,  -6.1875,  -6.1875,  -8.1875,  -0.1904,  -1.1953,  -0.8594,  11.5625,  -0.8672,   0.6836,  -1.2266,   4.2812,  -5.4375]]
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
name='[UnquantizedEmbeddingMethod] out' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------
name='[LOGITS] AFTER lm_head.quant_method.apply' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
name='[LOGITS] AFTER self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------
name='logits'                  | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
name='input_ids'               | dtype=torch.int32     | shape=(1,)                 | val = 0
name='input_embeds'            | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2988, -0.6953,  1.9141, -0.0649,  0.7461, -1.2031, -0.1865,  0.1436, -6.0000,  1.9844, -0.0967,  0.8711,  2.7969,  6.2812, -0.1177,  0.3008]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1514, -0.3164,  0.7500, -0.0874,  0.3145, -0.2969, -0.1357,  0.1069, -1.7891,  0.7383, -0.1523,  0.4590,  0.9961,  1.6094, -0.1602,  0.2598]]
[after   INPUT_LAYER_NORM] residual is None
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0347, -0.2080, -0.1133,  0.0315, -0.7070,  0.4199,  0.0229, -0.2012,  0.1758, -0.4941, -0.0178, -0.1162, -0.1836,  0.4629, -0.0299, -0.3789]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1699, -0.4434,  0.8086, -0.0659,  0.0153, -0.1216, -0.1895, -0.0649, -0.8555,  0.4844, -0.2412,  0.5273,  0.4395,  0.3711, -0.2598, -0.0742]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2637, -0.9023,  1.7969, -0.0334,  0.0391, -0.7812, -0.1641, -0.0576, -5.8125,  1.4922, -0.1143,  0.7539,  2.6094,  6.7500, -0.1475, -0.0781]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4277,  0.6836, -0.4023,  0.2988, -0.1396,  1.5547,  0.0442, -0.2930, -0.2139, -1.0312,  0.2100, -0.1709,  0.7578,  0.4766,  0.1211,  0.6680]]
-------------------------
name='positions layer 0'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 0'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4277,  0.6836, -0.4023,  0.2988, -0.1396,  1.5547,  0.0442, -0.2930, -0.2139, -1.0312,  0.2100, -0.1709,  0.7578,  0.4766,  0.1211,  0.6680]]
name='residual layer 0'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2637, -0.9023,  1.7969, -0.0334,  0.0391, -0.7812, -0.1641, -0.0576, -5.8125,  1.4922, -0.1143,  0.7539,  2.6094,  6.7500, -0.1475, -0.0781]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4082, -0.1128,  0.6016,  0.2676, -0.0381,  0.1846, -0.0903, -0.2598, -2.0000,  0.1738,  0.1021,  0.3418,  1.2344,  2.2344, -0.0239,  0.3574]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6914, -0.2188,  1.3906,  0.2656, -0.1006,  0.7734, -0.1201, -0.3516, -6.0312,  0.4609,  0.0957,  0.5820,  3.3750,  7.2188, -0.0264,  0.5898]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0723,  0.0972,  0.0045, -0.0039,  0.0981, -0.1221,  0.1357, -0.0383,  1.9609, -0.6445, -0.0167,  0.2812, -1.7266,  0.6602,  0.2617,  0.3379]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5000, -0.0708,  0.7812,  0.4219, -0.0011,  0.0957,  0.0194, -0.4395, -0.6289, -0.0645,  0.1260,  0.6758,  0.2227,  0.4492,  0.3105,  0.5859]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.6172,     -0.1216,      1.3984,      0.2617,     -0.0024,      0.6523,      0.0156,     -0.3906,     -4.0625,     -0.1836,      0.0791,      0.8633,      1.6484,      7.8750,      0.2354,      0.9297]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3867, -0.2676, -0.4648, -0.1465,  0.3652, -0.0757, -0.1338, -0.1299, -3.7344, -1.8906,  0.3457, -0.5664,  3.1094, -3.1562, -0.6875,  1.0469]]
-------------------------
name='positions layer 1'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 1'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3867, -0.2676, -0.4648, -0.1465,  0.3652, -0.0757, -0.1338, -0.1299, -3.7344, -1.8906,  0.3457, -0.5664,  3.1094, -3.1562, -0.6875,  1.0469]]
name='residual layer 1'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.6172,     -0.1216,      1.3984,      0.2617,     -0.0024,      0.6523,      0.0156,     -0.3906,     -4.0625,     -0.1836,      0.0791,      0.8633,      1.6484,      7.8750,      0.2354,      0.9297]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5977, -0.1904,  0.4199,  0.0879,  0.1377,  0.1445, -0.0864, -0.3340, -2.2656, -0.7578,  0.3496,  0.1660,  1.6094,  1.2188, -0.3320,  0.8750]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.0000, -0.3887,  0.9336,  0.1152,  0.3633,  0.5781, -0.1182, -0.5195, -7.8125, -2.0781,  0.4258,  0.2969,  4.7500,  4.7188, -0.4531,  1.9766]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4199,  0.3867,  0.2539, -0.2344,  0.3340,  0.5234,  0.0166, -0.3145, -1.0547, -0.7344, -0.2100,  0.0801,  0.8867,  0.7969,  0.1074, -0.1973]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.2188,     -0.0010,      0.7383,     -0.1367,      0.2930,      0.1416,     -0.1143,     -0.7734,     -1.3438,     -0.9453,      0.2432,      0.2852,      0.5352,      0.3008,     -0.3281,      0.7539]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.4219,     -0.0020,      1.1875,     -0.1191,      0.6953,      1.1016,     -0.1016,     -0.8359,     -8.8750,     -2.8125,      0.2158,      0.3770,      5.6250,      5.5000,     -0.3457,      1.7812]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0493,  0.6719,  0.3438,  0.4102, -0.1021, -1.8672, -0.2812, -0.1475, -1.7188, -0.3945, -0.6602, -0.1973,  3.6875,  0.4121,  1.0234, -0.8008]]
-------------------------
name='positions layer 2'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 2'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0493,  0.6719,  0.3438,  0.4102, -0.1021, -1.8672, -0.2812, -0.1475, -1.7188, -0.3945, -0.6602, -0.1973,  3.6875,  0.4121,  1.0234, -0.8008]]
name='residual layer 2'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.4219,     -0.0020,      1.1875,     -0.1191,      0.6953,      1.1016,     -0.1016,     -0.8359,     -8.8750,     -2.8125,      0.2158,      0.3770,      5.6250,      5.5000,     -0.3457,      1.7812]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6172,  0.2305,  0.5820,  0.1484,  0.1553, -0.1309, -0.2119, -0.4453, -2.0312, -0.8008, -0.2236,  0.0723,  1.7422,  1.0078,  0.2969,  0.2480]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.3750,   0.6719,   1.5312,   0.2910,   0.5938,  -0.7656,  -0.3828,  -0.9844, -10.6250,  -3.2031,  -0.4453,   0.1797,   9.3125,   5.9062,   0.6797,   0.9805]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0317,  0.1836, -0.1084, -0.3867,  0.5391,  0.5039,  0.1973,  0.0211, -0.2129, -0.6797, -0.5156, -0.0583,  2.9219,  0.7617,  0.1787, -0.2490]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9766,  0.3926,  0.8398, -0.0791,  0.3926, -0.0258, -0.1670, -0.6602, -1.2578, -1.0781, -0.7461,  0.0781,  0.7656,  0.3730,  0.5547,  0.2188]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.3438,   0.8555,   1.4219,  -0.0957,   1.1328,  -0.2617,  -0.1855,  -0.9648, -10.8125,  -3.8750,  -0.9609,   0.1211,  12.2500,   6.6562,   0.8594,   0.7305]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4375, -0.1162, -0.4980,  0.1226,  0.5078, -3.9062,  0.1201, -0.2969, -1.2969, -1.3594,  0.2451, -0.0114,  2.4844,  2.7500,  0.2598, -0.0981]]
-------------------------
name='positions layer 3'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 3'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4375, -0.1162, -0.4980,  0.1226,  0.5078, -3.9062,  0.1201, -0.2969, -1.2969, -1.3594,  0.2451, -0.0114,  2.4844,  2.7500,  0.2598, -0.0981]]
name='residual layer 3'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.3438,   0.8555,   1.4219,  -0.0957,   1.1328,  -0.2617,  -0.1855,  -0.9648, -10.8125,  -3.8750,  -0.9609,   0.1211,  12.2500,   6.6562,   0.8594,   0.7305]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3965,  0.2617,  0.3105,  0.0115,  0.4180, -0.6953, -0.0308, -0.5273, -2.7344, -1.2812, -0.3164,  0.0417,  3.7500,  2.1875,  0.4395,  0.1582]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.9062,   0.7383,   0.9219,   0.0269,   1.6406,  -4.1562,  -0.0654,  -1.2656, -12.1250,  -5.2500,  -0.7148,   0.1099,  14.7500,   9.3750,   1.1172,   0.6328]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2578, -0.5391,  0.1963,  0.2031, -1.6094, -2.2656, -0.1406, -0.0337,  1.7266, -1.7109, -0.3301, -0.7852,  1.6484,  0.0967,  0.6445, -0.7109]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.8281,  0.0815,  0.6836,  0.1602,  0.0096, -0.5703, -0.1709, -0.7852, -1.3828, -1.7891, -0.6992, -0.4062,  1.1172,  0.5742,  0.9961, -0.0188]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.1641,   0.1992,   1.1172,   0.2305,   0.0312,  -6.4375,  -0.2061,  -1.2969, -10.3750,  -6.9688,  -1.0469,  -0.6758,  16.3750,   9.5000,   1.7656,  -0.0781]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4082, -0.1895,  1.1172, -0.6758, -0.9062,  3.9844,  0.0356,  0.8672, -5.4688,  2.2500,  0.7109,  0.1807,  2.6719,  0.9531, -0.9570, -1.1875]]
-------------------------
name='positions layer 4'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 4'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4082, -0.1895,  1.1172, -0.6758, -0.9062,  3.9844,  0.0356,  0.8672, -5.4688,  2.2500,  0.7109,  0.1807,  2.6719,  0.9531, -0.9570, -1.1875]]
name='residual layer 4'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.1641,   0.1992,   1.1172,   0.2305,   0.0312,  -6.4375,  -0.2061,  -1.2969, -10.3750,  -6.9688,  -1.0469,  -0.6758,  16.3750,   9.5000,   1.7656,  -0.0781]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.2285,      0.0023,      0.5742,     -0.1226,     -0.1562,     -0.3398,     -0.0559,     -0.1104,     -2.4688,     -0.8398,     -0.0942,     -0.1299,      3.0312,      1.2734,      0.1992,     -0.2080]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.7578,      0.0098,      2.2344,     -0.4453,     -0.8750,     -2.4531,     -0.1699,     -0.4297,    -15.8750,     -4.7188,     -0.3359,     -0.4961,     19.0000,     10.4375,      0.8086,     -1.2656]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0996,  0.4414, -0.1143, -0.1089, -0.7266, -0.8477,  0.3340, -0.1260,  0.4727,  0.9023,  0.2734, -0.0613,  1.1172,  0.4844,  0.5391,  0.1162]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4004,  0.1216,  0.8633, -0.2373, -0.3027, -0.1865,  0.0874, -0.2080, -1.3438, -0.6406, -0.0255, -0.2168,  0.9375,  0.6406,  0.4668, -0.1689]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.8594,   0.4512,   2.1250,  -0.5547,  -1.6016,  -3.2969,   0.1641,  -0.5547, -15.3750,  -3.8125,  -0.0625,  -0.5586,  20.1250,  10.9375,   1.3438,  -1.1484]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.9336,  0.3066,  0.0222, -0.3223,  2.6875,  2.6250, -0.6328,  0.9766, -1.2734, -2.8750, -0.0933,  0.5117, -1.2422,  0.0121,  0.2793, -1.1875]]
-------------------------
name='positions layer 5'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 5'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.9336,  0.3066,  0.0222, -0.3223,  2.6875,  2.6250, -0.6328,  0.9766, -1.2734, -2.8750, -0.0933,  0.5117, -1.2422,  0.0121,  0.2793, -1.1875]]
name='residual layer 5'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.8594,   0.4512,   2.1250,  -0.5547,  -1.6016,  -3.2969,   0.1641,  -0.5547, -15.3750,  -3.8125,  -0.0625,  -0.5586,  20.1250,  10.9375,   1.3438,  -1.1484]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0206,  0.1621,  0.5078, -0.2158,  0.1670, -0.0835, -0.1377,  0.0986, -2.7812, -1.0156, -0.0408, -0.0114,  3.4062,  1.7422,  0.3574, -0.3418]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.0742,   0.7578,   2.1406,  -0.8750,   1.0859,  -0.6719,  -0.4688,   0.4219, -16.6250,  -6.6875,  -0.1562,  -0.0469,  18.8750,  10.9375,   1.6250,  -2.3438]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5781, -1.6094, -0.7734,  0.8008, -1.4844, -3.0469,  0.2236, -0.8711,  2.7031, -0.2539,  0.7578,  0.4922,  6.8750,  0.2461,  1.2344, -0.6719]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1934, -0.1973,  0.4883, -0.0251, -0.0598, -0.1943, -0.1060, -0.1348, -1.2812, -1.0000,  0.1943,  0.1416,  1.4062,  0.7695,  0.7930, -0.3730]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.5039,  -0.8516,   1.3672,  -0.0742,  -0.3984,  -3.7188,  -0.2451,  -0.4492, -13.9375,  -6.9375,   0.6016,   0.4453,  25.7500,  11.1875,   2.8594,  -3.0156]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.8281, -0.7969, -0.4023,  0.0933,  0.9219, -1.3906, -1.0312,  0.2197, -6.3750, -4.9688,  0.6992,  1.4141, 10.8125,  2.3906,  0.1992,  1.9844]]
-------------------------
name='positions layer 6'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 6'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.8281, -0.7969, -0.4023,  0.0933,  0.9219, -1.3906, -1.0312,  0.2197, -6.3750, -4.9688,  0.6992,  1.4141, 10.8125,  2.3906,  0.1992,  1.9844]]
name='residual layer 6'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.5039,  -0.8516,   1.3672,  -0.0742,  -0.3984,  -3.7188,  -0.2451,  -0.4492, -13.9375,  -6.9375,   0.6016,   0.4453,  25.7500,  11.1875,   2.8594,  -3.0156]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.5781,     -0.2246,      0.1455,      0.0027,      0.0581,     -0.5156,     -0.2422,     -0.0325,     -2.0625,     -1.2031,      0.2158,      0.3027,      4.9062,      1.8828,      0.4238,     -0.0947]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -3.3281,     -1.6484,      0.9648,      0.0190,      0.5234,     -5.1250,     -1.2734,     -0.2295,    -20.2500,    -11.8750,      1.2969,      1.8594,     36.5000,     13.5625,      3.0625,     -1.0312]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3730,  0.6094, -0.7812, -0.0525,  1.7812,  0.5039,  0.7734,  0.1934, -2.0469, -1.1094,  0.1494, -1.0859,  1.6875, -0.1162, -0.3301,  2.5312]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9531, -0.1660,  0.0447, -0.0074,  0.2236, -0.1738, -0.1455, -0.0071, -1.5547, -1.2812,  0.3066,  0.1621,  1.8438,  0.8711,  0.4941,  0.1279]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -3.7031,     -1.0391,      0.1836,     -0.0334,      2.3125,     -4.6250,     -0.5000,     -0.0361,    -22.2500,    -13.0000,      1.4453,      0.7734,     38.2500,     13.4375,      2.7344,      1.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 2.4375,  0.1113,  0.2178, -0.6602,  8.9375, -8.8125, -0.5742,  2.3750, -6.0625, -5.8125,  1.8125,  0.5703, 10.3125,  1.2344,  0.5977,  6.9375]]
-------------------------
name='positions layer 7'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 7'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 2.4375,  0.1113,  0.2178, -0.6602,  8.9375, -8.8125, -0.5742,  2.3750, -6.0625, -5.8125,  1.8125,  0.5703, 10.3125,  1.2344,  0.5977,  6.9375]]
name='residual layer 7'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -3.7031,     -1.0391,      0.1836,     -0.0334,      2.3125,     -4.6250,     -0.5000,     -0.0361,    -22.2500,    -13.0000,      1.4453,      0.7734,     38.2500,     13.4375,      2.7344,      1.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1875, -0.1069,  0.0566, -0.0898,  1.0234, -1.0234, -0.1777,  0.2852, -2.5625, -1.6172,  0.4609,  0.1768,  5.0312,  1.3203,  0.4082,  0.6914]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.2656,  -0.9297,   0.4023,  -0.6953,  11.2500, -13.4375,  -1.0781,   2.3438, -28.2500, -18.7500,   3.2500,   1.3438,  48.5000,  14.6875,   3.3281,   8.4375]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6484,  1.2031,  0.1504, -0.0337,  3.6562, -1.6562, -0.7695, -0.1748,  1.8828, -3.7656,  0.8281, -0.8438,  0.3242,  0.5508, -0.8086,  1.7344]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3613,  0.0330,  0.1025, -0.1250,  1.1328, -0.4492, -0.4023,  0.3242, -1.5391, -1.8047,  0.6289,  0.0771,  1.9688,  0.7773,  0.3496,  0.6680]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.9141,   0.2734,   0.5547,  -0.7305,  14.8750, -15.1250,  -1.8438,   2.1719, -26.3750, -22.5000,   4.0625,   0.5000,  48.7500,  15.2500,   2.5156,  10.1875]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.9219, -2.3594,  0.2471,  0.5859,  0.5977, -2.6875, -4.9375, -3.0000, -4.3750, -7.3125, -2.6562,  0.6680,  6.1562,  1.7188,  5.5312,  0.5820]]
-------------------------
name='positions layer 8'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 8'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.9219, -2.3594,  0.2471,  0.5859,  0.5977, -2.6875, -4.9375, -3.0000, -4.3750, -7.3125, -2.6562,  0.6680,  6.1562,  1.7188,  5.5312,  0.5820]]
name='residual layer 8'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.9141,   0.2734,   0.5547,  -0.7305,  14.8750, -15.1250,  -1.8438,   2.1719, -26.3750, -22.5000,   4.0625,   0.5000,  48.7500,  15.2500,   2.5156,  10.1875]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5391, -0.1953,  0.0845, -0.0140,  0.9336, -1.0078, -0.7812, -0.0796, -1.8750, -1.7109,  0.1377,  0.1104,  3.3438,  1.0703,  0.6445,  0.5547]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.8438,  -2.0938,   0.8008,  -0.1445,  15.5000, -17.7500,  -6.7812,  -0.8281, -30.7500, -29.7500,   1.4062,   1.1719,  55.0000,  17.0000,   8.0625,  10.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 2.0469,  3.4375, -0.3203, -2.9062, -1.6016,  4.5312, -2.5312, -1.1953,  1.2422,  1.9453, -1.3281,  0.2500,  3.4844, -0.0496, -0.3438,  0.7539]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3750,  0.1206,  0.0698, -0.3965,  0.8164, -0.3770, -1.4922, -0.2324, -1.3984, -1.7031,  0.0085,  0.1611,  1.8125,  0.7773,  0.8008,  0.6211]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -2.7969,   1.3438,   0.4805,  -3.0469,  13.8750, -13.2500,  -9.3125,  -2.0312, -29.5000, -27.7500,   0.0781,   1.4219,  58.5000,  17.0000,   7.7188,  11.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.5156,  -2.1406,  -2.2031,  -1.4688,   1.7734, -10.1250,  -0.5039,  -3.4062,  -2.2031,   0.8438,   0.9297,   3.2656,   9.5000,   0.8789,  -2.2500,   8.1875]]
-------------------------
name='positions layer 9'       | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 9'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.5156,  -2.1406,  -2.2031,  -1.4688,   1.7734, -10.1250,  -0.5039,  -3.4062,  -2.2031,   0.8438,   0.9297,   3.2656,   9.5000,   0.8789,  -2.2500,   8.1875]]
name='residual layer 9'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -2.7969,   1.3438,   0.4805,  -3.0469,  13.8750, -13.2500,  -9.3125,  -2.0312, -29.5000, -27.7500,   0.0781,   1.4219,  58.5000,  17.0000,   7.7188,  11.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0272, -0.0693, -0.1797, -0.3984,  1.0938, -1.7109, -1.1094, -0.4785, -2.0938, -1.8438,  0.0928,  0.3945,  5.6875,  1.3047,  0.4473,  1.2812]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.2812,  -0.7969,  -1.7188,  -4.5000,  15.6250, -23.3750,  -9.8125,  -5.4375, -31.7500, -26.8750,   1.0078,   4.6875,  68.0000,  17.8750,   5.4688,  19.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 2.2500,  0.6484,  0.4141, -2.5781, -0.4688, -3.3906,  1.2891, -3.4844,  0.7461,  1.8672, -0.8125, -1.6953,  2.0469,  0.0264,  0.3125,  4.7812]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.2285, -0.0129, -0.1748, -0.8320,  0.8594, -0.8125, -1.1953, -0.8945, -1.3906, -1.4922,  0.0186,  0.2949,  2.2031,  0.6953,  0.5664,  1.2656]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.9688,  -0.1484,  -1.3047,  -7.0625,  15.1250, -26.7500,  -8.5000,  -8.9375, -31.0000, -25.0000,   0.1953,   3.0000,  70.0000,  17.8750,   5.7812,  24.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-4.1250,  3.0156, -1.0703, -5.9375, -6.7188, -1.8516, -4.7812, -0.2080, -3.7969, -7.9688, -1.5938,  6.3750,  1.0938, -0.5117,  3.4219, -1.7031]]
-------------------------
name='positions layer 10'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 10'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-4.1250,  3.0156, -1.0703, -5.9375, -6.7188, -1.8516, -4.7812, -0.2080, -3.7969, -7.9688, -1.5938,  6.3750,  1.0938, -0.5117,  3.4219, -1.7031]]
name='residual layer 10'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.9688,  -0.1484,  -1.3047,  -7.0625,  15.1250, -26.7500,  -8.5000,  -8.9375, -31.0000, -25.0000,   0.1953,   3.0000,  70.0000,  17.8750,   5.7812,  24.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1533,  0.1592, -0.1602, -0.8164,  0.4258, -1.8203, -1.0234, -0.6133, -1.7500, -1.4844, -0.0879,  0.5898,  5.4375,  1.0469,  0.6016,  1.0234]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -2.1562,   2.8750,  -2.3750, -13.0000,   8.3750, -28.6250, -13.2500,  -9.1250, -34.7500, -33.0000,  -1.3984,   9.3750,  71.0000,  17.3750,   9.1875,  22.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-3.2344, -0.3691, -4.3750, -4.0625,  3.3750, -3.4844,  3.0625,  0.6172,  0.1465, -4.1562, -1.2578, -0.9766, -0.5391, -0.1562,  0.3750,  0.6836]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4434,  0.1660, -0.6797, -1.4844,  0.5312, -0.9414, -1.0156, -0.6758, -1.3516, -1.8359, -0.1875,  0.6445,  1.7891,  0.6758,  0.7227,  1.0156]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -5.3750,   2.5000,  -6.7500, -17.0000,  11.7500, -32.0000, -10.1875,  -8.5000, -34.5000, -37.2500,  -2.6562,   8.3750,  70.5000,  17.2500,   9.5625,  23.3750]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  4.2812,  -0.8125,   5.9375,  -3.2344,  -3.7500, -18.8750,   1.0469,  -2.8281,   3.9219, -11.9375,   5.3125,   4.0312,   0.6836,  -0.1328,  -3.0625,  -1.2969]]
-------------------------
name='positions layer 11'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 11'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  4.2812,  -0.8125,   5.9375,  -3.2344,  -3.7500, -18.8750,   1.0469,  -2.8281,   3.9219, -11.9375,   5.3125,   4.0312,   0.6836,  -0.1328,  -3.0625,  -1.2969]]
name='residual layer 11'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -5.3750,   2.5000,  -6.7500, -17.0000,  11.7500, -32.0000, -10.1875,  -8.5000, -34.5000, -37.2500,  -2.6562,   8.3750,  70.5000,  17.2500,   9.5625,  23.3750]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0723,  0.0986, -0.0552, -1.2656,  0.3828, -2.6406, -0.6758, -0.6641, -1.5703, -2.5156,  0.1631,  0.7500,  4.8750,  1.1016,  0.3867,  1.0469]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.0938,   1.6875,  -0.8125, -20.2500,   8.0000, -51.0000,  -9.1250, -11.3125, -30.6250, -49.2500,   2.6562,  12.3750,  71.0000,  17.1250,   6.5000,  22.1250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 4.0938, -2.6406,  2.8906,  2.7969,  1.0469, -7.3750, -0.3301, -2.7500, -0.5977, -0.1475,  6.4688, -1.4922,  2.7344,  0.1484,  4.0312, -0.1211]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1963, -0.0623,  0.1719, -1.2422,  0.3828, -1.6406, -0.7656, -0.9883, -1.2812, -2.2969,  0.5312,  0.6914,  2.0000,  0.6094,  0.6641,  0.9648]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.0000,  -0.9531,   2.0781, -17.5000,   9.0625, -58.5000,  -9.4375, -14.0625, -31.2500, -49.5000,   9.1250,  10.8750,  73.5000,  17.2500,  10.5000,  22.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    10.3125,     -2.7812,     -5.7188,     -9.8750,     -0.9414,     -2.0781,     -7.1250,     -4.1250,     -8.1250,     -4.4375,     -2.1875,      1.3359,     10.9375,      0.0006,    -12.8125,      2.0312]]
-------------------------
name='positions layer 12'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 12'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    10.3125,     -2.7812,     -5.7188,     -9.8750,     -0.9414,     -2.0781,     -7.1250,     -4.1250,     -8.1250,     -4.4375,     -2.1875,      1.3359,     10.9375,      0.0006,    -12.8125,      2.0312]]
name='residual layer 12'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.0000,  -0.9531,   2.0781, -17.5000,   9.0625, -58.5000,  -9.4375, -14.0625, -31.2500, -49.5000,   9.1250,  10.8750,  73.5000,  17.2500,  10.5000,  22.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.6836, -0.1875, -0.1709, -1.0938,  0.3281, -3.1719, -0.8984, -0.7891, -1.7812, -2.3594,  0.3340,  0.5312,  4.4375,  0.7109, -0.0957,  0.9023]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.3125,  -3.7344,  -3.6406, -27.3750,   8.1250, -60.5000, -16.5000, -18.2500, -39.5000, -54.0000,   6.9375,  12.1875,  84.5000,  17.2500,  -2.3125,  24.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.5625, -3.6875, -5.8438, -1.5000,  7.1562, -3.1406, -3.1094,  4.4688,  0.5625, -4.2500,  6.5312,  3.3750,  0.9805, -0.2832,  0.3223,  0.1748]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.7773, -0.3867, -0.6094, -1.5312,  0.5508, -1.5469, -1.1328, -0.7266, -1.4922, -2.4688,  0.5742,  0.7383,  2.2500,  0.7891, -0.0996,  0.9844]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 16.8750,  -7.4375,  -9.5000, -28.8750,  15.2500, -63.7500, -19.6250, -13.7500, -39.0000, -58.2500,  13.5000,  15.5625,  85.5000,  17.0000,  -1.9922,  24.1250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 4.2188, -7.5625,  7.6875,  5.7188, 10.5625,  3.5938, -7.8438,  0.9492,  1.3984,  1.0547,  4.4062, -3.8125,  0.9805,  0.2754, -0.6562, -5.5625]]
-------------------------
name='positions layer 13'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 13'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 4.2188, -7.5625,  7.6875,  5.7188, 10.5625,  3.5938, -7.8438,  0.9492,  1.3984,  1.0547,  4.4062, -3.8125,  0.9805,  0.2754, -0.6562, -5.5625]]
name='residual layer 13'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 16.8750,  -7.4375,  -9.5000, -28.8750,  15.2500, -63.7500, -19.6250, -13.7500, -39.0000, -58.2500,  13.5000,  15.5625,  85.5000,  17.0000,  -1.9922,  24.1250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 1.0000, -0.7812, -0.1001, -1.0547,  1.1094, -3.0625, -1.4531, -0.6133, -1.7344, -2.7656,  0.8242,  0.5469,  5.9688,  1.3203, -0.1260,  0.8438]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 21.1250, -15.0000,  -1.8125, -23.1250,  25.7500, -60.2500, -27.5000, -12.8125, -37.5000, -57.2500,  17.8750,  11.7500,  86.5000,  17.2500,  -2.6562,  18.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -7.8750,   5.9062,   2.4688,  -1.7344,   2.7031,  -6.5000,   0.0184,   0.6328,  -1.1953,  -0.1592, -17.5000,  -8.1875,   2.2031,   0.2539,  12.8125,  -1.5469]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4746, -0.4141,  0.0337, -1.1328,  0.9141, -1.7031, -1.2422, -0.5312, -1.6250, -2.2969,  0.0130,  0.1426,  3.5000,  1.3516,  0.4199,  0.6953]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.2500,  -9.1250,   0.6562, -24.8750,  28.5000, -67.0000, -27.5000, -12.1875, -38.7500, -57.5000,   0.3750,   3.5625,  88.5000,  17.5000,  10.1250,  17.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 10.3125, -16.5000,   9.3125,  -8.8750,   6.2188,  -5.3125,  -4.9375,   1.9688,   0.0796,  19.7500,   7.5312,  -6.3750,   0.3457,  -0.2129,  11.6250, -16.8750]]
-------------------------
name='positions layer 14'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 14'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 10.3125, -16.5000,   9.3125,  -8.8750,   6.2188,  -5.3125,  -4.9375,   1.9688,   0.0796,  19.7500,   7.5312,  -6.3750,   0.3457,  -0.2129,  11.6250, -16.8750]]
name='residual layer 14'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.2500,  -9.1250,   0.6562, -24.8750,  28.5000, -67.0000, -27.5000, -12.1875, -38.7500, -57.5000,   0.3750,   3.5625,  88.5000,  17.5000,  10.1250,  17.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     0.6250,     -0.8438,      0.2891,     -0.8477,      0.9492,     -2.5938,     -1.0547,     -0.3125,     -1.2422,     -1.1328,      0.2422,     -0.0767,      4.0312,      0.7266,      0.6094,      0.0033]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 23.5000, -25.6250,  10.0000, -33.7500,  34.7500, -72.5000, -32.5000, -10.2500, -38.7500, -37.7500,   7.9062,  -2.8125,  89.0000,  17.2500,  21.7500,   0.1250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[   -22.0000,    -14.0625,    -14.4375,    -13.1875,     -0.5742,     -0.5742,     -2.4844,     -5.8750,     -7.1562,     -4.3125,     10.0625,      0.7031,      4.9062,      0.0005,      1.3906,     -7.4375]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0332, -1.3047, -0.1670, -1.5000,  0.9023, -1.6953, -1.1016, -0.5352, -1.6016, -1.3281,  0.4102, -0.0625,  2.9062,  1.2734,  0.7109, -0.2363]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.5000, -39.7500,  -4.4375, -47.0000,  34.2500, -73.0000, -35.0000, -16.1250, -46.0000, -42.0000,  18.0000,  -2.1094,  94.0000,  17.2500,  23.1250,  -7.3125]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.0000,  -0.6406,  -8.8750,  -6.2812,   9.1875,   1.4922, -19.8750, -30.1250,  -8.8125, -12.7500, -19.7500,  24.6250,   6.7188,  -1.2500,  17.2500,   0.6758]]
-------------------------
name='positions layer 15'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 15'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.0000,  -0.6406,  -8.8750,  -6.2812,   9.1875,   1.4922, -19.8750, -30.1250,  -8.8125, -12.7500, -19.7500,  24.6250,   6.7188,  -1.2500,  17.2500,   0.6758]]
name='residual layer 15'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.5000, -39.7500,  -4.4375, -47.0000,  34.2500, -73.0000, -35.0000, -16.1250, -46.0000, -42.0000,  18.0000,  -2.1094,  94.0000,  17.2500,  23.1250,  -7.3125]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1875, -1.3359, -0.4082, -1.5781,  1.2656, -2.3594, -1.6875, -1.4453, -1.8594, -1.7500, -0.0449,  0.6055,  4.6250,  0.8555,  1.1953, -0.1943]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  6.5000, -40.5000, -13.3125, -53.2500,  43.5000, -71.5000, -55.0000, -46.2500, -54.7500, -54.7500,  -1.7500,  22.5000, 100.5000,  16.0000,  40.5000,  -6.6250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-53.7500,   7.6562,   2.3750,  17.8750,  -4.3125,  -1.5391, -16.1250,   4.1250,  -2.8125,  -9.5625,  -5.0938,   7.0000,   1.2422,  -0.1377,  16.1250,  -8.4375]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7422, -0.9609, -0.3262, -0.9922,  0.9961, -1.7500, -1.7188, -1.2188, -1.8906, -1.9453, -0.1157,  0.7695,  3.3125,  1.1250,  1.5312, -0.4648]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-47.2500, -32.7500, -10.9375, -35.5000,  39.2500, -73.0000, -71.0000, -42.0000, -57.5000, -64.5000,  -6.8438,  29.5000, 101.5000,  15.8750,  56.5000, -15.0625]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 15.3125,   8.4375,  -7.1562,   2.1875,  -2.7188,  -1.0234, -33.0000,   8.5625,   3.1719,  -3.6719,  38.2500,  -8.6250,  -6.4375,   0.6680,   4.2500, -11.5000]]
-------------------------
name='positions layer 16'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 16'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 15.3125,   8.4375,  -7.1562,   2.1875,  -2.7188,  -1.0234, -33.0000,   8.5625,   3.1719,  -3.6719,  38.2500,  -8.6250,  -6.4375,   0.6680,   4.2500, -11.5000]]
name='residual layer 16'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-47.2500, -32.7500, -10.9375, -35.5000,  39.2500, -73.0000, -71.0000, -42.0000, -57.5000, -64.5000,  -6.8438,  29.5000, 101.5000,  15.8750,  56.5000, -15.0625]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4629, -0.5977, -0.3555, -0.5312,  0.6836, -1.8203, -1.8438, -0.6602, -1.1484, -1.4922,  0.5039,  0.3926,  2.5938,  0.4844,  1.1250, -0.5430]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -32.0000,  -24.2500,  -18.1250,  -33.2500,   36.5000,  -74.0000, -104.0000,  -33.5000,  -54.2500,  -68.0000,   31.3750,   20.8750,   95.0000,   16.5000,   60.7500,  -26.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -6.9688,  16.8750, -12.1875,   1.8828,  -5.9062,   1.0625, -12.4375,   8.6875,  -1.9844,   1.9375,  17.0000,  18.8750,   1.3281,  -0.7305,  -0.5938,   4.8438]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3965, -0.1719, -0.6719, -0.6484,  0.6914, -1.6250, -2.0469, -0.5352, -1.6172, -1.6328,  0.5977,  0.8633,  2.6562,  1.4141,  1.1953, -0.5742]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -39.0000,   -7.3750,  -30.2500,  -31.3750,   30.6250,  -73.0000, -116.5000,  -24.7500,  -56.2500,  -66.0000,   48.5000,   39.7500,   96.5000,   15.7500,   60.2500,  -21.6250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 60.2500,  -8.8750, -18.3750, -51.2500,  -7.8750,  17.2500, -71.0000,   5.0312, -18.6250,  50.5000, -50.5000, -14.6875,  15.3125,   0.0830,   0.6367, -15.0625]]
-------------------------
name='positions layer 17'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 17'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 60.2500,  -8.8750, -18.3750, -51.2500,  -7.8750,  17.2500, -71.0000,   5.0312, -18.6250,  50.5000, -50.5000, -14.6875,  15.3125,   0.0830,   0.6367, -15.0625]]
name='residual layer 17'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -39.0000,   -7.3750,  -30.2500,  -31.3750,   30.6250,  -73.0000, -116.5000,  -24.7500,  -56.2500,  -66.0000,   48.5000,   39.7500,   96.5000,   15.7500,   60.2500,  -21.6250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.2695, -0.3086, -0.7969, -1.4453,  0.4238, -1.2891, -2.8750, -0.3418, -1.6562, -0.2949, -0.0283,  0.4336,  3.6094,  0.7266,  1.0078, -0.7031]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  21.2500,  -16.2500,  -48.5000,  -82.5000,   22.7500,  -55.7500, -188.0000,  -19.7500,  -75.0000,  -15.5000,   -2.0000,   25.0000,  112.0000,   15.8125,   61.0000,  -36.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-48.5000,  -9.8750,  -5.7188,   2.0156, -18.2500,  -8.2500, -29.3750,  -2.6094,  -0.9219,  18.0000,  15.1250,  -9.8125,  -2.3125,   0.1777,  13.1250,   7.2812]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1777, -0.4082, -0.7969, -1.0938,  0.0728, -1.1797, -2.5469, -0.3535, -1.5156,  0.0435,  0.0962,  0.2061,  2.6875,  1.2500,  0.9609, -0.5586]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -27.2500,  -26.1250,  -54.2500,  -80.5000,    4.5000,  -64.0000, -217.0000,  -22.3750,  -76.0000,    2.5000,   13.1250,   15.1875,  109.5000,   16.0000,   74.0000,  -29.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -165.0000,     -7.2812,     66.0000,    -15.6875,    -20.8750,      9.1875,    -56.5000,      0.1069,    -22.3750,     -7.7500,     -5.4688,     -8.3125,     10.5625,     -0.3164,     73.0000,    -52.5000]]
-------------------------
name='positions layer 18'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 18'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -165.0000,     -7.2812,     66.0000,    -15.6875,    -20.8750,      9.1875,    -56.5000,      0.1069,    -22.3750,     -7.7500,     -5.4688,     -8.3125,     10.5625,     -0.3164,     73.0000,    -52.5000]]
name='residual layer 18'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -27.2500,  -26.1250,  -54.2500,  -80.5000,    4.5000,  -64.0000, -217.0000,  -22.3750,  -76.0000,    2.5000,   13.1250,   15.1875,  109.5000,   16.0000,   74.0000,  -29.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1562, -0.4043,  0.1084, -0.9062, -0.2061, -0.9141, -2.4844, -0.2344, -1.3984, -0.0664,  0.0630,  0.0742,  2.1406,  0.4727,  1.4062, -0.9961]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-192.0000,  -33.5000,   11.7500,  -96.0000,  -16.3750,  -54.7500, -274.0000,  -22.2500,  -98.5000,   -5.2500,    7.6562,    6.8750,  120.0000,   15.6875,  147.0000,  -82.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-24.0000,  27.0000,  17.0000,  -2.7031, -11.2500, -16.1250,   6.3438, -49.0000,  -3.3750, -17.1250,  12.3750,   9.0625,   2.4531,   0.4863,  47.7500, -24.7500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9805, -0.0791,  0.3301, -1.0469, -0.3828, -1.1406, -2.2344, -0.8789, -1.6641, -0.3086,  0.1045,  0.1699,  2.1719,  1.0703,  1.9141, -1.6172]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-216.0000,   -6.5000,   28.7500,  -98.5000,  -27.6250,  -71.0000, -268.0000,  -71.0000, -102.0000,  -22.3750,   20.0000,   15.9375,  122.5000,   16.1250,  195.0000, -107.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -7.8125,  17.2500,   6.8438, -20.3750,  25.2500,   4.8125, -39.5000,  -6.6875,  -7.2188, -19.5000, -56.5000,   4.8750,   7.7500,   2.3438,   8.1250,   5.5938]]
-------------------------
name='positions layer 19'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 19'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -7.8125,  17.2500,   6.8438, -20.3750,  25.2500,   4.8125, -39.5000,  -6.6875,  -7.2188, -19.5000, -56.5000,   4.8750,   7.7500,   2.3438,   8.1250,   5.5938]]
name='residual layer 19'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-216.0000,   -6.5000,   28.7500,  -98.5000,  -27.6250,  -71.0000, -268.0000,  -71.0000, -102.0000,  -22.3750,   20.0000,   15.9375,  122.5000,   16.1250,  195.0000, -107.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.5234,  0.1040,  0.3105, -1.0547, -0.0261, -0.9766, -2.4688, -0.7266, -1.3125, -0.4395, -0.2480,  0.1768,  2.4688,  0.6523,  1.6953, -1.0469]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-224.0000,   10.7500,   35.5000, -119.0000,   -2.3750,  -66.0000, -308.0000,  -77.5000, -109.0000,  -42.0000,  -36.5000,   20.7500,  130.0000,   18.5000,  203.0000, -101.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-49.5000,  16.3750, -36.5000,  -4.4688, -17.6250,  -4.6562, -77.0000,   5.2812,   8.2500,   6.2500,  22.5000,  21.5000,   5.9375,  -1.2109,   6.5000, -17.3750]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.0859,  0.2773, -0.0095, -1.1328, -0.2500, -1.0547, -2.5938, -0.7266, -1.3984, -0.4258, -0.0605,  0.3926,  2.4375,  1.0000,  1.6641, -1.4922]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-274.0000,   27.1250,   -1.0000, -123.5000,  -20.0000,  -70.5000, -384.0000,  -72.0000, -101.0000,  -35.7500,  -14.0000,   42.2500,  136.0000,   17.2500,  210.0000, -119.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 23.0000,  22.3750,  60.2500,  33.7500,  -0.7227,  17.2500, -15.7500, -53.0000,  47.7500,  98.5000,  86.5000, -22.7500,  19.6250,  -2.6875,  14.9375, -17.6250]]
-------------------------
name='positions layer 20'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 20'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 23.0000,  22.3750,  60.2500,  33.7500,  -0.7227,  17.2500, -15.7500, -53.0000,  47.7500,  98.5000,  86.5000, -22.7500,  19.6250,  -2.6875,  14.9375, -17.6250]]
name='residual layer 20'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-274.0000,   27.1250,   -1.0000, -123.5000,  -20.0000,  -70.5000, -384.0000,  -72.0000, -101.0000,  -35.7500,  -14.0000,   42.2500,  136.0000,   17.2500,  210.0000, -119.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9688,  0.2871,  0.2695, -0.4375, -0.1279, -0.4785, -1.5391, -0.6914, -0.3887,  0.3379,  0.2734,  0.0957,  1.5703,  0.3223,  1.0078, -0.7695]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-251.0000,   49.5000,   59.2500,  -90.0000,  -20.7500,  -53.2500, -400.0000, -125.0000,  -53.2500,   62.7500,   72.5000,   19.5000,  156.0000,   14.5625,  225.0000, -137.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-12.4375,  30.5000, -30.5000, -11.4375, -17.6250, -10.4375, -60.7500, -32.5000, -27.1250,  -8.5625,  13.7500,  32.7500,   0.5117,  -0.5859,  67.5000,  -1.5859]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.8711,  0.5977,  0.1934, -0.6758, -0.3457, -0.8008, -2.2812, -1.2344, -0.8047,  0.4570,  0.3086,  0.3691,  2.0938,  0.5273,  1.6406, -1.2500]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-264.0000,   80.0000,   28.7500, -101.5000,  -38.5000,  -63.7500, -460.0000, -158.0000,  -80.5000,   54.2500,   86.0000,   52.2500,  157.0000,   14.0000,  292.0000, -139.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-83.5000,  27.3750, -28.6250, -20.1250, -35.7500,  22.8750, -61.7500,  -4.9688,  22.6250,  10.4375, 110.5000, -10.8125,  -6.2188,   1.6016,  43.0000,  22.7500]]
-------------------------
name='positions layer 21'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 21'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-83.5000,  27.3750, -28.6250, -20.1250, -35.7500,  22.8750, -61.7500,  -4.9688,  22.6250,  10.4375, 110.5000, -10.8125,  -6.2188,   1.6016,  43.0000,  22.7500]]
name='residual layer 21'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-264.0000,   80.0000,   28.7500, -101.5000,  -38.5000,  -63.7500, -460.0000, -158.0000,  -80.5000,   54.2500,   86.0000,   52.2500,  157.0000,   14.0000,  292.0000, -139.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.5859,      0.6016,      0.0006,     -0.6094,     -0.4766,     -0.3828,     -2.4062,     -0.9570,     -0.3828,      0.3906,      0.9688,      0.2207,      1.8125,      0.3887,      1.5391,     -0.6836]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -348.0000,    107.5000,      0.1250,   -121.5000,    -74.0000,    -41.0000,   -520.0000,   -163.0000,    -58.0000,     64.5000,    196.0000,     41.5000,    151.0000,     15.6250,    336.0000,   -116.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-129.0000,   13.7500,   31.1250,  -18.7500,  -29.3750,  -12.6875,   62.2500,   45.2500,   21.1250,    6.3125,   38.5000,   39.5000,   -8.1250,   -0.4473,   -2.5625,  -49.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4453,  0.6914,  0.1670, -0.7695, -0.7148, -0.5039, -1.8594, -0.6914, -0.2734,  0.4609,  0.7461,  0.4238,  1.4609,  0.4004,  1.5000, -1.1172]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-476.0000,  121.0000,   31.2500, -140.0000, -103.5000,  -53.7500, -458.0000, -118.0000,  -37.0000,   71.0000,  234.0000,   81.0000,  143.0000,   15.1875,  334.0000, -165.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -16.6250,  120.0000,   19.6250,   53.7500,   25.2500,  -20.3750,    3.2812,  -77.0000,  -25.2500,   15.8750,   36.7500, -160.0000,   20.2500,    2.7344,  183.0000,  -23.8750]]
-------------------------
name='positions layer 22'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 22'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -16.6250,  120.0000,   19.6250,   53.7500,   25.2500,  -20.3750,    3.2812,  -77.0000,  -25.2500,   15.8750,   36.7500, -160.0000,   20.2500,    2.7344,  183.0000,  -23.8750]]
name='residual layer 22'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-476.0000,  121.0000,   31.2500, -140.0000, -103.5000,  -53.7500, -458.0000, -118.0000,  -37.0000,   71.0000,  234.0000,   81.0000,  143.0000,   15.1875,  334.0000, -165.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1172,  0.8594,  0.1357, -0.2285, -0.3008, -0.4629, -1.2266, -0.6172, -0.2422,  0.2617,  0.7188, -0.2373,  1.1484,  0.2754,  1.2734, -0.5664]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-492.0000,  241.0000,   51.0000,  -86.0000,  -78.0000,  -74.0000, -454.0000, -195.0000,  -62.2500,   87.0000,  270.0000,  -79.0000,  163.0000,   17.8750,  516.0000, -189.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  7.2188,   9.5625,  11.8750, -27.3750, -62.0000, -50.7500, -62.7500,  -4.3438, -30.3750,  28.0000, -21.6250,  26.5000,  20.2500, -20.6250,  39.5000, -24.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1172,  0.9727,  0.2197, -0.3984, -0.6641, -0.7578, -1.5078, -0.7773, -0.4434,  0.4902,  0.6211, -0.1875,  1.2109, -0.0400,  1.8047, -0.9414]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-484.0000,  251.0000,   63.0000, -113.5000, -140.0000, -125.0000, -516.0000, -199.0000,  -92.5000,  115.0000,  248.0000,  -52.5000,  183.0000,   -2.7500,  556.0000, -213.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 42.0000, -48.7500,  64.5000,  11.8750, -25.1250, -34.0000,  64.5000,   6.1875, -30.6250,  12.0000, -11.5000, -81.0000, -43.7500, -21.8750, -11.6875, -22.7500]]
-------------------------
name='positions layer 23'      | dtype=torch.int64     | shape=(1,)                 | val = 10
[10]
name='x layer 23'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 42.0000, -48.7500,  64.5000,  11.8750, -25.1250, -34.0000,  64.5000,   6.1875, -30.6250,  12.0000, -11.5000, -81.0000, -43.7500, -21.8750, -11.6875, -22.7500]]
name='residual layer 23'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-484.0000,  251.0000,   63.0000, -113.5000, -140.0000, -125.0000, -516.0000, -199.0000,  -92.5000,  115.0000,  248.0000,  -52.5000,  183.0000,   -2.7500,  556.0000, -213.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
name='x after forward'         | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-9.1250,  3.9688,  2.3750, -2.0312, -3.5625, -4.7812, -9.6875, -3.6875, -3.0938,  2.8750,  5.4688, -2.6719,  3.7031, -0.8086, 12.1875, -5.5312]]
----------------------------------------------------------------------------------------------------
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
name='[COMPUTE_LOGITS] hidden states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-9.1250,  3.9688,  2.3750, -2.0312, -3.5625, -4.7812, -9.6875, -3.6875, -3.0938,  2.8750,  5.4688, -2.6719,  3.7031, -0.8086, 12.1875, -5.5312]]
----------------------------------------------------------------------------------------------------
name='[LOGITS] BEFORE self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-9.1250,  3.9688,  2.3750, -2.0312, -3.5625, -4.7812, -9.6875, -3.6875, -3.0938,  2.8750,  5.4688, -2.6719,  3.7031, -0.8086, 12.1875, -5.5312]]
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] x' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-9.1250,  3.9688,  2.3750, -2.0312, -3.5625, -4.7812, -9.6875, -3.6875, -3.0938,  2.8750,  5.4688, -2.6719,  3.7031, -0.8086, 12.1875, -5.5312]]
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
name='[UnquantizedEmbeddingMethod] out' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------
name='[LOGITS] AFTER lm_head.quant_method.apply' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
name='[LOGITS] AFTER self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------
name='logits'                  | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
name='input_ids'               | dtype=torch.int32     | shape=(1,)                 | val = 0
name='input_embeds'            | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2988, -0.6953,  1.9141, -0.0649,  0.7461, -1.2031, -0.1865,  0.1436, -6.0000,  1.9844, -0.0967,  0.8711,  2.7969,  6.2812, -0.1177,  0.3008]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1514, -0.3164,  0.7500, -0.0874,  0.3145, -0.2969, -0.1357,  0.1069, -1.7891,  0.7383, -0.1523,  0.4590,  0.9961,  1.6094, -0.1602,  0.2598]]
[after   INPUT_LAYER_NORM] residual is None
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1123, -0.2305,  0.0302,  0.0342, -0.6094, -0.1348,  0.0228, -0.1670, -0.1426, -0.4512, -0.0374, -0.2061,  0.5977, -0.0684,  0.0057, -0.1270]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1206, -0.4570,  0.8750, -0.0605,  0.0535, -0.2080, -0.1904, -0.0262, -0.9062,  0.4980, -0.2832,  0.4629,  0.5742,  0.3418, -0.1982,  0.1650]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1865, -0.9258,  1.9453, -0.0308,  0.1367, -1.3359, -0.1641, -0.0234, -6.1562,  1.5312, -0.1338,  0.6641,  3.3906,  6.2188, -0.1118,  0.1738]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1426,  0.3398, -0.2168,  0.2461, -0.3398, -1.2344,  0.0253, -0.1279,  0.4238, -1.2344,  0.1807, -0.1367,  0.9219, -1.5859,  0.0022,  0.6680]]
-------------------------
name='positions layer 0'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 0'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1426,  0.3398, -0.2168,  0.2461, -0.3398, -1.2344,  0.0253, -0.1279,  0.4238, -1.2344,  0.1807, -0.1367,  0.9219, -1.5859,  0.0022,  0.6680]]
name='residual layer 0'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1865, -0.9258,  1.9453, -0.0308,  0.1367, -1.3359, -0.1641, -0.0234, -6.1562,  1.5312, -0.1338,  0.6641,  3.3906,  6.2188, -0.1118,  0.1738]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2041, -0.3164,  0.7812,  0.2266, -0.0801, -0.6445, -0.1094, -0.1177, -1.9844,  0.1167,  0.0525,  0.3242,  1.6484,  1.5000, -0.1045,  0.5352]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3281, -0.5859,  1.7266,  0.2148, -0.2031, -2.5625, -0.1387, -0.1514, -5.7188,  0.2969,  0.0469,  0.5273,  4.3125,  4.6250, -0.1094,  0.8438]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1172,  0.4219, -0.2178, -0.2266,  0.3145,  1.1172,  0.1992,  0.0262,  2.0469, -1.2266, -0.0216,  0.2637, -2.2812,  1.0234,  0.2539, -0.4355]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3730, -0.0986,  0.8750, -0.0194,  0.0532, -0.2178,  0.0776, -0.1455, -0.5820, -0.3340,  0.0417,  0.6367,  0.2812,  0.3320,  0.1963,  0.2656]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4453, -0.1641,  1.5078, -0.0117,  0.1113, -1.4453,  0.0605, -0.1250, -3.6719, -0.9297,  0.0253,  0.7891,  2.0312,  5.6562,  0.1445,  0.4082]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7227,  0.3535, -0.4336, -0.3379, -0.6914,  2.6094, -0.0967, -0.0518,  0.2451, -1.4922,  0.6328,  0.0874,  0.9922, -2.8750,  0.0737,  0.0737]]
-------------------------
name='positions layer 1'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 1'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7227,  0.3535, -0.4336, -0.3379, -0.6914,  2.6094, -0.0967, -0.0518,  0.2451, -1.4922,  0.6328,  0.0874,  0.9922, -2.8750,  0.0737,  0.0737]]
name='residual layer 1'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4453, -0.1641,  1.5078, -0.0117,  0.1113, -1.4453,  0.0605, -0.1250, -3.6719, -0.9297,  0.0253,  0.7891,  2.0312,  5.6562,  0.1445,  0.4082]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7031,  0.0933,  0.4883, -0.2695, -0.2227,  0.2949, -0.0267, -0.1143, -1.0078, -0.8906,  0.5430,  0.4980,  1.0312,  0.7266,  0.1611,  0.2168]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1719,  0.1895,  1.0781, -0.3496, -0.5781,  1.1641, -0.0361, -0.1768, -3.4219, -2.4219,  0.6562,  0.8750,  3.0312,  2.7812,  0.2188,  0.4824]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3164,  0.7539,  0.0471, -0.2256,  0.5625,  0.1104,  0.0723, -0.6094, -0.1367, -1.1641, -0.3340,  0.0149, -0.6211,  0.3145,  0.0864, -0.7695]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2656,  0.4980,  0.6914, -0.6523, -0.0065,  0.1631,  0.0403, -0.7227, -0.5312, -1.1875,  0.3574,  0.6641,  0.2266,  0.1670,  0.2871, -0.1206]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4844,  0.9453,  1.1250, -0.5742, -0.0156,  1.2734,  0.0361, -0.7852, -3.5625, -3.5938,  0.3223,  0.8906,  2.4062,  3.0938,  0.3047, -0.2871]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0664,  0.6562,  0.1289,  0.1592, -1.2578, -1.0391, -0.2080, -0.3359,  0.5078, -0.4219, -0.3125, -0.1650, -0.3145, -0.4141, -0.3926, -0.0923]]
-------------------------
name='positions layer 2'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 2'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0664,  0.6562,  0.1289,  0.1592, -1.2578, -1.0391, -0.2080, -0.3359,  0.5078, -0.4219, -0.3125, -0.1650, -0.3145, -0.4141, -0.3926, -0.0923]]
name='residual layer 2'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4844,  0.9453,  1.1250, -0.5742, -0.0156,  1.2734,  0.0361, -0.7852, -3.5625, -3.5938,  0.3223,  0.8906,  2.4062,  3.0938,  0.3047, -0.2871]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6953,  0.6055,  0.5195, -0.2305, -0.3633,  0.0437, -0.1040, -0.5508, -0.6367, -1.0859,  0.0053,  0.3164,  0.4258,  0.5000, -0.0422, -0.1045]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4219,  1.6016,  1.2500, -0.4141, -1.2734,  0.2344, -0.1719, -1.1250, -3.0625, -4.0000,  0.0098,  0.7266,  2.0938,  2.6875, -0.0879, -0.3789]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0835,  0.0684, -0.1021, -0.2676,  0.1289,  0.6992,  0.0991,  0.1108,  0.1211, -0.7109, -0.3477,  0.0542,  2.4062,  0.3047,  0.4277, -0.6211]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2344,  0.8633,  0.7656, -0.6328, -0.4473,  0.1040, -0.0737, -0.7852, -0.3867, -1.4766, -0.2969,  0.5664,  0.3164,  0.1895,  0.2480, -0.3359]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.5078,  1.6719,  1.1484, -0.6797, -1.1406,  0.9336, -0.0728, -1.0156, -2.9375, -4.7188, -0.3379,  0.7812,  4.5000,  3.0000,  0.3398, -1.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.3203,  0.8203,  0.1016, -0.3262,  1.4766, -2.4219,  0.2578, -0.1245, -0.7344, -0.7891,  0.2988,  0.3145,  1.6250, -0.3145, -0.0977,  0.4062]]
-------------------------
name='positions layer 3'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 3'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.3203,  0.8203,  0.1016, -0.3262,  1.4766, -2.4219,  0.2578, -0.1245, -0.7344, -0.7891,  0.2988,  0.3145,  1.6250, -0.3145, -0.0977,  0.4062]]
name='residual layer 3'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.5078,  1.6719,  1.1484, -0.6797, -1.1406,  0.9336, -0.0728, -1.0156, -2.9375, -4.7188, -0.3379,  0.7812,  4.5000,  3.0000,  0.3398, -1.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5664,  0.9570,  0.4570, -0.4648,  0.0923, -0.2676,  0.0942, -0.5156, -0.8984, -1.4609, -0.0187,  0.4512,  1.6797,  0.6797,  0.1025, -0.1611]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1875,  2.5000,  1.2500, -1.0078,  0.3359, -1.4844,  0.1855, -1.1406, -3.6719, -5.5000, -0.0391,  1.0938,  6.1250,  2.6875,  0.2422, -0.5938]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0344, -0.5586,  0.1260,  0.3926, -1.8750, -3.4844, -0.2598, -0.2578,  2.5000, -1.8125, -0.7344, -0.5117,  4.5000,  0.6133,  0.4434, -2.9688]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.8828,  0.8516,  0.8984, -0.4629, -0.5117, -0.4746, -0.0654, -0.9062, -0.1670, -2.0156, -0.5547,  0.3750,  0.7773,  0.2148,  0.4160, -0.9219]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1562,  1.9375,  1.3750, -0.6172, -1.5391, -4.9688, -0.0742, -1.3984, -1.1719, -7.3125, -0.7734,  0.5820, 10.6250,  3.2969,  0.6875, -3.5625]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.8047, -1.1406, -1.0859, -0.5625, -1.8672,  8.6250,  0.6914,  0.8633, -3.2031, -1.3984,  1.1016, -0.6094,  2.4688,  1.7578, -0.7969, -0.8047]]
-------------------------
name='positions layer 4'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 4'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.8047, -1.1406, -1.0859, -0.5625, -1.8672,  8.6250,  0.6914,  0.8633, -3.2031, -1.3984,  1.1016, -0.6094,  2.4688,  1.7578, -0.7969, -0.8047]]
name='residual layer 4'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1562,  1.9375,  1.3750, -0.6172, -1.5391, -4.9688, -0.0742, -1.3984, -1.1719, -7.3125, -0.7734,  0.5820, 10.6250,  3.2969,  0.6875, -3.5625]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1094,  0.1953,  0.0762, -0.3340, -0.6250,  0.5156,  0.2080, -0.1406, -0.6953, -1.5938,  0.0942, -0.0073,  2.1250,  0.6289, -0.0275, -0.7344]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3516,  0.7969,  0.2891, -1.1797, -3.4062,  3.6562,  0.6172, -0.5352, -4.3750, -8.6875,  0.3281, -0.0273, 13.1250,  5.0625, -0.1094, -4.3750]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2891,  0.4062, -0.1348, -0.6055, -1.9922,  1.9219,  0.6562, -0.2227, -1.3828,  1.9141,  0.6367, -0.1147, -0.6016,  0.1445, -0.3242, -0.4512]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3047,  0.3301,  0.0640, -0.7734, -1.0391,  0.3203,  0.6875, -0.2891, -0.5117, -1.1484,  0.3984, -0.0562,  0.5938,  0.3105, -0.1533, -0.7188]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6406,  1.2031,  0.1543, -1.7812, -5.4062,  5.5625,  1.2734, -0.7578, -5.7500, -6.7812,  0.9648, -0.1426, 12.5000,  5.2188, -0.4336, -4.8125]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1250, -0.7734, -0.9688,  0.0654,  3.4375,  2.0781, -0.2539,  1.2188,  0.1934, -0.1494,  0.5078, -0.3945, -0.5781,  0.4902,  0.9297, -0.2617]]
-------------------------
name='positions layer 5'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 5'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1250, -0.7734, -0.9688,  0.0654,  3.4375,  2.0781, -0.2539,  1.2188,  0.1934, -0.1494,  0.5078, -0.3945, -0.5781,  0.4902,  0.9297, -0.2617]]
name='residual layer 5'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6406,  1.2031,  0.1543, -1.7812, -5.4062,  5.5625,  1.2734, -0.7578, -5.7500, -6.7812,  0.9648, -0.1426, 12.5000,  5.2188, -0.4336, -4.8125]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2256,  0.0967, -0.2031, -0.4453, -0.3223,  1.0000,  0.3164,  0.1138, -0.9805, -1.1016,  0.4082, -0.1387,  2.2656,  0.9609,  0.1157, -0.7812]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7656,  0.4297, -0.8125, -1.7188, -1.9688,  7.6250,  1.0156,  0.4609, -5.5625, -6.9375,  1.4688, -0.5391, 11.9375,  5.7188,  0.4961, -5.0625]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6875,  0.6484,  0.4570,  0.5156, -2.1875, -0.7266,  0.2617, -0.4277, -0.3984, -2.6406, -0.5312, -0.6250,  2.7188,  0.1338,  1.0234, -1.5156]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6250,  0.2812, -0.1416, -0.4570, -0.6992,  0.4023,  0.6211,  0.0112, -0.6172, -1.5547,  0.3398, -0.4160,  0.8945,  0.4531,  0.4727, -0.9141]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4531,  1.0781, -0.3555, -1.2031, -4.1562,  6.9062,  1.2812,  0.0332, -5.9688, -9.5625,  0.9375, -1.1641, 14.6250,  5.8438,  1.5156, -6.5625]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.5781,  1.7422, -3.4219, -0.4648, -1.7109,  0.8125, -2.1250,  3.3438, -1.2344, -4.8125,  0.6953,  0.1992,  5.4375, -2.7812,  0.2354, -4.1562]]
-------------------------
name='positions layer 6'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 6'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.5781,  1.7422, -3.4219, -0.4648, -1.7109,  0.8125, -2.1250,  3.3438, -1.2344, -4.8125,  0.6953,  0.1992,  5.4375, -2.7812,  0.2354, -4.1562]]
name='residual layer 6'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4531,  1.0781, -0.3555, -1.2031, -4.1562,  6.9062,  1.2812,  0.0332, -5.9688, -9.5625,  0.9375, -1.1641, 14.6250,  5.8438,  1.5156, -6.5625]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7852,  0.4316, -0.6406, -0.2695, -0.7383,  0.8789, -0.1807,  0.5391, -0.8242, -1.6328,  0.3047, -0.1768,  3.0312,  0.4785,  0.2734, -1.1172]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.0312,   2.8125,  -3.7812,  -1.6719,  -5.8750,   7.7188,  -0.8438,   3.3750,  -7.1875, -14.3750,   1.6328,  -0.9648,  20.0000,   3.0625,   1.7500, -10.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.8711, -0.8359, -0.5820, -0.1514,  0.3105, -1.0078,  1.0547, -0.2793, -0.2207, -0.2910,  0.1592, -0.2930,  2.5156,  0.2373, -0.3887,  0.2500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3672,  0.3438, -1.1484, -0.4375, -0.5859,  0.2734,  0.0669,  0.6641, -0.5586, -1.5703,  0.4121, -0.2852,  1.1875,  0.2305,  0.2656, -0.9766]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.9062,   1.9766,  -4.3750,  -1.8203,  -5.5625,   6.7188,   0.2109,   3.0938,  -7.4062, -14.6875,   1.7891,  -1.2578,  22.5000,   3.2969,   1.3594, -10.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.6875,  1.9141,  1.6484,  1.2031,  6.1562, -8.3750, -0.1011, -0.6523, -4.6250, -8.5000,  2.7031, -0.4199, -0.3711,  1.2266, -4.9062, -6.7188]]
-------------------------
name='positions layer 7'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 7'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.6875,  1.9141,  1.6484,  1.2031,  6.1562, -8.3750, -0.1011, -0.6523, -4.6250, -8.5000,  2.7031, -0.4199, -0.3711,  1.2266, -4.9062, -6.7188]]
name='residual layer 7'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.9062,   1.9766,  -4.3750,  -1.8203,  -5.5625,   6.7188,   0.2109,   3.0938,  -7.4062, -14.6875,   1.7891,  -1.2578,  22.5000,   3.2969,   1.3594, -10.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6797,  0.4883, -0.4180, -0.0869,  0.0588, -0.1367,  0.0198,  0.3242, -1.1875, -2.1719,  0.6875, -0.2412,  2.5000,  0.4434, -0.4727, -1.5391]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.2188,   3.8906,  -2.7188,  -0.6172,   0.5938,  -1.6562,   0.1099,   2.4375, -12.0000, -23.2500,   4.5000,  -1.6797,  22.1250,   4.5312,  -3.5469, -17.2500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7695,  0.2480, -0.5977, -0.1699,  5.3125, -0.0303,  0.2422, -1.2031,  2.3906,  0.0771, -0.0796, -0.2188,  0.6523,  0.1719,  1.1641,  2.4062]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.0391,  0.5547, -0.6797, -0.1494,  0.4961, -0.0557,  0.0850,  0.2041, -0.6211, -2.0625,  0.7539, -0.3262,  1.0156,  0.2656, -0.3652, -1.0859]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -5.0000,   4.1250,  -3.3125,  -0.7891,   5.9062,  -1.6875,   0.3516,   1.2344,  -9.6250, -23.1250,   4.4062,  -1.8984,  22.7500,   4.6875,  -2.3750, -14.8750]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  6.2188,   9.4375,   2.6406,  -0.2246,  -8.8750, -12.8125,  -5.2812,  -3.8125,  -7.9375,  -3.2656,  -5.7500,  -2.3906,   3.9531,  -5.4062,  -1.1016,   3.7969]]
-------------------------
name='positions layer 8'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 8'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  6.2188,   9.4375,   2.6406,  -0.2246,  -8.8750, -12.8125,  -5.2812,  -3.8125,  -7.9375,  -3.2656,  -5.7500,  -2.3906,   3.9531,  -5.4062,  -1.1016,   3.7969]]
name='residual layer 8'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -5.0000,   4.1250,  -3.3125,  -0.7891,   5.9062,  -1.6875,   0.3516,   1.2344,  -9.6250, -23.1250,   4.4062,  -1.8984,  22.7500,   4.6875,  -2.3750, -14.8750]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1396,  1.2969, -0.0723, -0.1006, -0.1846, -0.8398, -0.5820, -0.2539, -1.0938, -1.5547, -0.1338, -0.4141,  1.6641, -0.0464, -0.2852, -0.5859]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.2188,  13.5625,  -0.6719,  -1.0156,  -2.9688, -14.5000,  -4.9375,  -2.5781, -17.5000, -26.3750,  -1.3438,  -4.2812,  26.7500,  -0.7188,  -3.4688, -11.0625]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1875,  0.8008, -1.5156, -2.2188, -0.2559,  8.6250, -1.5703, -0.2129,  1.2344, -0.0248, -1.5312,  1.0859,  4.0938,  0.0977,  0.5234, -0.5234]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0044,  1.3359, -0.3320, -0.4355, -0.1953, -0.1738, -1.0859, -0.3340, -0.8008, -1.6719, -0.3262, -0.3750,  0.9961, -0.0295, -0.3164, -0.6445]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.0312,  14.3750,  -2.1875,  -3.2344,  -3.2188,  -5.8750,  -6.5000,  -2.7969, -16.2500, -26.3750,  -2.8750,  -3.1875,  30.8750,  -0.6211,  -2.9375, -11.5625]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2871, -4.1875, -1.9844,  3.3594,  5.8125, -3.4219, -8.6875, -4.5312,  3.8750, -2.2031,  4.5938,  6.5625,  0.6836,  0.2148,  2.1094,  7.5938]]
-------------------------
name='positions layer 9'       | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 9'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2871, -4.1875, -1.9844,  3.3594,  5.8125, -3.4219, -8.6875, -4.5312,  3.8750, -2.2031,  4.5938,  6.5625,  0.6836,  0.2148,  2.1094,  7.5938]]
name='residual layer 9'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.0312,  14.3750,  -2.1875,  -3.2344,  -3.2188,  -5.8750,  -6.5000,  -2.7969, -16.2500, -26.3750,  -2.8750,  -3.1875,  30.8750,  -0.6211,  -2.9375, -11.5625]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0234,  0.8320, -0.4102,  0.0104,  0.1719, -0.6445, -1.6094, -0.6094, -0.7695, -1.8359,  0.1484,  0.2676,  2.4844, -0.0280, -0.0640, -0.2441]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.2559,  10.1875,  -4.1875,   0.1250,   2.5938,  -9.3125, -15.1875,  -7.3125, -12.3750, -28.6250,   1.7188,   3.3750,  31.5000,  -0.4062,  -0.8281,  -3.9688]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.8789,  0.0991,  2.4688, -2.1250,  1.0156, -8.3750, -0.6992, -6.8125,  2.0781,  5.3750,  0.7422, -0.9141,  4.4062, -0.1729,  1.2578,  3.1875]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0674,  0.8281, -0.2148, -0.2188,  0.1904, -0.4980, -2.0781, -1.3281, -0.4277, -1.2969,  0.2188,  0.2266,  1.0469, -0.0210,  0.0393, -0.0376]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.6250,  10.3125,  -1.7188,  -2.0000,   3.6094, -17.7500, -15.8750, -14.1250, -10.3125, -23.2500,   2.4688,   2.4688,  36.0000,  -0.5781,   0.4297,  -0.7812]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.2500,   1.9766,   1.4062,   2.2969, -13.2500,  -8.6875,  -4.1562,  -4.8438,  -3.2500,  -6.2500,   1.2188,  -4.9375,  -3.1406,   0.2871,   0.2002,  -6.7188]]
-------------------------
name='positions layer 10'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 10'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.2500,   1.9766,   1.4062,   2.2969, -13.2500,  -8.6875,  -4.1562,  -4.8438,  -3.2500,  -6.2500,   1.2188,  -4.9375,  -3.1406,   0.2871,   0.2002,  -6.7188]]
name='residual layer 10'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.6250,  10.3125,  -1.7188,  -2.0000,   3.6094, -17.7500, -15.8750, -14.1250, -10.3125, -23.2500,   2.4688,   2.4688,  36.0000,  -0.5781,   0.4297,  -0.7812]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1953,  0.6523, -0.0203,  0.0178, -0.4688, -1.6172, -1.4844, -1.2188, -0.6523, -1.2734,  0.2217, -0.1484,  2.4062, -0.0170,  0.0396, -0.3242]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.8750,  12.3125,  -0.3125,   0.2969,  -9.6250, -26.5000, -20.0000, -19.0000, -13.5625, -29.5000,   3.6875,  -2.4688,  32.7500,  -0.2910,   0.6289,  -7.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.3125, -0.6211, -1.5156, -1.3594,  2.2344, -4.1875,  2.9375, -1.1328, -0.6914, -1.7188,  2.3750, -1.6250,  1.1719, -0.0840, -1.5078, -1.5391]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.2539,  0.7539, -0.1787, -0.0894, -0.3262, -0.8750, -1.6484, -1.5547, -0.5391, -1.5000,  0.4160, -0.3066,  0.8359, -0.0142, -0.0649, -0.3809]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.1875,  11.6875,  -1.8281,  -1.0625,  -7.3750, -30.7500, -17.0000, -20.1250, -14.2500, -31.2500,   6.0625,  -4.0938,  34.0000,  -0.3750,  -0.8789,  -9.0625]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 7.7812, -2.5938, -0.7617, -1.6094,  7.1250, -4.5938,  0.1143, -1.9922,  1.3672, -6.4062, -0.2480, -1.0156, -0.5352, -0.0796,  5.3750, -4.4688]]
-------------------------
name='positions layer 11'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 11'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 7.7812, -2.5938, -0.7617, -1.6094,  7.1250, -4.5938,  0.1143, -1.9922,  1.3672, -6.4062, -0.2480, -1.0156, -0.5352, -0.0796,  5.3750, -4.4688]]
name='residual layer 11'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.1875,  11.6875,  -1.8281,  -1.0625,  -7.3750, -30.7500, -17.0000, -20.1250, -14.2500, -31.2500,   6.0625,  -4.0938,  34.0000,  -0.3750,  -0.8789,  -9.0625]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.6875,  0.5078, -0.1680, -0.1592, -0.0114, -1.7500, -1.1797, -1.2344, -0.6328, -1.8359,  0.3398, -0.2930,  2.1875, -0.0278,  0.2539, -0.6094]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 11.0000,   9.1250,  -2.5938,  -2.6719,  -0.2500, -35.2500, -16.8750, -22.1250, -12.8750, -37.7500,   5.8125,  -5.1250,  33.5000,  -0.4551,   4.5000, -13.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 2.6875, -3.0000,  0.5039,  3.4844,  0.2148, -6.8438, -2.1094, -1.4297,  1.8828,  0.8867,  7.5000, -3.7812,  5.3438,  0.4434,  2.1562,  1.1328]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     0.8711,      0.3906,     -0.1680,      0.0564,     -0.0014,     -1.1562,     -1.4922,     -1.6172,     -0.4414,     -1.6641,      0.7539,     -0.5508,      1.0312,     -0.0004,      0.4082,     -0.5273]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    13.6875,      6.1250,     -2.0938,      0.8125,     -0.0352,    -42.0000,    -19.0000,    -23.5000,    -11.0000,    -36.7500,     13.3125,     -8.8750,     38.7500,     -0.0117,      6.6562,    -12.3750]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -3.1875,   4.0938,  -1.0234,   0.9922,  13.0000,   4.0000,  -6.2500,  -5.0000, -12.0625,  -1.2500,   3.8125,  -1.4766,   0.0427,  -1.1172,  -3.6250,  -5.6250]]
-------------------------
name='positions layer 12'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 12'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -3.1875,   4.0938,  -1.0234,   0.9922,  13.0000,   4.0000,  -6.2500,  -5.0000, -12.0625,  -1.2500,   3.8125,  -1.4766,   0.0427,  -1.1172,  -3.6250,  -5.6250]]
name='residual layer 12'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    13.6875,      6.1250,     -2.0938,      0.8125,     -0.0352,    -42.0000,    -19.0000,    -23.5000,    -11.0000,    -36.7500,     13.3125,     -8.8750,     38.7500,     -0.0117,      6.6562,    -12.3750]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4824,  0.4609, -0.1309,  0.0645,  0.4707, -1.7812, -1.2266, -1.1094, -0.9375, -1.4922,  0.7422, -0.4043,  1.8438, -0.0415,  0.1128, -0.6055]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 10.5000,  10.2500,  -3.1250,   1.8047,  12.9375, -38.0000, -25.2500, -28.5000, -23.0000, -38.0000,  17.1250, -10.3750,  38.7500,  -1.1250,   3.0312, -18.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.3750, -4.7812, -6.7500,  3.2188,  5.5312, -3.5000,  3.8594,  3.3594,  0.5859, -1.8203,  1.2734,  4.0312,  1.2031, -0.0625,  0.0562, -1.7031]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.6055,  0.2695, -0.6016,  0.2539,  0.6289, -0.9570, -1.1719, -1.2578, -0.8164, -1.6016,  0.7461, -0.2852,  0.9961, -0.0527,  0.1475, -0.7617]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.8750,   5.4688,  -9.8750,   5.0312,  18.5000, -41.5000, -21.3750, -25.1250, -22.3750, -39.7500,  18.3750,  -6.3438,  40.0000,  -1.1875,   3.0938, -19.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.1250,  -6.3438,   0.8516,   9.4375,   7.8125,   2.4844,  -4.7500,  -4.0625,  -1.4062,  -7.8750, -16.5000, -13.5625,   4.9688,  -0.7148,  14.9375,  -2.6094]]
-------------------------
name='positions layer 13'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 13'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.1250,  -6.3438,   0.8516,   9.4375,   7.8125,   2.4844,  -4.7500,  -4.0625,  -1.4062,  -7.8750, -16.5000, -13.5625,   4.9688,  -0.7148,  14.9375,  -2.6094]]
name='residual layer 13'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.8750,   5.4688,  -9.8750,   5.0312,  18.5000, -41.5000, -21.3750, -25.1250, -22.3750, -39.7500,  18.3750,  -6.3438,  40.0000,  -1.1875,   3.0938, -19.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4473, -0.0442, -0.4824,  0.6406,  1.0938, -1.9219, -1.3438, -1.3516, -1.0703, -2.2344,  0.0835, -0.9023,  3.0312, -0.1406,  0.8359, -0.9844]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  9.7500,  -0.8750,  -9.0000,  14.5000,  26.2500, -39.0000, -26.1250, -29.2500, -23.7500, -47.5000,   1.8750, -19.8750,  45.0000,  -1.9062,  18.0000, -22.3750]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-19.6250,  14.0625,   3.1719,  -3.6875,   6.9375,  -9.0625,  -2.3125,  -1.7422,   5.9375,  -5.7812, -12.1875,  -0.8555,   0.2773,   0.5547,   1.1641,  -6.9375]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3457,  0.5859, -0.2910,  0.4824,  1.0391, -1.1953, -1.2578, -1.3203, -0.7344, -2.0781, -0.3477, -0.8125,  1.7500, -0.1021,  0.7773, -1.1719]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -9.8750,  13.1875,  -5.8125,  10.8125,  33.2500, -48.0000, -28.5000, -31.0000, -17.7500, -53.2500, -10.3125, -20.7500,  45.2500,  -1.3516,  19.1250, -29.2500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.3750,  -2.1406,   8.9375,  -3.3125,  -3.7188,   4.8125,   3.0781,   7.0938,  36.0000,   3.0000,  13.2500, -16.5000,   4.6875,  -0.8828,  -7.0000, -21.6250]]
-------------------------
name='positions layer 14'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 14'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -4.3750,  -2.1406,   8.9375,  -3.3125,  -3.7188,   4.8125,   3.0781,   7.0938,  36.0000,   3.0000,  13.2500, -16.5000,   4.6875,  -0.8828,  -7.0000, -21.6250]]
name='residual layer 14'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -9.8750,  13.1875,  -5.8125,  10.8125,  33.2500, -48.0000, -28.5000, -31.0000, -17.7500, -53.2500, -10.3125, -20.7500,  45.2500,  -1.3516,  19.1250, -29.2500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3770,  0.3633,  0.0913,  0.1885,  0.8047, -1.5469, -0.8242, -0.7266,  0.5859, -1.5078,  0.0894, -1.0156,  2.2500, -0.0938,  0.3379, -1.3281]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-14.2500,  11.0625,   3.1250,   7.5000,  29.5000, -43.2500, -25.3750, -23.8750,  18.2500, -50.2500,   2.9375, -37.2500,  50.0000,  -2.2344,  12.1250, -51.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-12.6875,  -8.7500, -13.1875,  -2.8438,   6.1250,  -4.9062,  -2.3750,   5.6250,  -6.0625,  -5.5938,   8.5000,   3.3281,   3.8281,  -0.2246, -15.6875,  -0.0165]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5938,  0.0757, -0.3750,  0.1475,  0.9297, -1.1016, -0.8672, -0.6016,  0.4199, -1.7422,  0.2578, -0.9922,  1.6562, -0.1787, -0.1084, -1.6250]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-27.0000,   2.3125, -10.0625,   4.6562,  35.5000, -48.2500, -27.7500, -18.2500,  12.1875, -55.7500,  11.4375, -34.0000,  53.7500,  -2.4531,  -3.5625, -51.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.5938, -17.7500, -55.7500,  -3.0000, -21.8750,   5.5000,  -1.9141,  -9.2500, -11.1875,  -5.2812,  17.5000,  38.0000,  15.8750,   0.5156,  11.1875,  20.1250]]
-------------------------
name='positions layer 15'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 15'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.5938, -17.7500, -55.7500,  -3.0000, -21.8750,   5.5000,  -1.9141,  -9.2500, -11.1875,  -5.2812,  17.5000,  38.0000,  15.8750,   0.5156,  11.1875,  20.1250]]
name='residual layer 15'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-27.0000,   2.3125, -10.0625,   4.6562,  35.5000, -48.2500, -27.7500, -18.2500,  12.1875, -55.7500,  11.4375, -34.0000,  53.7500,  -2.4531,  -3.5625, -51.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5820, -0.4805, -1.8984,  0.0464,  0.3750, -1.3281, -0.8594, -0.8125,  0.0320, -1.8438,  0.6992,  0.1021,  3.0000, -0.0981,  0.2139, -0.8516]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-21.3750, -15.4375, -66.0000,   1.6562,  13.6250, -42.7500, -29.6250, -27.5000,   1.0000, -61.0000,  29.0000,   4.0000,  69.5000,  -1.9375,   7.6250, -30.8750]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[   -12.6250,      8.1875,      5.8125,     16.3750,     -5.6562,     -4.6875,    -23.0000,     -8.9375,      0.0092,     -6.5625,    -22.3750,      9.0000,      6.1562,      0.2695,     17.5000,      0.2910]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5039, -0.2012, -1.7031,  0.4785,  0.1934, -1.0781, -1.2031, -1.0000,  0.0315, -1.9453,  0.1060,  0.3203,  2.3438, -0.1123,  0.6406, -0.8984]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-34.0000,  -7.2500, -60.2500,  18.0000,   7.9688, -47.5000, -52.5000, -36.5000,   1.0078, -67.5000,   6.6250,  13.0000,  75.5000,  -1.6719,  25.1250, -30.6250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.0000,  10.9375, -29.0000,  57.5000,   3.6875,  -1.7500, -42.5000, -34.7500,   7.5000,   6.2188, -28.6250,  15.0000,   4.1250,  -2.0156,  11.1250, -10.3750]]
-------------------------
name='positions layer 16'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 16'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.0000,  10.9375, -29.0000,  57.5000,   3.6875,  -1.7500, -42.5000, -34.7500,   7.5000,   6.2188, -28.6250,  15.0000,   4.1250,  -2.0156,  11.1250, -10.3750]]
name='residual layer 16'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-34.0000,  -7.2500, -60.2500,  18.0000,   7.9688, -47.5000, -52.5000, -36.5000,   1.0078, -67.5000,   6.6250,  13.0000,  75.5000,  -1.6719,  25.1250, -30.6250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4961,  0.0884, -1.7109,  1.1797,  0.2119, -1.1875, -1.6406, -1.3672,  0.1758, -1.3125, -0.3438,  0.5117,  2.1094, -0.1055,  0.6523, -0.8125]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-35.0000,   3.6875, -89.0000,  75.5000,  11.6250, -49.2500, -95.0000, -71.0000,   8.5000, -61.2500, -22.0000,  28.0000,  79.5000,  -3.6875,  36.2500, -41.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -2.9375,  10.3750,  -5.3125,  -1.7188,   9.0625,   2.0938, -20.7500,  10.6875,  -2.0938,   3.6719,   0.9570,  11.1250,  -1.0469,  -0.7188,  12.5625,  -3.9531]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3691,  0.3164, -2.0156,  1.4688,  0.4492, -1.0078, -1.9453, -1.2500,  0.1777, -1.3672, -0.2490,  0.8164,  2.0781, -0.3789,  0.9297, -1.1406]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -38.0000,   14.0625,  -94.5000,   74.0000,   20.7500,  -47.2500, -116.0000,  -60.2500,    6.4062,  -57.5000,  -21.0000,   39.0000,   78.5000,   -4.4062,   48.7500,  -45.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-10.5625,  11.3750, -30.8750, -45.0000, -15.1875,  11.6875, -94.5000,  -3.4219,   7.4375,  42.5000,  92.0000,   3.0156,  23.3750,  -0.1680, -49.5000,  -9.5000]]
-------------------------
name='positions layer 17'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 17'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-10.5625,  11.3750, -30.8750, -45.0000, -15.1875,  11.6875, -94.5000,  -3.4219,   7.4375,  42.5000,  92.0000,   3.0156,  23.3750,  -0.1680, -49.5000,  -9.5000]]
name='residual layer 17'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -38.0000,   14.0625,  -94.5000,   74.0000,   20.7500,  -47.2500, -116.0000,  -60.2500,    6.4062,  -57.5000,  -21.0000,   39.0000,   78.5000,   -4.4062,   48.7500,  -45.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5781,  0.4551, -1.9453,  0.4746,  0.0977, -0.7773, -3.0469, -1.0391,  0.2891, -0.2676,  0.9492,  0.6836,  3.1094, -0.1973, -0.0118, -0.9805]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -48.5000,   25.5000, -125.5000,   29.0000,    5.5625,  -35.5000, -210.0000,  -63.7500,   13.8750,  -15.0000,   71.0000,   42.0000,  102.0000,   -4.5625,   -0.7500,  -54.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -9.8125,     -1.6562,     -1.6875,     -4.3438,    -13.7500,    -14.5000,    -40.2500,    -19.7500,      2.7500,     -2.5625,     -2.9531,     -0.5156,     -5.8750,      0.0046,     19.5000,     -0.8203]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3633,  0.3574, -1.7891,  0.3223, -0.1270, -0.8828, -2.8125, -1.2656,  0.3203, -0.2930,  0.4805,  0.5391,  2.2656, -0.3438,  0.2334, -1.0078]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -58.2500,   23.8750, -127.0000,   24.6250,   -8.1875,  -50.0000, -250.0000,  -83.5000,   16.6250,  -17.5000,   68.0000,   41.5000,   96.0000,   -4.5625,   18.7500,  -55.2500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-17.2500,  29.2500,  57.2500,   1.9453,  15.6875,   9.5000, -58.7500,  32.5000,  16.1250,  15.0000,  53.7500,  -2.4219,  -3.0469,  -0.9844,  20.3750, -27.2500]]
-------------------------
name='positions layer 18'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 18'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-17.2500,  29.2500,  57.2500,   1.9453,  15.6875,   9.5000, -58.7500,  32.5000,  16.1250,  15.0000,  53.7500,  -2.4219,  -3.0469,  -0.9844,  20.3750, -27.2500]]
name='residual layer 18'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -58.2500,   23.8750, -127.0000,   24.6250,   -8.1875,  -50.0000, -250.0000,  -83.5000,   16.6250,  -17.5000,   68.0000,   41.5000,   96.0000,   -4.5625,   18.7500,  -55.2500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4473,  0.6289, -0.6328,  0.2461,  0.0923, -0.6602, -2.7500, -0.5273,  0.4551, -0.0309,  0.9805,  0.4141,  1.6172, -0.1641,  0.3652, -0.9805]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -75.5000,   53.0000,  -70.0000,   26.6250,    7.5000,  -40.5000, -308.0000,  -51.0000,   32.7500,   -2.5000,  122.0000,   39.0000,   93.0000,   -5.5625,   39.0000,  -82.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-68.0000,   3.2188,   1.6406,   3.2969,   4.5938,  -6.1562,  21.3750, -37.2500, -19.0000, -10.8125,  28.2500,  21.5000,  -3.3906,   1.0938,   0.8047,  -1.1406]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7031,  0.7422, -0.8516,  0.3438,  0.1807, -0.8125, -2.5781, -1.1797,  0.2432, -0.1982,  0.8477,  0.7031,  1.7188, -0.3203,  0.4238, -1.3750]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-144.0000,   56.2500,  -68.5000,   29.8750,   12.1250,  -46.7500, -286.0000,  -88.0000,   13.7500,  -13.3125,  150.0000,   60.5000,   89.5000,   -4.4688,   39.7500,  -83.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -38.2500,   27.8750,   61.5000,  -54.7500,   65.5000,   10.1250, -147.0000,   11.1250,    2.2344,  -37.0000,   59.7500,   39.0000,   11.3750,    0.8672,   -8.9375,   29.6250]]
-------------------------
name='positions layer 19'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 19'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -38.2500,   27.8750,   61.5000,  -54.7500,   65.5000,   10.1250, -147.0000,   11.1250,    2.2344,  -37.0000,   59.7500,   39.0000,   11.3750,    0.8672,   -8.9375,   29.6250]]
name='residual layer 19'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-144.0000,   56.2500,  -68.5000,   29.8750,   12.1250,  -46.7500, -286.0000,  -88.0000,   13.7500,  -13.3125,  150.0000,   60.5000,   89.5000,   -4.4688,   39.7500,  -83.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2344,  0.8125, -0.0610, -0.2197,  0.8516, -0.5391, -3.4844, -0.7188,  0.1924, -0.5273,  1.4297,  0.8438,  1.9062, -0.1270,  0.2578, -0.5547]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-182.0000,   84.0000,   -7.0000,  -24.8750,   77.5000,  -36.5000, -432.0000,  -77.0000,   16.0000,  -50.2500,  210.0000,   99.5000,  101.0000,   -3.5938,   30.7500,  -54.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-137.0000,  -18.3750,   -7.1250,  -10.0000,  -19.8750,   -9.5625,  -73.5000,   24.0000,   13.5625,   17.0000,   -2.1406,   12.5000,    9.4375,   -1.5078,   13.9375,   -6.7812]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2969,  0.6875, -0.1367, -0.3262,  0.7305, -0.7070, -3.5000, -0.5469,  0.4199, -0.4062,  0.9219,  1.0625,  2.0156, -0.3008,  0.3613, -0.7812]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-320.0000,   65.5000,  -14.1250,  -35.0000,   57.5000,  -46.0000, -506.0000,  -53.0000,   29.5000,  -33.2500,  208.0000,  112.0000,  110.5000,   -5.0938,   44.7500,  -60.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -69.5000,   47.0000,   50.5000,  -16.7500,  -16.6250,    6.9688,   23.2500, -106.5000,   33.2500,  120.0000,  151.0000,   10.8125,   17.8750,   -4.4375,   62.2500,    7.3125]]
-------------------------
name='positions layer 20'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 20'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -69.5000,   47.0000,   50.5000,  -16.7500,  -16.6250,    6.9688,   23.2500, -106.5000,   33.2500,  120.0000,  151.0000,   10.8125,   17.8750,   -4.4375,   62.2500,    7.3125]]
name='residual layer 20'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-320.0000,   65.5000,  -14.1250,  -35.0000,   57.5000,  -46.0000, -506.0000,  -53.0000,   29.5000,  -33.2500,  208.0000,  112.0000,  110.5000,   -5.0938,   44.7500,  -60.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.6016,  0.6992,  0.1768, -0.2715,  0.2676, -0.3770, -1.9922, -0.9414,  0.4902,  0.4980,  1.4531,  0.6445,  1.3906, -0.2256,  0.5156, -0.3223]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-390.0000,  112.5000,   36.5000,  -51.7500,   41.0000,  -39.0000, -482.0000, -160.0000,   62.7500,   87.0000,  360.0000,  123.0000,  128.0000,   -9.5000,  107.0000,  -53.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-18.3750,  45.5000,  25.7500,  23.3750, -40.2500, -24.0000, -59.5000, -59.0000,   6.5625,  19.1250, -38.0000,  10.1250,   1.5703,  -0.0850,  39.2500,   6.8125]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3906,  1.2188,  0.4297, -0.1943,  0.0070, -0.8203, -2.7500, -1.7578,  0.7148,  0.9219,  1.1797,  0.9688,  1.7891, -0.3730,  0.8398, -0.4336]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-408.0000,  158.0000,   62.2500,  -28.3750,    0.7500,  -63.0000, -540.0000, -219.0000,   69.5000,  106.0000,  322.0000,  133.0000,  130.0000,   -9.5625,  146.0000,  -46.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-98.5000, -15.6875, -83.5000,  13.8125,  -3.3906,  18.1250, -44.7500, -19.6250, -20.0000,  23.8750, 142.0000,  71.0000,   7.5312,   0.9023, -21.3750,  90.5000]]
-------------------------
name='positions layer 21'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 21'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-98.5000, -15.6875, -83.5000,  13.8125,  -3.3906,  18.1250, -44.7500, -19.6250, -20.0000,  23.8750, 142.0000,  71.0000,   7.5312,   0.9023, -21.3750,  90.5000]]
name='residual layer 21'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-408.0000,  158.0000,   62.2500,  -28.3750,    0.7500,  -63.0000, -540.0000, -219.0000,   69.5000,  106.0000,  322.0000,  133.0000,  130.0000,   -9.5625,  146.0000,  -46.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.3594,  0.8164, -0.1104, -0.0747, -0.0173, -0.4297, -2.7656, -1.4297,  0.3340,  0.8047,  2.3281,  1.1094,  1.6875, -0.2207,  0.5898,  0.2637]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-506.0000,  142.0000,  -21.2500,  -14.5625,   -2.6406,  -45.0000, -584.0000, -239.0000,   49.5000,  130.0000,  464.0000,  204.0000,  138.0000,   -8.6875,  124.5000,   43.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -172.0000,     39.7500,     58.2500,    -18.8750,    -56.5000,    -11.0625,    -21.0000,     53.7500,      9.7500,     16.5000,     68.5000,     32.7500,      0.1641,      0.5977,     33.5000,    -43.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.0312,  1.0312,  0.1963, -0.1826, -0.4082, -0.5234, -2.4375, -1.0781,  0.4395,  0.9492,  1.6953,  1.2266,  1.4062, -0.2129,  0.7109,  0.0051]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-680.0000,  182.0000,   37.0000,  -33.5000,  -59.2500,  -56.0000, -604.0000, -185.0000,   59.2500,  146.0000,  532.0000,  237.0000,  138.0000,   -8.0625,  158.0000,    0.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[109.5000,   9.0000, -52.7500,  82.5000,  86.0000, -67.5000, 136.0000, -46.0000,  -9.6875,  55.7500, 106.0000, -88.0000,   1.3125,  -2.8281,  91.0000, -17.3750]]
-------------------------
name='positions layer 22'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 22'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[109.5000,   9.0000, -52.7500,  82.5000,  86.0000, -67.5000, 136.0000, -46.0000,  -9.6875,  55.7500, 106.0000, -88.0000,   1.3125,  -2.8281,  91.0000, -17.3750]]
name='residual layer 22'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-680.0000,  182.0000,   37.0000,  -33.5000,  -59.2500,  -56.0000, -604.0000, -185.0000,   59.2500,  146.0000,  532.0000,  237.0000,  138.0000,   -8.0625,  158.0000,    0.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3281,  0.6953, -0.0430,  0.1328,  0.1050, -0.7891, -1.2969, -0.7461,  0.1973,  0.6250,  1.7266,  0.4609,  1.0078, -0.1709,  0.6289, -0.0513]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-572.0000,  191.0000,  -15.7500,   49.0000,   26.7500, -123.5000, -468.0000, -231.0000,   49.5000,  202.0000,  640.0000,  149.0000,  139.0000,  -10.8750,  249.0000,  -16.6250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -3.0938,    5.5312,   41.7500,  -19.0000,  -99.5000,  -61.2500, -105.0000, -100.5000,  -40.0000,   52.7500,  -31.0000,   -5.1250,   13.0000,  -21.2500,   76.0000,  -39.0000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2109,  0.6953,  0.0830,  0.0962, -0.3164, -1.0234, -1.5312, -1.1875,  0.0417,  0.9922,  1.3984,  0.4688,  0.9219, -0.4258,  0.9648, -0.2246]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-576.0000,  197.0000,   26.0000,   30.0000,  -73.0000, -185.0000, -572.0000, -332.0000,    9.5000,  255.0000,  608.0000,  144.0000,  152.0000,  -32.0000,  324.0000,  -55.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 139.0000,  -21.5000,   54.2500,  -27.8750, -152.0000,  -76.0000,  -47.0000,  105.0000,  -11.8750,   93.0000,  -19.6250, -152.0000,  -96.0000,   -4.2188,   13.1250, -112.0000]]
-------------------------
name='positions layer 23'      | dtype=torch.int64     | shape=(1,)                 | val = 11
[11]
name='x layer 23'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 139.0000,  -21.5000,   54.2500,  -27.8750, -152.0000,  -76.0000,  -47.0000,  105.0000,  -11.8750,   93.0000,  -19.6250, -152.0000,  -96.0000,   -4.2188,   13.1250, -112.0000]]
name='residual layer 23'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-576.0000,  197.0000,   26.0000,   30.0000,  -73.0000, -185.0000, -572.0000, -332.0000,    9.5000,  255.0000,  608.0000,  144.0000,  152.0000,  -32.0000,  324.0000,  -55.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
name='x after forward'         | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.5625,   3.2656,   1.4219,   0.0403,  -4.5938,  -7.4375, -12.5625,  -4.1250,  -0.0564,   7.4688,  12.9375,  -0.1514,   1.4141,  -1.1250,   7.2188,  -3.7344]]
----------------------------------------------------------------------------------------------------
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
name='[COMPUTE_LOGITS] hidden states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.5625,   3.2656,   1.4219,   0.0403,  -4.5938,  -7.4375, -12.5625,  -4.1250,  -0.0564,   7.4688,  12.9375,  -0.1514,   1.4141,  -1.1250,   7.2188,  -3.7344]]
----------------------------------------------------------------------------------------------------
name='[LOGITS] BEFORE self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.5625,   3.2656,   1.4219,   0.0403,  -4.5938,  -7.4375, -12.5625,  -4.1250,  -0.0564,   7.4688,  12.9375,  -0.1514,   1.4141,  -1.1250,   7.2188,  -3.7344]]
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] x' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.5625,   3.2656,   1.4219,   0.0403,  -4.5938,  -7.4375, -12.5625,  -4.1250,  -0.0564,   7.4688,  12.9375,  -0.1514,   1.4141,  -1.1250,   7.2188,  -3.7344]]
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
name='[UnquantizedEmbeddingMethod] out' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------
name='[LOGITS] AFTER lm_head.quant_method.apply' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
name='[LOGITS] AFTER self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------
name='logits'                  | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
name='input_ids'               | dtype=torch.int32     | shape=(1,)                 | val = 0
name='input_embeds'            | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2988, -0.6953,  1.9141, -0.0649,  0.7461, -1.2031, -0.1865,  0.1436, -6.0000,  1.9844, -0.0967,  0.8711,  2.7969,  6.2812, -0.1177,  0.3008]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1514, -0.3164,  0.7500, -0.0874,  0.3145, -0.2969, -0.1357,  0.1069, -1.7891,  0.7383, -0.1523,  0.4590,  0.9961,  1.6094, -0.1602,  0.2598]]
[after   INPUT_LAYER_NORM] residual is None
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1543, -0.2100,  0.0527,  0.0742, -0.5586, -0.2051,  0.0544, -0.2002, -0.0104, -0.5078,  0.0187, -0.2441,  1.0391, -0.3340,  0.0542, -0.1553]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0938, -0.4453,  0.8906,  0.0183,  0.0732, -0.2197, -0.1533, -0.0635, -0.8867,  0.4805, -0.1650,  0.4375,  0.6484,  0.3262, -0.1128,  0.1387]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1445, -0.9062,  1.9688,  0.0093,  0.1875, -1.4062, -0.1318, -0.0566, -6.0000,  1.4766, -0.0781,  0.6250,  3.8438,  5.9375, -0.0635,  0.1455]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1816,  0.4473, -0.2773,  0.2852, -0.8867, -2.8594, -0.0540, -0.0292,  0.5742, -1.5312,  0.1270,  0.0483,  1.0781, -2.8750, -0.0280,  0.6562]]
-------------------------
name='positions layer 0'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 0'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1816,  0.4473, -0.2773,  0.2852, -0.8867, -2.8594, -0.0540, -0.0292,  0.5742, -1.5312,  0.1270,  0.0483,  1.0781, -2.8750, -0.0280,  0.6562]]
name='residual layer 0'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1445, -0.9062,  1.9688,  0.0093,  0.1875, -1.4062, -0.1318, -0.0566, -6.0000,  1.4766, -0.0781,  0.6250,  3.8438,  5.9375, -0.0635,  0.1455]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2002, -0.2441,  0.7539,  0.3066, -0.2715, -1.0547, -0.1445, -0.0659, -1.8594, -0.0214,  0.0540,  0.4082,  1.8594,  0.9766, -0.0864,  0.5039]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3262, -0.4590,  1.6875,  0.2949, -0.6992, -4.2500, -0.1855, -0.0859, -5.4375, -0.0547,  0.0488,  0.6719,  4.9375,  3.0625, -0.0913,  0.8008]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2578,  0.5938, -0.2402, -0.2598,  0.3711,  1.2812,  0.2178,  0.0942,  1.8672, -1.1797, -0.0256,  0.2637, -1.3672,  0.3242,  0.0884, -0.9727]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4785,  0.0796,  0.8242,  0.0574, -0.1543, -0.4414,  0.0405,  0.0095, -0.5586, -0.4355,  0.0376,  0.7422,  0.4863,  0.1953, -0.0039, -0.1099]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.5859,      0.1348,      1.4453,      0.0352,     -0.3281,     -2.9688,      0.0322,      0.0083,     -3.5625,     -1.2344,      0.0232,      0.9375,      3.5625,      3.3906,     -0.0029,     -0.1719]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.3379,      1.1328,      0.3730,      0.0008,     -1.1016,      2.0625,      0.4590,      0.0923,      1.0156,     -0.9297,      0.5859,      0.4355,     -1.4609,     -0.2402,     -0.1060,      0.6445]]
-------------------------
name='positions layer 1'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 1'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.3379,      1.1328,      0.3730,      0.0008,     -1.1016,      2.0625,      0.4590,      0.0923,      1.0156,     -0.9297,      0.5859,      0.4355,     -1.4609,     -0.2402,     -0.1060,      0.6445]]
name='residual layer 1'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.5859,      0.1348,      1.4453,      0.0352,     -0.3281,     -2.9688,      0.0322,      0.0083,     -3.5625,     -1.2344,      0.0232,      0.9375,      3.5625,      3.3906,     -0.0029,     -0.1719]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.5508,  0.6211,  0.8164,  0.0275, -0.5430, -0.2285,  0.3613,  0.0645, -0.7422, -0.7891,  0.5000,  0.7695,  0.7109,  0.8125, -0.0801,  0.2100]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9219,  1.2656,  1.8203,  0.0359, -1.4297, -0.9062,  0.4922,  0.1006, -2.5469, -2.1562,  0.6094,  1.3750,  2.0938,  3.1562, -0.1089,  0.4727]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2412,  0.7695, -0.0250, -0.2637,  0.2832, -0.2422,  0.0044, -0.3418, -0.0554, -1.5781, -0.3418, -0.1875, -1.6094, -0.2051,  0.1084, -1.4141]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.9844,      1.0703,      1.0938,     -0.2578,     -0.4785,     -0.1455,      0.5508,     -0.2207,     -0.3887,     -1.2344,      0.2949,      0.8828,      0.0452,      0.1592,     -0.0005,     -0.3926]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.1641,      2.0312,      1.7969,     -0.2275,     -1.1484,     -1.1484,      0.4961,     -0.2412,     -2.6094,     -3.7344,      0.2676,      1.1875,      0.4844,      2.9531,     -0.0005,     -0.9414]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1680,  0.6172,  0.1021,  0.2119, -0.2793, -0.5977, -0.1426, -0.0146,  0.7188, -2.1406, -0.0894, -0.0437,  0.9258, -0.1768, -0.0099, -0.4785]]
-------------------------
name='positions layer 2'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 2'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1680,  0.6172,  0.1021,  0.2119, -0.2793, -0.5977, -0.1426, -0.0146,  0.7188, -2.1406, -0.0894, -0.0437,  0.9258, -0.1768, -0.0099, -0.4785]]
name='residual layer 2'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -1.1641,      2.0312,      1.7969,     -0.2275,     -1.1484,     -1.1484,      0.4961,     -0.2412,     -2.6094,     -3.7344,      0.2676,      1.1875,      0.4844,      2.9531,     -0.0005,     -0.9414]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6641,  1.0078,  0.8008, -0.0088, -0.4141, -0.3320,  0.2178, -0.1279, -0.4004, -1.6172,  0.0986,  0.5078,  0.2910,  0.5234, -0.0051, -0.3965]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3281,  2.6562,  1.8984, -0.0156, -1.4297, -1.7500,  0.3535, -0.2559, -1.8906, -5.8750,  0.1777,  1.1406,  1.4062,  2.7812, -0.0104, -1.4219]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0096, -0.1953, -0.2090, -0.0369,  0.5820,  1.6406,  0.0179, -0.1455, -0.7773, -0.6953, -0.0159,  0.1982,  1.0938,  0.0129,  0.3047, -0.7227]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1094,  1.2969,  1.1406, -0.0496, -0.3379, -0.0123,  0.3828, -0.3164, -0.3555, -2.0938,  0.1436,  0.9883,  0.1787,  0.1787,  0.2178, -0.7344]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3359,  2.4688,  1.6875, -0.0525, -0.8477, -0.1094,  0.3711, -0.4023, -2.6719, -6.5625,  0.1621,  1.3359,  2.5000,  2.7969,  0.2949, -2.1406]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0752,  0.8086, -0.2441, -0.3301,  1.2656, -1.0859, -0.2559, -0.5781, -0.2578, -0.9844,  0.0845,  0.1475, -0.8398, -0.0791, -0.2793, -1.2031]]
-------------------------
name='positions layer 3'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 3'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0752,  0.8086, -0.2441, -0.3301,  1.2656, -1.0859, -0.2559, -0.5781, -0.2578, -0.9844,  0.0845,  0.1475, -0.8398, -0.0791, -0.2793, -1.2031]]
name='residual layer 3'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3359,  2.4688,  1.6875, -0.0525, -0.8477, -0.1094,  0.3711, -0.4023, -2.6719, -6.5625,  0.1621,  1.3359,  2.5000,  2.7969,  0.2949, -2.1406]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6680,  1.2500,  0.5234, -0.1758,  0.1143, -0.2148,  0.0583, -0.4395, -0.7109, -1.9922,  0.1167,  0.6055,  0.4531,  0.6836,  0.0066, -0.9023]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4141,  3.2812,  1.4453, -0.3828,  0.4180, -1.1953,  0.1152, -0.9805, -2.9375, -7.5625,  0.2461,  1.4844,  1.6562,  2.7188,  0.0156, -3.3438]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1826,  0.1465,  0.1182,  0.3594, -1.3047, -1.3906, -0.1543, -0.2871,  2.8594, -2.0469, -0.1172, -0.5117,  8.0000,  0.0952,  0.0471, -2.9844]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9805,  1.5625,  1.0625, -0.0182, -0.3047, -0.2578, -0.0359, -0.8516, -0.0115, -2.7500,  0.0962,  0.6484,  0.7344,  0.1904,  0.0396, -1.6953]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2344,  3.4219,  1.5625, -0.0234, -0.8867, -2.5938, -0.0391, -1.2656, -0.0781, -9.6250,  0.1289,  0.9727,  9.6250,  2.8125,  0.0625, -6.3125]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2158, -0.7031, -2.5156, -0.8203,  0.0977,  3.7344,  1.2344,  0.7969,  1.4375, -1.2969,  0.9414,  0.1748,  3.2812,  1.6641, -0.3496, -1.7891]]
-------------------------
name='positions layer 4'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 4'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2158, -0.7031, -2.5156, -0.8203,  0.0977,  3.7344,  1.2344,  0.7969,  1.4375, -1.2969,  0.9414,  0.1748,  3.2812,  1.6641, -0.3496, -1.7891]]
name='residual layer 4'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2344,  3.4219,  1.5625, -0.0234, -0.8867, -2.5938, -0.0391, -1.2656, -0.0781, -9.6250,  0.1289,  0.9727,  9.6250,  2.8125,  0.0625, -6.3125]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4727,  0.6992, -0.2637, -0.2500, -0.1504,  0.1689,  0.4219, -0.1289,  0.2266, -2.0938,  0.3223,  0.3223,  2.1875,  0.5820, -0.0757, -1.4297]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.4531,   2.7188,  -0.9531,  -0.8438,  -0.7891,   1.1406,   1.1953,  -0.4688,   1.3594, -10.9375,   1.0703,   1.1484,  12.8750,   4.4688,  -0.2871,  -8.1250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0354,  0.3281,  0.0187, -0.3926, -0.4336,  2.0469,  0.7344,  0.0454, -1.3906,  1.7344,  0.3203,  0.6250,  1.8984,  0.2676, -0.4727, -2.5312]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6797,  0.8477, -0.3906, -0.5430, -0.2373,  0.1855,  1.0547, -0.1641, -0.0028, -1.5781,  0.5820,  0.7109,  0.7070,  0.2852, -0.2715, -1.6094]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.4141,   3.0469,  -0.9336,  -1.2344,  -1.2188,   3.1875,   1.9297,  -0.4238,  -0.0312,  -9.1875,   1.3906,   1.7734,  14.7500,   4.7500,  -0.7578, -10.6250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1738, -0.1196, -0.1260,  0.5547,  1.9219,  0.7344,  0.3398,  0.6836,  2.7344,  1.4062,  0.0117, -0.4453, -0.8789,  0.2539,  2.2500,  0.5820]]
-------------------------
name='positions layer 5'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 5'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1738, -0.1196, -0.1260,  0.5547,  1.9219,  0.7344,  0.3398,  0.6836,  2.7344,  1.4062,  0.0117, -0.4453, -0.8789,  0.2539,  2.2500,  0.5820]]
name='residual layer 5'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.4141,   3.0469,  -0.9336,  -1.2344,  -1.2188,   3.1875,   1.9297,  -0.4238,  -0.0312,  -9.1875,   1.3906,   1.7734,  14.7500,   4.7500,  -0.7578, -10.6250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.4609,  0.6562, -0.2617, -0.1748,  0.1138,  0.5117,  0.6992,  0.0640,  0.4727, -1.2344,  0.3848,  0.3398,  2.6094,  0.8320,  0.3457, -1.5312]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.5859,   2.9219,  -1.0625,  -0.6797,   0.7031,   3.9219,   2.2656,   0.2598,   2.7031,  -7.7812,   1.4062,   1.3281,  13.8750,   5.0000,   1.4922, -10.0625]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1475,  1.1797,  0.5664, -0.2949, -1.0781, -1.5625,  0.2852, -0.0923, -1.1719, -0.9961, -0.3125, -0.5938, -1.7969,  0.2949, -0.0903, -0.6758]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6328,  1.0859, -0.2021, -0.3770, -0.0645,  0.1406,  1.2656,  0.0576,  0.1611, -1.4531,  0.4043,  0.2676,  0.7500,  0.4180,  0.4473, -1.5156]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.4375,   4.0938,  -0.4961,  -0.9766,  -0.3750,   2.3594,   2.5469,   0.1680,   1.5312,  -8.7500,   1.0938,   0.7344,  12.0625,   5.2812,   1.3984, -10.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4043, -0.2734, -0.4102, -1.1484, -2.9844,  4.2500, -2.3125,  3.4688,  3.7344, -2.4844, -2.5625, -0.5508,  7.4062, -1.7031,  2.8906, -9.3750]]
-------------------------
name='positions layer 6'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 6'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4043, -0.2734, -0.4102, -1.1484, -2.9844,  4.2500, -2.3125,  3.4688,  3.7344, -2.4844, -2.5625, -0.5508,  7.4062, -1.7031,  2.8906, -9.3750]]
name='residual layer 6'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.4375,   4.0938,  -0.4961,  -0.9766,  -0.3750,   2.3594,   2.5469,   0.1680,   1.5312,  -8.7500,   1.0938,   0.7344,  12.0625,   5.2812,   1.3984, -10.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2041,  0.5938, -0.1562, -0.3477, -0.4258,  0.7617,  0.0510,  0.5898,  0.6094, -1.2969, -0.2773,  0.0342,  3.0000,  0.5664,  0.6758, -2.1250]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -1.0312,   3.8125,  -0.9062,  -2.1250,  -3.3594,   6.6250,   0.2344,   3.6406,   5.2500, -11.2500,  -1.4688,   0.1836,  19.5000,   3.5781,   4.2812, -20.1250]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1279, -1.2500, -0.0613,  0.0146,  0.5742, -0.0267, -0.0684,  0.2969, -0.7422,  2.6562,  0.6445,  0.7891,  4.2500,  0.0703, -0.6250, -1.4688]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2539,  0.4492, -0.2578, -0.5117, -0.2969,  0.2715,  0.0532,  0.8516,  0.3438, -0.9297, -0.1914,  0.2227,  1.2578,  0.2578,  0.7227, -2.0312]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.9023,   2.5625,  -0.9688,  -2.1094,  -2.7812,   6.5938,   0.1660,   3.9375,   4.5000,  -8.6250,  -0.8242,   0.9727,  23.7500,   3.6562,   3.6562, -21.6250]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     1.0859,      2.2344,     -1.0000,      0.5703,      2.0156,    -15.4375,      2.3281,     -1.1719,     -7.0000,     -2.2656,      1.4062,      0.0106,     -5.0312,      1.0391,     -4.2500,    -17.8750]]
-------------------------
name='positions layer 7'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 7'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     1.0859,      2.2344,     -1.0000,      0.5703,      2.0156,    -15.4375,      2.3281,     -1.1719,     -7.0000,     -2.2656,      1.4062,      0.0106,     -5.0312,      1.0391,     -4.2500,    -17.8750]]
name='residual layer 7'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.9023,   2.5625,  -0.9688,  -2.1094,  -2.7812,   6.5938,   0.1660,   3.9375,   4.5000,  -8.6250,  -0.8242,   0.9727,  23.7500,   3.6562,   3.6562, -21.6250]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0299,  0.6094, -0.3047, -0.2188, -0.0767, -0.7383,  0.4531,  0.3730, -0.2500, -1.0312,  0.0898,  0.1426,  2.1406,  0.4648, -0.0801, -3.5625]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.1836,   4.8125,  -1.9688,  -1.5391,  -0.7656,  -8.8750,   2.5000,   2.7656,  -2.5000, -10.8750,   0.5820,   0.9844,  18.7500,   4.6875,  -0.5938, -39.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.3242,     -0.8516,      0.5312,      0.1367,      1.8516,     -1.2656,     -0.3652,      1.1562,      2.5000,      0.4238,     -0.0771,     -0.0052,     -6.4688,     -0.1719,      0.3926,      4.6875]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0310,  0.5586, -0.3125, -0.2812,  0.0967, -0.3535,  0.5430,  0.6875,  0.0000, -0.9805,  0.0913,  0.1777,  0.5781,  0.2695, -0.0327, -2.6719]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.1406,   3.9688,  -1.4375,  -1.4062,   1.0859, -10.1250,   2.1406,   3.9219,   0.0000, -10.4375,   0.5039,   0.9805,  12.2500,   4.5000,  -0.2012, -34.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.3906,  3.1406,  2.0000,  0.0258, -1.1094, -7.3125, -1.3438, -7.0000, -1.3906,  1.7734, -2.9219,  0.2256, -3.5469, -0.2236,  1.7266,  0.3496]]
-------------------------
name='positions layer 8'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 8'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.3906,  3.1406,  2.0000,  0.0258, -1.1094, -7.3125, -1.3438, -7.0000, -1.3906,  1.7734, -2.9219,  0.2256, -3.5469, -0.2236,  1.7266,  0.3496]]
name='residual layer 8'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -0.1406,   3.9688,  -1.4375,  -1.4062,   1.0859, -10.1250,   2.1406,   3.9219,   0.0000, -10.4375,   0.5039,   0.9805,  12.2500,   4.5000,  -0.2012, -34.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     0.4238,      0.7734,      0.0693,     -0.1562,     -0.0017,     -1.1484,      0.1069,     -0.3457,     -0.0991,     -0.5820,     -0.2754,      0.1328,      0.6172,      0.3145,      0.1426,     -2.0625]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[     3.2500,      7.1250,      0.5625,     -1.3828,     -0.0234,    -17.5000,      0.7969,     -3.0781,     -1.3906,     -8.6875,     -2.4219,      1.2031,      8.6875,      4.2812,      1.5234,    -34.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.6484,  0.1855,  0.1201, -1.6094, -1.1250,  2.1250, -1.1406, -0.3945, -0.4453, -1.8125, -1.5469, -0.1328,  2.3281, -0.1279, -1.7734, -0.7383]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.2773,  0.8438,  0.1289, -0.5000, -0.0869, -0.5664, -0.0713, -0.5156, -0.1123, -0.8281, -0.5586,  0.1562,  0.4414,  0.2461, -0.0334, -2.4375]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.6016,   7.3125,   0.6836,  -3.0000,  -1.1484, -15.3750,  -0.3438,  -3.4688,  -1.8359, -10.5000,  -3.9688,   1.0703,  11.0000,   4.1562,  -0.2500, -35.2500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6211, -2.1250,  2.0312, -1.9766,  6.8438,  3.8750, -5.9688, -2.1250,  2.1094,  1.6094, -1.3047,  8.8750,  0.1924,  0.2197,  3.0938, -2.3906]]
-------------------------
name='positions layer 9'       | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 9'               | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.6211, -2.1250,  2.0312, -1.9766,  6.8438,  3.8750, -5.9688, -2.1250,  2.1094,  1.6094, -1.3047,  8.8750,  0.1924,  0.2197,  3.0938, -2.3906]]
name='residual layer 9'        | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.6016,   7.3125,   0.6836,  -3.0000,  -1.1484, -15.3750,  -0.3438,  -3.4688,  -1.8359, -10.5000,  -3.9688,   1.0703,  11.0000,   4.1562,  -0.2500, -35.2500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1147,  0.5430,  0.3398, -0.5273,  0.4805, -1.0156, -0.8555, -0.5938,  0.0217, -0.7266, -0.5820,  1.0078,  1.1250,  0.3828,  0.2812, -2.9531]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.9805,   5.1875,   2.7188,  -4.9688,   5.6875, -11.5000,  -6.3125,  -5.5938,   0.2734,  -8.8750,  -5.2812,   9.9375,  11.1875,   4.3750,   2.8438, -37.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 1.4688,  1.2344,  0.0442, -0.9531,  0.1680, -4.7500, -1.6797, -4.3125,  2.2812,  3.0938,  2.9688, -2.1875,  0.6953, -0.3438,  0.5312,  1.9062]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.3340,  0.6523,  0.4355, -0.8125,  0.3887, -0.5781, -1.3047, -1.1641,  0.1338, -0.4043, -0.2559,  0.8945,  0.4375,  0.1836,  0.3887, -2.1719]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.4531,   6.4375,   2.7656,  -5.9375,   5.8438, -16.2500,  -8.0000,  -9.8750,   2.5625,  -5.7812,  -2.3125,   7.7500,  11.8750,   4.0312,   3.3750, -35.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.4062, -2.2344,  4.5000,  0.7148, -6.9375, -6.5000, -1.9531, -5.4688, -1.1094, -1.9297,  2.4844,  5.0312, -8.4375,  0.1953,  3.6562, -4.2812]]
-------------------------
name='positions layer 10'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 10'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.4062, -2.2344,  4.5000,  0.7148, -6.9375, -6.5000, -1.9531, -5.4688, -1.1094, -1.9297,  2.4844,  5.0312, -8.4375,  0.1953,  3.6562, -4.2812]]
name='residual layer 10'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  2.4531,   6.4375,   2.7656,  -5.9375,   5.8438, -16.2500,  -8.0000,  -9.8750,   2.5625,  -5.7812,  -2.3125,   7.7500,  11.8750,   4.0312,   3.3750, -35.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4980,  0.2793,  0.5859, -0.3945, -0.0669, -1.7344, -0.9180, -1.2344,  0.0874, -0.4160,  0.0129,  0.9648,  0.3145,  0.3066,  0.5508, -2.1562]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.8750,   4.1875,   7.2500,  -5.2188,  -1.0938, -22.7500,  -9.9375, -15.3750,   1.4531,  -7.7188,   0.1719,  12.7500,   3.4375,   4.2188,   7.0312, -40.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.9844,  1.4609, -0.8477,  1.4141, -0.1270, -5.7188,  0.9102, -1.9688,  1.1875, -0.2246,  1.5234,  4.7500,  1.1875, -0.0845, -0.3496, -0.5820]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.3750,  0.4414,  0.7578, -0.3887, -0.0654, -0.9766, -1.0625, -1.6172,  0.1216, -0.4609,  0.1396,  1.5781,  0.1377,  0.1895,  0.5938, -2.0781]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.8906,   5.6562,   6.4062,  -3.8125,  -1.2188, -28.5000,  -9.0000, -17.3750,   2.6406,  -7.9375,   1.6953,  17.5000,   4.6250,   4.1250,   6.6875, -40.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 1.9531, -9.0000,  1.1484, -3.2188, -2.9844, -7.8125,  3.4531,  2.0625,  0.5508, -5.7500, -1.3984,  0.8711, -3.4688, -0.4082, -0.3105, -1.0078]]
-------------------------
name='positions layer 11'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 11'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 1.9531, -9.0000,  1.1484, -3.2188, -2.9844, -7.8125,  3.4531,  2.0625,  0.5508, -5.7500, -1.3984,  0.8711, -3.4688, -0.4082, -0.3105, -1.0078]]
name='residual layer 11'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  3.8906,   5.6562,   6.4062,  -3.8125,  -1.2188, -28.5000,  -9.0000, -17.3750,   2.6406,  -7.9375,   1.6953,  17.5000,   4.6250,   4.1250,   6.6875, -40.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.4277, -0.2178,  0.5742, -0.4902, -0.2236, -2.1094, -0.4531, -0.9961,  0.1826, -0.7812,  0.0203,  1.2344,  0.0879,  0.2656,  0.4219, -2.1875]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.8438,  -3.3438,   7.5625,  -7.0312,  -4.1875, -36.2500,  -5.5625, -15.3125,   3.1875, -13.6875,   0.2969,  18.3750,   1.1562,   3.7188,   6.3750, -41.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.2031, -2.4375,  1.4688,  2.4375,  1.9844, -3.1250, -0.1226, -1.6172, -0.0459,  4.1250,  4.5000, -0.6211, -1.8594, -0.4121,  2.4688,  1.4453]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.7109, -0.4551,  0.9023, -0.3945, -0.1128, -1.3359, -0.5547, -1.4297,  0.1562, -0.5312,  0.3379,  1.3594, -0.0231,  0.1406,  0.6719, -2.1094]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  9.0625,  -5.7812,   9.0000,  -4.5938,  -2.2031, -39.5000,  -5.6875, -16.8750,   3.1406,  -9.5625,   4.8125,  17.7500,  -0.7031,   3.3125,   8.8750, -40.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.8750,  -0.7930,  16.5000,  -6.0312,   1.2500,   4.2812, -13.3125,  -7.5938,  -4.6250,  -1.1250, -10.4375,   5.2188,   3.1562,  -0.2266,  -0.0913,  -2.3750]]
-------------------------
name='positions layer 12'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 12'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  5.8750,  -0.7930,  16.5000,  -6.0312,   1.2500,   4.2812, -13.3125,  -7.5938,  -4.6250,  -1.1250, -10.4375,   5.2188,   3.1562,  -0.2266,  -0.0913,  -2.3750]]
name='residual layer 12'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  9.0625,  -5.7812,   9.0000,  -4.5938,  -2.2031, -39.5000,  -5.6875, -16.8750,   3.1406,  -9.5625,   4.8125,  17.7500,  -0.7031,   3.3125,   8.8750, -40.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.8398, -0.3633,  1.3203, -0.4668, -0.0422, -2.0312, -1.1328, -1.1719, -0.0742, -0.5156, -0.2988,  1.0938,  0.1426,  0.1396,  0.4004, -1.7500]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 14.9375,  -6.5625,  25.5000, -10.6250,  -0.9531, -35.2500, -19.0000, -24.5000,  -1.4844, -10.6875,  -5.6250,  23.0000,   2.4531,   3.0938,   8.8125, -42.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 3.8750, -2.2812, -0.6250,  3.3438,  0.7695,  1.5469,  1.1797, -0.5547,  1.9062, -1.9531,  1.3281,  2.4062,  1.1953, -0.2578,  3.9531, -5.1562]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.9766, -0.5195,  1.8203, -0.4375, -0.0075, -0.9258, -1.1719, -1.5000,  0.0184, -0.6094, -0.2070,  1.3672,  0.1089,  0.1504,  0.7266, -2.2031]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 18.7500,  -8.8750,  24.8750,  -7.2812,  -0.1836, -33.7500, -17.8750, -25.0000,   0.4219, -12.6250,  -4.3125,  25.3750,   3.6562,   2.8438,  12.7500, -47.7500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -3.9062,   4.0625,  -1.2734,   5.3125,  -2.5312,  -3.5469,   5.9688,  -5.8438,  -2.5312, -10.0000,   8.0625,  -3.7500,  -0.0299,  -0.6602,   3.2656,  -5.9375]]
-------------------------
name='positions layer 13'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 13'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -3.9062,   4.0625,  -1.2734,   5.3125,  -2.5312,  -3.5469,   5.9688,  -5.8438,  -2.5312, -10.0000,   8.0625,  -3.7500,  -0.0299,  -0.6602,   3.2656,  -5.9375]]
name='residual layer 13'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 18.7500,  -8.8750,  24.8750,  -7.2812,  -0.1836, -33.7500, -17.8750, -25.0000,   0.4219, -12.6250,  -4.3125,  25.3750,   3.6562,   2.8438,  12.7500, -47.7500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.8594, -0.3066,  1.5938, -0.1104, -0.1426, -2.3125, -0.7695, -1.8047, -0.1187, -1.3438,  0.2109,  1.2344,  0.3086,  0.2041,  0.9375, -2.9844]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 14.8750,  -4.8125,  23.6250,  -1.9688,  -2.7188, -37.2500, -11.8750, -30.8750,  -2.1094, -22.6250,   3.7500,  21.6250,   3.6250,   2.1875,  16.0000, -53.7500]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[   -23.5000,      5.8750,      7.0312,     -8.6875,      7.9062,     -8.5000,     -0.0002,      1.0938,      4.1562,     -1.8047,    -16.8750,      1.0938,      3.2969,     -0.0742,      1.5781,      2.5938]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3594,  0.0564,  1.8281, -0.5664,  0.1934, -1.3594, -0.6250, -1.5000,  0.1001, -1.1406, -0.5273,  1.0547,  0.3184,  0.1895,  0.8516, -2.4375]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.6250,   1.0625,  30.6250, -10.6250,   5.1875, -45.7500, -11.8750, -29.7500,   2.0469, -24.3750, -13.1250,  22.7500,   6.9375,   2.1094,  17.6250, -51.2500]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 10.2500,  -8.0625,  11.4375,  -7.6562,  18.5000,   5.2500,  11.0625,  14.8125,   7.1562,  23.3750,  -8.0625,   0.3691,  -7.0312,  -0.5234,   6.4375, -20.0000]]
-------------------------
name='positions layer 14'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 14'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 10.2500,  -8.0625,  11.4375,  -7.6562,  18.5000,   5.2500,  11.0625,  14.8125,   7.1562,  23.3750,  -8.0625,   0.3691,  -7.0312,  -0.5234,   6.4375, -20.0000]]
name='residual layer 14'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -8.6250,   1.0625,  30.6250, -10.6250,   5.1875, -45.7500, -11.8750, -29.7500,   2.0469, -24.3750, -13.1250,  22.7500,   6.9375,   2.1094,  17.6250, -51.2500]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.0510, -0.2734,  1.4453, -0.5430,  0.7695, -1.7188, -0.0312, -0.5391,  0.3516, -0.0356, -0.7656,  0.7461, -0.0050,  0.0791,  0.7969, -2.1875]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  1.6250,  -7.0000,  42.0000, -18.2500,  23.7500, -40.5000,  -0.8125, -14.9375,   9.1875,  -1.0000, -21.2500,  23.1250,  -0.0938,   1.5859,  24.0000, -71.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-14.7500,  -2.8438, -19.0000,   8.6875,   2.4375,  -6.8750,   6.6562,   1.6484,  -5.5312,  -2.4062, -16.6250,   6.0938,   1.6719,  -0.3477,  -4.7812,  -3.3750]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.3281, -0.3652,  0.9766, -0.3457,  0.7773, -1.2344,  0.2080, -0.4980,  0.1426, -0.1211, -0.9766,  0.9727,  0.0554,  0.1025,  0.6641, -2.7031]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-13.1250,  -9.8750,  23.0000,  -9.5625,  26.2500, -47.5000,   5.8438, -13.3125,   3.6562,  -3.4062, -38.0000,  29.2500,   1.5781,   1.2344,  19.2500, -74.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-27.6250,  -4.5625, -28.3750,  -5.9688,  -0.5352,  -0.0874,   2.9062,  10.6875,   8.4375,  -9.5000,  11.3125,  12.7500,   2.7500,  -1.3281,  22.6250,   8.1875]]
-------------------------
name='positions layer 15'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 15'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-27.6250,  -4.5625, -28.3750,  -5.9688,  -0.5352,  -0.0874,   2.9062,  10.6875,   8.4375,  -9.5000,  11.3125,  12.7500,   2.7500,  -1.3281,  22.6250,   8.1875]]
name='residual layer 15'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-13.1250,  -9.8750,  23.0000,  -9.5625,  26.2500, -47.5000,   5.8438, -13.3125,   3.6562,  -3.4062, -38.0000,  29.2500,   1.5781,   1.2344,  19.2500, -74.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3281, -0.5391, -0.1865, -0.5195,  0.8516, -1.7734,  0.3047, -0.0928,  0.4629, -0.4648, -0.7734,  1.2812,  0.2236, -0.0056,  1.4062, -2.1875]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-40.7500, -14.4375,  -5.3750, -15.5000,  25.7500, -47.5000,   8.7500,  -2.6250,  12.1250, -12.8750, -26.7500,  42.0000,   4.3125,  -0.0938,  42.0000, -66.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-42.7500,   6.5938,  10.5000,  16.0000,  -5.0625,  -6.5938, -17.7500, -17.0000,   4.8750, -15.3125,  -6.4062,  11.4375,   9.0625,   0.1816,  15.1250,   1.4453]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.4531, -0.2559,  0.1699,  0.0156,  0.5859, -1.4453, -0.2422, -0.6289,  0.6211, -0.9531, -0.6211,  1.5547,  0.4863,  0.0070,  1.7188, -2.2500]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-83.5000,  -7.8438,   5.1250,   0.5000,  20.7500, -54.0000,  -9.0000, -19.6250,  17.0000, -28.2500, -33.2500,  53.5000,  13.3750,   0.0879,  57.0000, -65.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.6250,   5.8750, -18.0000,  21.3750,  21.6250,   9.1250, -46.0000, -10.0000,  -3.2812,  16.1250,  12.5000,  18.7500,  -2.9688,  -0.6055,   1.9922, -14.6875]]
-------------------------
name='positions layer 16'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 16'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 13.6250,   5.8750, -18.0000,  21.3750,  21.6250,   9.1250, -46.0000, -10.0000,  -3.2812,  16.1250,  12.5000,  18.7500,  -2.9688,  -0.6055,   1.9922, -14.6875]]
name='residual layer 16'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-83.5000,  -7.8438,   5.1250,   0.5000,  20.7500, -54.0000,  -9.0000, -19.6250,  17.0000, -28.2500, -33.2500,  53.5000,  13.3750,   0.0879,  57.0000, -65.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1250, -0.0535, -0.2793,  0.3887,  0.8789, -1.2266, -1.0781, -0.6484,  0.3223, -0.2930, -0.3691,  1.5078,  0.3145, -0.0167,  1.2109, -1.8047]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-70.0000,  -1.9688, -12.8750,  21.8750,  42.5000, -45.0000, -55.0000, -29.6250,  13.7500, -12.1250, -20.7500,  72.0000,  10.3750,  -0.5156,  59.0000, -79.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.9453,  12.8750, -20.8750,   0.0747,   4.9062,   1.8047, -25.0000,  14.1875,  -6.3438,   5.1875,  15.1250,  12.5000,   1.2344,  -0.5234,   9.1875,   0.9883]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.7578,  0.2754, -0.8086,  0.4883,  1.1562, -1.0312, -1.5156, -0.3594,  0.2305, -0.1855, -0.0752,  1.9688,  0.3457, -0.1001,  1.4609, -2.2344]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-69.0000,  10.8750, -33.7500,  22.0000,  47.5000, -43.2500, -80.0000, -15.4375,   7.4062,  -6.9375,  -5.6250,  84.5000,  11.6250,  -1.0391,  68.0000, -78.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -13.2500,    2.5938,   -4.8125,  -18.6250,   -2.7031,    2.4531, -115.5000,    6.5625,    8.2500,   72.0000,    4.5625,  -15.8125,   19.1250,   -1.3281,   -6.0312,  -20.6250]]
-------------------------
name='positions layer 17'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 17'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -13.2500,    2.5938,   -4.8125,  -18.6250,   -2.7031,    2.4531, -115.5000,    6.5625,    8.2500,   72.0000,    4.5625,  -15.8125,   19.1250,   -1.3281,   -6.0312,  -20.6250]]
name='residual layer 17'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-69.0000,  10.8750, -33.7500,  22.0000,  47.5000, -43.2500, -80.0000, -15.4375,   7.4062,  -6.9375,  -5.6250,  84.5000,  11.6250,  -1.0391,  68.0000, -78.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1484,  0.2832, -0.6992,  0.0649,  0.9219, -1.0469, -3.3125, -0.1699,  0.3828,  1.3672, -0.0166,  1.3125,  1.0938, -0.1196,  1.1406, -2.0781]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ -82.0000,   13.5000,  -38.5000,    3.3750,   44.7500,  -40.7500, -196.0000,   -8.8750,   15.6250,   65.0000,   -1.0625,   68.5000,   30.7500,   -2.3750,   62.0000,  -99.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[   -46.0000,      8.6250,     -6.3438,    -21.8750,    -25.0000,    -23.2500,     -4.4688,     -7.4375,    -13.1250,     20.0000,     25.8750,      0.9648,     -4.0000,     -0.0112,     17.1250,      4.1562]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.8828,  0.3652, -0.6953, -0.2676,  0.3379, -1.2578, -2.4844, -0.2734,  0.0532,  1.5625,  0.1924,  1.0000,  0.6992, -0.1982,  1.0859, -1.9141]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-128.0000,   22.1250,  -44.7500,  -18.5000,   19.7500,  -64.0000, -200.0000,  -16.2500,    2.5000,   85.0000,   24.7500,   69.5000,   26.7500,   -2.3906,   79.0000,  -95.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-15.1875,   6.9688,  20.0000,  -9.0625,   7.9375,   8.7500, -18.5000,  22.1250,  13.9375,   6.2188, -14.3125,   0.8359,  -0.4922,   0.0654,   7.5625, -16.3750]]
-------------------------
name='positions layer 18'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 18'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-15.1875,   6.9688,  20.0000,  -9.0625,   7.9375,   8.7500, -18.5000,  22.1250,  13.9375,   6.2188, -14.3125,   0.8359,  -0.4922,   0.0654,   7.5625, -16.3750]]
name='residual layer 18'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-128.0000,   22.1250,  -44.7500,  -18.5000,   19.7500,  -64.0000, -200.0000,  -16.2500,    2.5000,   85.0000,   24.7500,   69.5000,   26.7500,   -2.3906,   79.0000,  -95.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.9219,  0.3770, -0.2441, -0.2793,  0.3730, -0.9883, -2.1250,  0.0659,  0.2500,  1.2344,  0.0918,  0.8164,  0.5000, -0.0747,  0.8828, -1.4375]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-143.0000,   29.1250,  -24.7500,  -27.5000,   27.7500,  -55.2500, -218.0000,    5.8750,   16.5000,   91.0000,   10.4375,   70.5000,   26.2500,   -2.3281,   86.5000, -111.5000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-58.2500,   9.5625,  -0.5742,  -3.7812,   3.3906,  -3.5938,  19.1250, -36.7500, -34.2500, -12.1250,   6.6875,  26.5000,   1.9922,   1.5312, -10.5625,  -2.4219]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.0625,  0.5469, -0.3359, -0.3867,  0.5000, -1.1016, -1.9141, -0.4395, -0.3359,  1.2578,  0.1040,  1.2031,  0.5820, -0.0613,  0.8633, -2.0000]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-201.0000,   38.7500,  -25.3750,  -31.2500,   31.1250,  -58.7500, -199.0000,  -30.8750,  -17.7500,   79.0000,   17.1250,   97.0000,   28.2500,   -0.7969,   76.0000, -114.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-37.2500, -19.0000, -28.3750,  20.1250,  63.7500,  -4.0312, -47.0000, -49.7500,  -7.0938, -44.2500,  39.0000,  32.2500,   5.5312,   1.1484,  13.5000,   6.8750]]
-------------------------
name='positions layer 19'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 19'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-37.2500, -19.0000, -28.3750,  20.1250,  63.7500,  -4.0312, -47.0000, -49.7500,  -7.0938, -44.2500,  39.0000,  32.2500,   5.5312,   1.1484,  13.5000,   6.8750]]
name='residual layer 19'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-201.0000,   38.7500,  -25.3750,  -31.2500,   31.1250,  -58.7500, -199.0000,  -30.8750,  -17.7500,   79.0000,   17.1250,   97.0000,   28.2500,   -0.7969,   76.0000, -114.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.8125,  0.2139, -0.5234, -0.1104,  1.1719, -1.0391, -2.2188, -0.8438, -0.3359,  0.4082,  0.4258,  1.2266,  0.7148,  0.0139,  0.8359, -1.2344]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-238.0000,   19.7500,  -53.7500,  -11.1250,   95.0000,  -62.7500, -246.0000,  -80.5000,  -24.8750,   34.7500,   56.0000,  129.0000,   33.7500,    0.3516,   89.5000, -107.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -121.5000,     -1.9453,    -22.6250,    -17.1250,    -27.2500,    -13.9375,    -74.0000,     20.5000,     18.0000,     -4.0000,     45.2500,      8.0625,     18.2500,     -0.1187,     35.2500,     -6.6250]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.6016,  0.2051, -0.8125, -0.2910,  0.9453, -1.2891, -2.4219, -0.6797, -0.1074,  0.4121,  0.4941,  1.4297,  1.0469,  0.0152,  1.1094, -1.6016]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -360.0000,     17.7500,    -76.5000,    -28.2500,     68.0000,    -76.5000,   -320.0000,    -60.0000,     -6.8750,     30.7500,    101.0000,    137.0000,     52.0000,      0.2324,    125.0000,   -113.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  55.0000,   13.1875,    4.9375,   66.0000,  -66.0000,   21.2500,   -6.7500, -141.0000,   42.5000,   75.5000,   64.0000,  -57.2500,   21.1250,   -1.6641,   59.2500,   -2.6094]]
-------------------------
name='positions layer 20'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 20'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  55.0000,   13.1875,    4.9375,   66.0000,  -66.0000,   21.2500,   -6.7500, -141.0000,   42.5000,   75.5000,   64.0000,  -57.2500,   21.1250,   -1.6641,   59.2500,   -2.6094]]
name='residual layer 20'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -360.0000,     17.7500,    -76.5000,    -28.2500,     68.0000,    -76.5000,   -320.0000,    -60.0000,     -6.8750,     30.7500,    101.0000,    137.0000,     52.0000,      0.2324,    125.0000,   -113.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2969,  0.1982, -0.3613,  0.2041,  0.0135, -0.5508, -1.3906, -1.2266,  0.2871,  0.6328,  0.6914,  0.4336,  0.8203, -0.0352,  0.9180, -0.7227]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-304.0000,   31.0000,  -71.5000,   37.7500,    2.0000,  -55.2500, -326.0000, -201.0000,   35.5000,  106.0000,  165.0000,   80.0000,   73.0000,   -1.4297,  184.0000, -116.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  0.3184,  47.2500,  23.1250,  26.5000, -38.2500, -16.1250, -58.2500, -46.0000,  -3.5938,   4.2188, -36.5000,   4.4062,  -0.5000,  -0.3594,  37.0000,  18.5000]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.1328,  0.6641, -0.3691,  0.4863, -0.3711, -1.0156, -2.1406, -2.1875,  0.3613,  1.0547,  0.5195,  0.6758,  1.1016, -0.0767,  1.3984, -1.0000]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-304.0000,   78.0000,  -48.5000,   64.0000,  -36.2500,  -71.5000, -384.0000, -247.0000,   31.8750,  110.0000,  128.0000,   84.5000,   72.5000,   -1.7891,  221.0000,  -97.5000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -107.0000,     55.2500,     -8.4375,     25.7500,    -28.5000,     36.0000,      8.9375,     67.0000,    -20.7500,      4.1250,     79.5000,     -9.5000,      6.3125,      0.0269,    -24.2500,     75.5000]]
-------------------------
name='positions layer 21'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 21'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -107.0000,     55.2500,     -8.4375,     25.7500,    -28.5000,     36.0000,      8.9375,     67.0000,    -20.7500,      4.1250,     79.5000,     -9.5000,      6.3125,      0.0269,    -24.2500,     75.5000]]
name='residual layer 21'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-304.0000,   78.0000,  -48.5000,   64.0000,  -36.2500,  -71.5000, -384.0000, -247.0000,   31.8750,  110.0000,  128.0000,   84.5000,   72.5000,   -1.7891,  221.0000,  -97.5000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-2.0312,  0.8125, -0.3105,  0.4863, -0.4492, -0.3613, -1.8750, -1.1406,  0.0796,  0.7500,  1.1016,  0.4316,  1.0234, -0.0474,  0.9805, -0.1406]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-412.0000,  133.0000,  -57.0000,   90.0000,  -65.0000,  -35.5000, -376.0000, -180.0000,   11.1250,  114.0000,  208.0000,   75.0000,   79.0000,   -1.7656,  197.0000,  -22.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-156.0000,    0.2949,   30.8750,  -25.5000,  -28.1250,   -8.1875,   21.8750,   30.6250,   21.3750,   14.3750,   98.0000,   18.1250,    3.1406,    1.2422,   27.7500,  -42.2500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.8906,  0.8398, -0.1533,  0.3887, -0.7109, -0.4531, -1.5859, -0.9688,  0.2656,  0.9180,  1.0781,  0.5352,  0.9219, -0.0153,  1.1172, -0.4824]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -568.0000,    133.0000,    -26.1250,     64.5000,    -93.0000,    -43.7500,   -354.0000,   -149.0000,     32.5000,    128.0000,    306.0000,     93.0000,     82.0000,     -0.5234,    225.0000,    -64.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  38.7500,   16.5000,  -92.5000,  -24.7500,   40.2500,  -34.2500,   68.0000,  -43.7500,   22.6250,    9.4375,   11.0000, -155.0000,   25.1250,   -2.2344,  123.5000,    9.1250]]
-------------------------
name='positions layer 22'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 22'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  38.7500,   16.5000,  -92.5000,  -24.7500,   40.2500,  -34.2500,   68.0000,  -43.7500,   22.6250,    9.4375,   11.0000, -155.0000,   25.1250,   -2.2344,  123.5000,    9.1250]]
name='residual layer 22'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -568.0000,    133.0000,    -26.1250,     64.5000,    -93.0000,    -43.7500,   -354.0000,   -149.0000,     32.5000,    128.0000,    306.0000,     93.0000,     82.0000,     -0.5234,    225.0000,    -64.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.3203,  0.5781, -0.3438,  0.1152, -0.2207, -0.5312, -0.8398, -0.6680,  0.2324,  0.4531,  0.9180, -0.2041,  0.8242, -0.0461,  0.9375, -0.1797]]
name='[after   INPUT_LAYER_NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-528.0000,  150.0000, -118.5000,   39.7500,  -52.7500,  -78.0000, -286.0000, -193.0000,   55.0000,  137.0000,  316.0000,  -62.0000,  107.0000,   -2.7500,  348.0000,  -55.0000]]
-------------------------
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[  -7.9062,    6.2500,   27.2500,  -30.1250,  -70.5000,  -59.2500, -103.5000,  -61.0000,  -32.2500,   37.0000,  -26.1250,    4.4688,   24.2500,  -21.2500,   54.7500,  -23.2500]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-1.2656,  0.6172, -0.3262,  0.0347, -0.5977, -0.8516, -1.1641, -1.0078,  0.1113,  0.7578,  0.7422, -0.2109,  0.8867, -0.3574,  1.3359, -0.3535]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-536.0000,  156.0000,  -91.0000,    9.6250, -123.0000, -137.0000, -390.0000, -254.0000,   22.7500,  174.0000,  290.0000,  -57.5000,  131.0000,  -24.0000,  402.0000,  -78.0000]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 54.5000, -16.8750,  77.5000,   6.0000, -51.0000, -23.1250,  -7.2500, -18.6250,  -9.5625,  41.7500, -31.6250, -71.5000, -70.5000, -22.5000, -52.2500, -47.7500]]
-------------------------
name='positions layer 23'      | dtype=torch.int64     | shape=(1,)                 | val = 12
[12]
name='x layer 23'              | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 54.5000, -16.8750,  77.5000,   6.0000, -51.0000, -23.1250,  -7.2500, -18.6250,  -9.5625,  41.7500, -31.6250, -71.5000, -70.5000, -22.5000, -52.2500, -47.7500]]
name='residual layer 23'       | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-536.0000,  156.0000,  -91.0000,    9.6250, -123.0000, -137.0000, -390.0000, -254.0000,   22.7500,  174.0000,  290.0000,  -57.5000,  131.0000,  -24.0000,  402.0000,  -78.0000]]
--------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
name='x after forward'         | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-11.0625,   3.0469,  -0.2793,   0.3457,  -4.1875,  -5.3438,  -9.4375,  -5.7812,   0.3691,   5.4062,   6.6562,  -2.8594,   1.7969,  -1.6953,   8.7500,  -3.2812]]
----------------------------------------------------------------------------------------------------
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
name='[COMPUTE_LOGITS] hidden states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-11.0625,   3.0469,  -0.2793,   0.3457,  -4.1875,  -5.3438,  -9.4375,  -5.7812,   0.3691,   5.4062,   6.6562,  -2.8594,   1.7969,  -1.6953,   8.7500,  -3.2812]]
----------------------------------------------------------------------------------------------------
name='[LOGITS] BEFORE self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-11.0625,   3.0469,  -0.2793,   0.3457,  -4.1875,  -5.3438,  -9.4375,  -5.7812,   0.3691,   5.4062,   6.6562,  -2.8594,   1.7969,  -1.6953,   8.7500,  -3.2812]]
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
name='[UnquantizedEmbeddingMethod] x' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-11.0625,   3.0469,  -0.2793,   0.3457,  -4.1875,  -5.3438,  -9.4375,  -5.7812,   0.3691,   5.4062,   6.6562,  -2.8594,   1.7969,  -1.6953,   8.7500,  -3.2812]]
name='[UnquantizedEmbeddingMethod] weight' | dtype=torch.bfloat16  | shape=(201088, 2880)       | 
[[    -0.0060,      0.0007,      0.0013,      0.0022,     -0.0041,     -0.0044,     -0.0015,     -0.0018,      0.0017,      0.0035,      0.0069,      0.0022,      0.0081,      0.0020,     -0.0030,      0.0027],
        [    -0.0135,     -0.0019,     -0.0039,      0.0056,      0.0030,     -0.0011,      0.0046,     -0.0022,     -0.0020,     -0.0001,      0.0051,      0.0052,     -0.0013,      0.0003,     -0.0023,     -0.0008],
        [    -0.0045,     -0.0077,      0.0067,     -0.0057,     -0.0029,     -0.0089,      0.0044,      0.0028,      0.0041,     -0.0014,      0.0182,      0.0032,      0.0026,      0.0018,      0.0057,     -0.0004],
        [    -0.0012,     -0.0049,     -0.0005,      0.0167,     -0.0077,     -0.0037,     -0.0036,      0.0052,      0.0091,      0.0036,      0.0052,     -0.0009,     -0.0096,      0.0013,      0.0065,      0.0049],
        [     0.0030,     -0.0059,      0.0077,      0.0042,     -0.0052,     -0.0040,     -0.0067,      0.0048,     -0.0041,      0.0051,      0.0146,      0.0031,     -0.0037,      0.0003,      0.0042,     -0.0012],
        [    -0.0028,     -0.0085,     -0.0014,     -0.0040,     -0.0040,     -0.0036,     -0.0010,     -0.0037,     -0.0026,     -0.0023,     -0.0077,      0.0081,      0.0044,     -0.0031,      0.0048,      0.0101],
        [    -0.0177,      0.0005,     -0.0038,      0.0021,     -0.0028,     -0.0045,     -0.0010,     -0.0049,      0.0059,     -0.0019,      0.0024,      0.0090,     -0.0001,      0.0015,     -0.0032,      0.0011],
        [    -0.0126,     -0.0056,      0.0080,      0.0079,     -0.0018,     -0.0010,      0.0001,      0.0025,      0.0000,     -0.0093,      0.0026,      0.0149,      0.0031,     -0.0028,     -0.0063,      0.0050],
        [    -0.0039,      0.0019,     -0.0051,      0.0017,      0.0011,     -0.0042,     -0.0041,      0.0011,      0.0051,     -0.0040,     -0.0028,      0.0059,     -0.0007,     -0.0002,     -0.0006,      0.0005],
        [     0.0009,     -0.0028,     -0.0041,      0.0058,     -0.0048,      0.0029,      0.0044,     -0.0033,      0.0074,      0.0040,      0.0064,      0.0106,     -0.0026,     -0.0006,     -0.0008,      0.0042],
        [     0.0069,     -0.0069,      0.0074,      0.0090,     -0.0071,     -0.0041,     -0.0034,      0.0034,      0.0004,     -0.0045,      0.0011,      0.0031,     -0.0001,      0.0002,      0.0063,      0.0028],
        [     0.0054,      0.0021,     -0.0005,     -0.0030,     -0.0016,     -0.0011,     -0.0016,     -0.0015,     -0.0012,      0.0012,      0.0042,      0.0035,      0.0014,     -0.0004,      0.0018,     -0.0018],
        [     0.0109,     -0.0040,      0.0066,      0.0012,     -0.0017,     -0.0034,     -0.0005,     -0.0012,      0.0030,     -0.0027,     -0.0032,      0.0075,      0.0002,     -0.0003,      0.0034,      0.0032],
        [    -0.0125,      0.0039,      0.0003,      0.0010,     -0.0040,     -0.0036,      0.0020,      0.0011,      0.0031,     -0.0008,      0.0057,      0.0041,      0.0035,     -0.0008,      0.0027,      0.0001],
        [    -0.0128,     -0.0012,     -0.0038,      0.0028,      0.0024,     -0.0010,     -0.0026,      0.0044,      0.0065,      0.0040,     -0.0135,      0.0105,     -0.0023,      0.0015,      0.0019,      0.0023],
        [    -0.0003,     -0.0035,      0.0033,      0.0019,      0.0023,     -0.0063,      0.0066,     -0.0001,     -0.0010,      0.0004,      0.0095,      0.0079,      0.0020,      0.0004,      0.0008,     -0.0049]]
bias is None
name='[UnquantizedEmbeddingMethod] out' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------
name='[LOGITS] AFTER lm_head.quant_method.apply' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
name='[LOGITS] AFTER self._get_logits(hidden_states, lm_head, embedding_bias)' | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------
name='logits'                  | dtype=torch.bfloat16  | shape=(1, 201088)          | 
[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
name='input_ids'               | dtype=torch.int32     | shape=(1,)                 | val = 0
name='input_embeds'            | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.2988, -0.6953,  1.9141, -0.0649,  0.7461, -1.2031, -0.1865,  0.1436, -6.0000,  1.9844, -0.0967,  0.8711,  2.7969,  6.2812, -0.1177,  0.3008]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
name='[after   INPUT_LAYER_NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1514, -0.3164,  0.7500, -0.0874,  0.3145, -0.2969, -0.1357,  0.1069, -1.7891,  0.7383, -0.1523,  0.4590,  0.9961,  1.6094, -0.1602,  0.2598]]
[after   INPUT_LAYER_NORM] residual is None
name='[after   ATTENTION] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[ 0.1484, -0.1357,  0.0469,  0.0684, -0.5352, -0.3418,  0.0659, -0.1797, -0.0850, -0.5938,  0.0086, -0.2432,  1.0156, -0.4199,  0.0596, -0.0854]]
-------------------------
name='[after  _POST_ATTENTION NORM] hidden_states' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.0972, -0.4102,  0.8867,  0.0067,  0.0825, -0.2393, -0.1396, -0.0405, -0.8906,  0.4531, -0.1875,  0.4375,  0.6445,  0.3223, -0.1025,  0.2041]]
name='[after  _POST_ATTENTION NORM] residual' | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[    -0.1504,     -0.8320,      1.9609,      0.0034,      0.2109,     -1.5469,     -0.1206,     -0.0361,     -6.0938,      1.3906,     -0.0879,      0.6289,      3.8125,      5.8750,     -0.0581,      0.2148]]
-------------------------
name='[after  MLP] output'     | dtype=torch.bfloat16  | shape=(1, 2880)            | 
[[-0.1729,  0.4434, -0.2754,  0.2930, -1.0234, -3.2969, -0.0684,  0.0282,  0.7656, -1.6016,  0.0830,  0.0613,  1.3125, -3.4844, -0.0447,  0.6602]]
-------------------------
name='positions layer 0'       | dtype=torch.int64     | shape=(1,)                 | val = 13
[13]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
====================================================================================================
>>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 0
[after   INPUT_LAYER_NORM] residual is None
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 2
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 3
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 4
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 5
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 6
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 7
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 8
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 9
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 10
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 11
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 12
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 13
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 14
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 15
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 16
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 17
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 18
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 19
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 20
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 21
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 22
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> START Layer 23
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> END Layer 23
Compute logits:: hidden_states.shape = torch.Size([1, 2880]), sample_hidden_states.shape = torch.Size([1, 2880]), logits_indices = tensor([0], device='cuda:0', dtype=torch.int32), hidden_states.dtype = torch.bfloat16, sample_hidden_states.dtype = torch.bfloat16
---------------------------------------------------------------------------
sampling_metadata=None
------------------------------
===> CALLING F.linear(x, layer.weight, bias)
bias is None
---------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
>>>>>>>>>>> SAMPLING >>>>>>>>>>
sampler_output.sampled_token_ids = tensor([[0]], device='cuda:0', dtype=torch.int32)
