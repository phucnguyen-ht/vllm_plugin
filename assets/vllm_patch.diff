diff --git a/.gitignore b/.gitignore
index f1fe8d1..88f6237 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1 +1,3 @@
+**/__pycache__
+
 *.so
\ No newline at end of file
diff --git a/attention/ops/chunked_prefill_paged_decode.py b/attention/ops/chunked_prefill_paged_decode.py
old mode 100644
new mode 100755
index cbf863a..ba3643f
--- a/attention/ops/chunked_prefill_paged_decode.py
+++ b/attention/ops/chunked_prefill_paged_decode.py
@@ -24,6 +24,7 @@ def kernel_paged_attention_2d(
         query_ptr,  # [num_tokens, num_query_heads, head_size]
         key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]
         value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]
+        sink_ptr,  # [num_query_heads]
         block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]
         seq_lens_ptr,  # [num_seqs]
         alibi_slopes_ptr,  # [num_query_heads]
@@ -55,6 +56,7 @@ def kernel_paged_attention_2d(
         stride_v_cache_3: tl.constexpr,  # int
         filter_by_query_len: tl.constexpr,  # bool
         query_start_len_ptr,  # [num_seqs+1]
+        USE_SINKS: tl.constexpr,  # bool
 ):
     seq_idx = tl.program_id(0)
     kv_head_idx = tl.program_id(1)
@@ -91,8 +93,23 @@ def kernel_paged_attention_2d(
 
     block_table_offset = seq_idx * block_table_stride
 
-    M = tl.full([num_queries_per_kv_padded], float("-inf"), dtype=tl.float32)
+    ########################################
+    # M = tl.full([num_queries_per_kv_padded], float("-inf"), dtype=tl.float32)
+    # L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)
+    
+    if not USE_SINKS:
+        M = tl.full([num_queries_per_kv_padded],
+                    float("-inf"),
+                    dtype=tl.float32)
+    else:
+        M = tl.load(
+            sink_ptr + query_head_idx,
+            mask=head_mask,
+            other=float("-inf"),
+        ).to(dtype=tl.float32)
     L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)
+    ########################################
+    
     acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED],
                    dtype=tl.float32)
 
@@ -218,6 +235,8 @@ def chunked_prefill_paged_decode(
     alibi_slopes=None,
     sliding_window=None,
     sm_scale=None,
+    # Optional tensor for sinks
+    sinks=None,
 ):
 
     if sm_scale is None:
@@ -247,6 +266,7 @@ def chunked_prefill_paged_decode(
             sliding_window=sliding_window,
             sm_scale=sm_scale,
             skip_decode=True,
+            sinks=sinks,
         )
 
     block_size = value_cache.shape[3]
@@ -283,6 +303,7 @@ def chunked_prefill_paged_decode(
         query_ptr=query,
         key_cache_ptr=key_cache,
         value_cache_ptr=value_cache,
+        sink_ptr=sinks,
         block_tables_ptr=block_table,
         seq_lens_ptr=seq_lens,
         alibi_slopes_ptr=alibi_slopes,
@@ -314,5 +335,6 @@ def chunked_prefill_paged_decode(
         stride_v_cache_3=value_cache.stride(3),
         filter_by_query_len=True,
         query_start_len_ptr=query_start_loc,
-        num_stages=1
+        num_stages=1,
+        USE_SINKS=sinks is not None,
     )
diff --git a/attention/ops/prefix_prefill.py b/attention/ops/prefix_prefill.py
old mode 100644
new mode 100755
index e85ec60..4316904
--- a/attention/ops/prefix_prefill.py
+++ b/attention/ops/prefix_prefill.py
@@ -25,6 +25,7 @@ if triton.__version__ >= "2.1.0":
         V,
         K_cache,
         V_cache,
+        sink_ptr,
         B_Loc,
         sm_scale,
         k_scale,
@@ -65,6 +66,7 @@ if triton.__version__ >= "2.1.0":
         BLOCK_N: tl.constexpr,
         SLIDING_WINDOW: tl.constexpr,
         SKIP_DECODE: tl.constexpr,
+        USE_SINKS: tl.constexpr,
     ):
 
         cur_batch = tl.program_id(0)
@@ -109,8 +111,23 @@ if triton.__version__ >= "2.1.0":
                     other=0.0)  # [M,D]
 
         # initialize pointer to m and l
-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")  # [M]
-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # [M]
+        # m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")  # [M]
+        # l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # [M]
+        
+        ########################################
+        # initialize pointer to m and l
+        if not USE_SINKS:
+            m_i = tl.full([BLOCK_M], float("-inf"), dtype=tl.float32)
+        else:
+            m_i = tl.load(
+                sink_ptr + tl.full([BLOCK_M], cur_head, dtype=tl.int64),
+                mask=(offs_m < cur_batch_query_len),
+                other=float("-inf"),
+            ).to(dtype=tl.float32)
+
+        l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)
+        ########################################
+        
         acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED],
                        dtype=tl.float32)  # [M,D]
 
@@ -731,7 +748,8 @@ if triton.__version__ >= "2.1.0":
                               alibi_slopes=None,
                               sliding_window=None,
                               sm_scale=None,
-                              skip_decode=False):
+                              skip_decode=False,
+                            sinks=None):
 
         q_dtype_is_f32 = q.dtype is torch.float32
         # need to reduce num. blocks when using fp32
@@ -785,6 +803,7 @@ if triton.__version__ >= "2.1.0":
             sliding_window = 0
 
         if alibi_slopes is not None:
+            assert sinks is None, "Sinks arg is not supported with alibi"
             _fwd_kernel_alibi[grid](
                 q,
                 k,
@@ -845,6 +864,7 @@ if triton.__version__ >= "2.1.0":
             v,
             k_cache,
             v_cache,
+            sinks,
             b_loc,
             sm_scale,
             k_scale,
@@ -889,5 +909,6 @@ if triton.__version__ >= "2.1.0":
             SKIP_DECODE=skip_decode,
             num_warps=NUM_WARPS,
             num_stages=1,
+            USE_SINKS=sinks is not None,
         )
         return
diff --git a/attention/selector.py b/attention/selector.py
old mode 100644
new mode 100755
index ebbdea2..2b6a66e
--- a/attention/selector.py
+++ b/attention/selector.py
@@ -145,6 +145,7 @@ def _cached_get_attn_backend(
             selected_backend = backend_name_to_enum(backend_by_env_var)
 
     # get device-specific attn_backend
+    print(f"current_platform.get_attn_backend_cls: {selected_backend = }, {head_size = }, {dtype = }, {kv_cache_dtype = }")
     attention_cls = current_platform.get_attn_backend_cls(
         selected_backend, head_size, dtype, kv_cache_dtype, block_size, use_v1,
         use_mla)
diff --git a/benchmarks/multi_turn/generate_multi_turn.json b/benchmarks/multi_turn/generate_multi_turn.json
new file mode 100755
index 0000000..274d03c
--- /dev/null
+++ b/benchmarks/multi_turn/generate_multi_turn.json
@@ -0,0 +1,35 @@
+{
+    "filetype": "generate_conversations",
+    "num_conversations": 24,
+    "text_files": ["pg1184.txt"],
+    "print_stats": false,
+    "prompt_input": {
+        "num_turns": {
+            "distribution": "uniform",
+            "min": 12,
+            "max": 18
+        },
+        "common_prefix_num_tokens": {
+            "distribution": "constant",
+            "value": 500
+        },
+        "prefix_num_tokens": {
+            "distribution": "lognormal",
+            "mean": 6,
+            "sigma": 4,
+            "max": 1500
+        },
+        "num_tokens": {
+            "distribution": "uniform",
+            "min": 120,
+            "max": 160
+        }
+    },
+    "prompt_output": {
+        "num_tokens": {
+            "distribution": "uniform",
+            "min": 80,
+            "max": 120
+        }
+    }
+}
\ No newline at end of file
diff --git a/benchmarks/structured_schemas/structured_schema_1.json b/benchmarks/structured_schemas/structured_schema_1.json
new file mode 100755
index 0000000..13bd6b6
--- /dev/null
+++ b/benchmarks/structured_schemas/structured_schema_1.json
@@ -0,0 +1,19 @@
+{
+    "type": "object",
+    "properties": {
+      "name": { "type": "string" },
+      "email": { "type": "string" },
+      "street": { "type": "string" },
+      "city": { "type": "string" },
+      "state": { "type": "string" },
+      "zip": { "type": "string" },
+      "phone": { "type": "string" },
+      "website": { "type": "string" },
+      "company": { "type": "string" },
+      "age": { "type": "integer" }
+    },
+    "required": [
+      "name",
+      "email"
+    ]
+}
diff --git a/config.py b/config.py
old mode 100644
new mode 100755
index a2e547c..83f3abd
--- a/config.py
+++ b/config.py
@@ -537,7 +537,7 @@ class ModelConfig:
             ("RewardModel", "reward"),
         ]
         _, arch = self.registry.inspect_model_cls(architectures)
-
+        # print(f'---> Go here: {arch = }')
         for suffix, pref_task in suffix_to_preferred_task:
             if arch.endswith(suffix) and pref_task in supported_tasks:
                 return pref_task
@@ -581,6 +581,11 @@ class ModelConfig:
                     architectures, supported_tasks)
                 if preferred_task is not None:
                     selected_task = preferred_task
+                    
+                # print(f'-' * 50)
+                # print(f'{selected_task = }, {preferred_task = }, {architectures = }')
+                # print(f'{runner_support = }, {supported_runner_types_lst = }, {supported_tasks_lst = }')
+                # print(f'-' * 50)
 
                 logger.info(
                     "This model supports multiple tasks: %s. "
@@ -1159,7 +1164,7 @@ class CacheConfig:
         elif self.cache_dtype in ("fp8", "fp8_e4m3", "fp8_e5m2"):
             logger.info(
                 "Using fp8 data type to store kv cache. It reduces the GPU "
-                "memory footprint and boosts the performance. "
+                # "memory footprint and boosts the performance. "
                 "Meanwhile, it may cause accuracy drop without a proper "
                 "scaling factor")
         else:
@@ -1909,7 +1914,7 @@ class SpeculativeConfig:
         - enable_chunked_prefill (bool): Whether vLLM is configured to use
             chunked prefill or not. Used for raising an error since it's not
             yet compatible with speculative decode.
-        - disable_log_stats (bool): Whether to disable the periodic printing of
+        # - disable_log_stats (bool): Whether to disable the periodic printing of
             stage times in speculative decoding.
     """
     # speculative configs from cli args
@@ -2542,7 +2547,12 @@ def _get_and_verify_dtype(
     # NOTE: getattr(config, "torch_dtype", torch.float32) is not correct
     # because config.torch_dtype can be None.
     config_dtype = getattr(config, "torch_dtype", None)
-
+    # print(f'-' * 50)
+    # print(f"{config = }")
+    # print(f"{dtype = }")
+    # print(f"{config_dtype = }")
+    # print(f'-' * 50)
+    
     # Fallbacks for multi-modal models if the root config
     # does not define torch_dtype
     if config_dtype is None and hasattr(config, "text_config"):
diff --git a/debug.py b/debug.py
new file mode 100644
index 0000000..ea14538
--- /dev/null
+++ b/debug.py
@@ -0,0 +1,68 @@
+import torch
+import re
+
+FILE_LOG = "/home/tester/phucnguyen/hygon-test/dev/vllm_plugin_vllmm_0.8.2/logs/log.log" 
+import os
+if os.path.exists(FILE_LOG):
+    os.remove(FILE_LOG)
+    print(f"Deleted old log file: {FILE_LOG}")
+
+def print_debug_dash(file_path, dash: int = 100):
+    with open(file_path, "a", encoding="utf-8") as f:
+        if dash > 0:
+            f.write("-" * dash + "\n")
+
+def print_debug_text(file_path, text: str):
+    with open(file_path, "a", encoding="utf-8") as f:
+        if text != "":
+            f.write(text + "\n")
+
+COUNT = 0
+def print_debug(a: torch.Tensor, file_path: str, name: str, print_shape=False, dash=0, layer=-1, force_print_weight:bool=False,force_more_print:int=-1):
+    # return
+    global COUNT
+        
+    P = 256
+    if force_more_print == -1:
+        MAX_SLICE = 16
+    else:
+        MAX_SLICE = force_more_print
+        
+    def print_dash(f, dash=0):
+        if dash > 0:
+            f.write("-" * dash + "\n")
+
+    if a.shape[0] in [1, 10] or force_print_weight:
+        if layer==0: COUNT += 1
+        if COUNT>=3: return
+
+
+        if a.ndim > 3:
+            raise ValueError(f"Tensor '{name}' has {a.ndim} dims. Max 3 supported.")
+
+        a_cpu = a.detach().cpu()
+        a_print = a_cpu
+
+        if any(dim > P for dim in a_cpu.shape):
+            slices = tuple(slice(0, MAX_SLICE) if dim > P else slice(None) for dim in a_cpu.shape)
+            a_print = a_cpu[slices]
+
+        try:
+            torch.set_printoptions(threshold=float("inf"), linewidth=20000, sci_mode=False, precision=4)
+            tensor_str = str(a_print)
+            tensor_str = re.sub(r"^tensor\(", "", tensor_str)
+            tensor_str = re.sub(r",\s*(dtype|device)=.*?\)$", "", tensor_str, flags=re.DOTALL)
+            tensor_str = re.sub(r"\)$", "", tensor_str)
+        except Exception as e:
+            raise RuntimeError("Tensor print failed") from e
+
+        with open(file_path, "a", encoding="utf-8") as f:
+            f.write(
+                f"name={name!r:<25} | "
+                f"dtype={str(a.dtype):<15} | "
+                f"shape={str(tuple(a.shape)):<20} | "
+                f"{('val = ' + str(a_cpu.item()) if a_cpu.numel() == 1 else '')}\n"
+            )
+            if not print_shape:
+                f.write(f"{tensor_str}\n")
+            print_dash(f, dash)
\ No newline at end of file
diff --git a/external/matmul.py b/external/matmul.py
new file mode 100644
index 0000000..63b0dc4
--- /dev/null
+++ b/external/matmul.py
@@ -0,0 +1,75 @@
+import torch
+import triton
+import triton.language as tl
+
+@triton.jit
+def matmul_kernel(
+    a_ptr, b_ptr, c_ptr,
+    M, N, K,
+    stride_am, stride_ak,
+    stride_bn, stride_bk,
+    stride_cm, stride_cn,
+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
+):
+    # Lấy PID (Program ID) của các khối
+    pid = tl.program_id(axis=0)
+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
+    pid_m = pid // num_pid_n
+    pid_n = pid % num_pid_n
+
+    # Tạo con trỏ cho khối A và B
+    # A shape [M, K], B shape [N, K] (Do B chưa transpose trong bộ nhớ)
+    
+    # Offsets cho A: hàng pid_m, chạy dọc K
+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
+    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
+
+    # Offsets cho B: hàng pid_n, chạy dọc K (vì ta cần B^T nên ta đọc B theo hàng của nó tương ứng cột của kết quả)
+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
+    b_ptrs = b_ptr + (offs_bn[None, :] * stride_bn + offs_k[:, None] * stride_bk)
+
+    # Biến tích lũy
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+
+    # Loop qua chiều K
+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
+        
+        # Thực hiện phép nhân: a [BLOCK_M, BLOCK_K] x b [BLOCK_K, BLOCK_N]
+        # Lưu ý: b ở đây đã được load sao cho chiều K là chiều dọc (nhờ cách set offsets ở trên)
+        accumulator += tl.dot(a, b)
+        
+        # Advance pointers
+        a_ptrs += BLOCK_SIZE_K * stride_ak
+        b_ptrs += BLOCK_SIZE_K * stride_bk
+
+    # Chuyển về đúng dtype của C (ví dụ float16/bfloat16)
+    c = accumulator.to(tl.float16) # Hoặc bfloat16 tùy input
+
+    # Ghi kết quả C
+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
+    tl.store(c_ptrs, c, mask=c_mask)
+
+def triton_matmul(A, B):
+    # A: MxK, B: NxK -> C: MxN
+    M, K = A.shape
+    N, _ = B.shape
+    C = torch.empty((M, N), device=A.device, dtype=A.dtype)
+    
+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)
+    
+    matmul_kernel[grid](
+        A, B, C,
+        M, N, K,
+        A.stride(0), A.stride(1),
+        B.stride(0), B.stride(1),
+        C.stride(0), C.stride(1),
+        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=32 # Tuning tùy GPU
+    )
+    return C
\ No newline at end of file
diff --git a/model_executor/custom_op.py b/model_executor/custom_op.py
old mode 100644
new mode 100755
index dfd052f..f2f07cc
--- a/model_executor/custom_op.py
+++ b/model_executor/custom_op.py
@@ -79,11 +79,16 @@ class CustomOp(nn.Module):
         else:
             compilation_config.disabled_custom_ops.update(
                 [self.__class__.name])
+            
+        print(f"=" * 50 + f"\n{enabled = }\n" + '='*50)
 
         if not enabled:
             return self.forward_native
+        
+        
 
         if current_platform.is_rocm():
+            print("===> self.forward_hip")
             return self.forward_hip
         elif current_platform.is_cpu():
             return self.forward_cpu
diff --git a/model_executor/layers/layernorm.py b/model_executor/layers/layernorm.py
old mode 100644
new mode 100755
index 6e3d480..c85a9f8
--- a/model_executor/layers/layernorm.py
+++ b/model_executor/layers/layernorm.py
@@ -9,6 +9,7 @@ import vllm.envs as envs
 from vllm.model_executor.custom_op import CustomOp
 from vllm.platforms import current_platform
 
+from vllm.debug import print_debug, FILE_LOG, print_debug_dash, print_debug_text
 
 def is_rocm_aiter_rmsnorm_enabled() -> bool:
     return current_platform.is_rocm() \
@@ -41,20 +42,42 @@ def fused_add_rms_norm(
         x: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor,
         variance_epsilon: float) -> Tuple[torch.Tensor, torch.Tensor]:
     from vllm import _custom_ops as ops
+    
+    envs.VLLM_USE_OPT_OP=False
     if envs.VLLM_USE_OPT_OP:
+        # print_debug_dash(FILE_LOG, 150)
+        # print_debug_text(FILE_LOG, f"{envs.VLLM_USE_OPT_OP = }, {variance_epsilon = }, {weight.shape =}")
+        # print_debug(weight, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] weight", force_print_weight=True)
+        # print_debug(x, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] x")
+        # print_debug(residual, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] residual")
+
         ops.fused_add_rms_norm_opt(
             x,
             residual,
             weight,
             variance_epsilon,
         )
+
+        # print_debug(x, file_path=FILE_LOG, name=f"[AFTER fused_add_rms_norm] x")
+        # print_debug(residual, file_path=FILE_LOG, name=f"[AFTER fused_add_rms_norm] residual")
+        # print_debug_dash(FILE_LOG, 150)
     else:
+        # print_debug_dash(FILE_LOG, 150)
+        # print_debug_text(FILE_LOG, f"NOOO {envs.VLLM_USE_OPT_OP = }")
+        # print_debug(weight, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] weight", force_print_weight=True)
+        # print_debug(x, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] x")
+        # print_debug(residual, file_path=FILE_LOG, name=f"[BEFORE fused_add_rms_norm] residual")
+        
         ops.fused_add_rms_norm(
             x,
             residual,
             weight,
             variance_epsilon,
         )
+        # print_debug(x, file_path=FILE_LOG, name=f"[AFTER fused_add_rms_norm] x")
+        # print_debug(residual, file_path=FILE_LOG, name=f"[AFTER fused_add_rms_norm] residual")
+        # print_debug_dash(FILE_LOG, 150)
+
     return x, residual
 
 
@@ -108,6 +131,7 @@ class RMSNorm(CustomOp):
         eps: float = 1e-6,
         var_hidden_size: Optional[int] = None,
         has_weight: bool = True,
+        name: str=""
     ) -> None:
         super().__init__()
 
@@ -115,6 +139,9 @@ class RMSNorm(CustomOp):
         self.variance_epsilon = eps
         self.variance_size_override = (None if var_hidden_size == hidden_size
                                        else var_hidden_size)
+        if name != "":
+            print(f"{name = }, {self.hidden_size = }, {self.variance_epsilon = }, {self.variance_size_override=}")
+            
         self.has_weight = has_weight
 
         self.weight = torch.ones(hidden_size)
@@ -127,6 +154,7 @@ class RMSNorm(CustomOp):
         residual: Optional[torch.Tensor] = None,
     ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
         """PyTorch-native implementation equivalent to forward()."""
+        # print("x before compute"); print(x)
         orig_dtype = x.dtype
         x = x.to(torch.float32)
         if residual is not None:
@@ -152,6 +180,7 @@ class RMSNorm(CustomOp):
 
         x = x * torch.rsqrt(variance + self.variance_epsilon)
         x = x.to(orig_dtype)
+        # print("x after compute"); print(x)
         if self.has_weight:
             x = x * self.weight
         if residual is None:
@@ -164,6 +193,8 @@ class RMSNorm(CustomOp):
         x: torch.Tensor,
         residual: Optional[torch.Tensor] = None,
     ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+        return self.forward_native(x, residual)
+    
         if self.variance_size_override is not None:
             return self.forward_native(x, residual)
 
@@ -171,9 +202,16 @@ class RMSNorm(CustomOp):
         norm_func = dispatch_cuda_rmsnorm_func(add_residual)
 
         if add_residual:
-            return norm_func(x, residual, self.weight.data,
+            # print_debug(x, file_path=FILE_LOG, name=f"[BEFORE] x RMSNorm::forward_cuda")
+            # print_debug(residual, file_path=FILE_LOG, name=f"[BEFORE] residual RMSNorm::forward_cuda")
+            # print_debug_text(FILE_LOG, f"RMSNorm::forward_cuda with residual")
+            out = norm_func(x, residual, self.weight.data,
                              self.variance_epsilon)
+            # print_debug(out[0], file_path=FILE_LOG, name=f"[AFTER] x RMSNorm::forward_cuda")
+            # print_debug(out[1], file_path=FILE_LOG, name=f"[AFTER] residual RMSNorm::forward_cuda", dash=100)
+            return out
         else:
+            # print_debug_text(FILE_LOG, f"RMSNorm::forward_cuda with no residual")
             return norm_func(x, self.weight.data, self.variance_epsilon)
 
     def forward_hpu(
diff --git a/model_executor/layers/logits_processor.py b/model_executor/layers/logits_processor.py
old mode 100644
new mode 100755
index 4a35972..a522c54
--- a/model_executor/layers/logits_processor.py
+++ b/model_executor/layers/logits_processor.py
@@ -20,6 +20,7 @@ if envs.VLLM_LOGITS_PROCESSOR_THREADS is not None:
     _logits_processor_threadpool = ThreadPoolExecutor(
         envs.VLLM_LOGITS_PROCESSOR_THREADS)
 
+from vllm.debug import print_debug, print_debug_text, print_debug_dash, FILE_LOG 
 
 class LogitsProcessor(nn.Module):
     """Process logits and apply logits processors from sampling metadata.
@@ -67,7 +68,12 @@ class LogitsProcessor(nn.Module):
                                                      sampling_metadata)
 
             # Get the logits for the next tokens.
+            # print_debug(hidden_states, file_path=FILE_LOG, name=f"[LOGITS] BEFORE self._get_logits(hidden_states, lm_head, embedding_bias)")
+            
             logits = self._get_logits(hidden_states, lm_head, embedding_bias)
+            
+            # print_debug(logits, file_path=FILE_LOG, name=f"[LOGITS] AFTER self._get_logits(hidden_states, lm_head, embedding_bias)", dash=50)
+        
         if logits is not None:
             if self.soft_cap is not None:
                 logits = logits / self.soft_cap
@@ -108,6 +114,7 @@ class LogitsProcessor(nn.Module):
         logits = lm_head.quant_method.apply(lm_head,
                                             hidden_states,
                                             bias=embedding_bias)
+        # print_debug(logits, file_path=FILE_LOG, name=f"[LOGITS] AFTER lm_head.quant_method.apply")
 
         # Gather logits for TP
         logits = self._gather_logits(logits)
diff --git a/model_executor/layers/vocab_parallel_embedding.py b/model_executor/layers/vocab_parallel_embedding.py
old mode 100644
new mode 100755
index d102bcf..834d219
--- a/model_executor/layers/vocab_parallel_embedding.py
+++ b/model_executor/layers/vocab_parallel_embedding.py
@@ -1,3 +1,68 @@
+import torch
+import triton
+import triton.language as tl
+
+@triton.jit
+def matmul_kernel(
+    a_ptr, b_ptr, c_ptr,
+    M, N, K,
+    stride_am, stride_ak,
+    stride_bn, stride_bk,
+    stride_cm, stride_cn,
+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
+):
+    pid = tl.program_id(axis=0)
+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
+    pid_m = pid // num_pid_n
+    pid_n = pid % num_pid_n
+
+    
+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
+    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
+
+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
+    b_ptrs = b_ptr + (offs_bn[None, :] * stride_bn + offs_k[:, None] * stride_bk)
+
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+
+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
+        
+        accumulator += tl.dot(a, b)
+        
+        a_ptrs += BLOCK_SIZE_K * stride_ak
+        b_ptrs += BLOCK_SIZE_K * stride_bk
+
+    c = accumulator.to(tl.float16)
+
+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
+    tl.store(c_ptrs, c, mask=c_mask)
+
+def triton_matmul(A, B):
+    # A: MxK, B: NxK -> C: MxN
+    M, K = A.shape
+    N, _ = B.shape
+    C = torch.empty((M, N), device=A.device, dtype=A.dtype)
+    
+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)
+    
+    matmul_kernel[grid](
+        A, B, C,
+        M, N, K,
+        A.stride(0), A.stride(1),
+        B.stride(0), B.stride(1),
+        C.stride(0), C.stride(1),
+        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=32 # Tuning tùy GPU
+    )
+    return C
+
+
 # SPDX-License-Identifier: Apache-2.0
 import os
 from dataclasses import dataclass
@@ -18,6 +83,27 @@ from vllm.platforms import current_platform
 
 DEFAULT_VOCAB_PADDING_SIZE = 64
 
+from vllm.debug import print_debug, print_debug_text, print_debug_dash, FILE_LOG 
+from vllm.external.matmul import triton_matmul
+
+def naive_matmul_bias_3_loops(A, B, bias=None):
+    M, K = A.shape
+    N, _ = B.shape
+    
+    C = torch.zeros((M, N), dtype=A.dtype, device=A.device)
+    
+    for i in range(M):
+        for j in range(N):
+            total = 0.0
+            for k in range(K):
+                total += A[i, k] * B[j, k]
+            
+            if bias is not None:
+                total += bias[j]
+                
+            C[i, j] = total
+            
+    return C
 
 class UnquantizedEmbeddingMethod(QuantizeMethodBase):
     """Unquantized method for embeddings."""
@@ -30,6 +116,8 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
                        output_size: int, params_dtype: torch.dtype,
                        **extra_weight_attrs):
         """Create weights for embedding layer."""
+        # print_debug_text(FILE_LOG, f"==> Create WEIGHT(LLM HEAD): {sum(output_partition_sizes) = }, {input_size_per_partition = }, {output_partition_sizes = }\n=======================\n")
+        
         weight = Parameter(torch.empty(sum(output_partition_sizes),
                                        input_size_per_partition,
                                        dtype=params_dtype),
@@ -42,6 +130,10 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
               layer: torch.nn.Module,
               x: torch.Tensor,
               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        # print_debug_text(FILE_LOG, f"********** {self.use_llama_nn=},{os.environ['LM_NN']=}")
+        
+        # self.use_llama_nn=True
+        # os.environ['LM_NN'] = '1'
         if self.use_llama_nn and os.environ['LM_NN'] == '1':
             if bias is not None:
                 if len(x.shape) == 2:
@@ -49,9 +141,34 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
                 else:
                     return torch.matmul(x, layer.weight) + bias
             else:
+                # print_debug_dash(FILE_LOG, dash=30)
+                # print_debug_text(FILE_LOG, "===> CALLING x.matmul(layer.weight.t())")
+                # print_debug(x, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] x", force_more_print=-1)
+                # print_debug(layer.weight, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] weight", force_print_weight=True, force_more_print=-1)
+                # if bias is not None:
+                #     print_debug(bias, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] bias", force_print_weight=True)
+                # else:
+                #     print_debug_text(FILE_LOG, "bias is None")
+                # out = x.matmul(layer.weight.t())
+                # out = torch.matmul(x, layer.weight)
+                # print_debug(out, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] out", dash=20, force_more_print=-1)
+                # return out
                 return torch.matmul(x, layer.weight)
         else:
-            return F.linear(x, layer.weight, bias)
+            # print_debug_dash(FILE_LOG, dash=30)
+            # print_debug_text(FILE_LOG, "===> CALLING F.linear(x, layer.weight, bias)")
+            # print_debug(x, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] x", force_more_print=-1)
+            # print_debug(layer.weight, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] weight", force_print_weight=True, force_more_print=-1)
+            # if bias is not None:
+            #     print_debug(bias, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] bias", force_print_weight=True)
+            # else:
+            #     print_debug_text(FILE_LOG, "bias is None")
+            # out = F.linear(x, layer.weight, bias)
+            # out = naive_matmul_bias_3_loops(x, layer.weight, bias)
+            out = triton_matmul(x, layer.weight)
+            # print_debug(out, file_path=FILE_LOG, name=f"[UnquantizedEmbeddingMethod] out", dash=20, force_more_print=-1)
+            return out
+            # return F.linear(x, layer.weight, bias)
 
     def embedding(self, layer: torch.nn.Module,
                   input_: torch.Tensor) -> torch.Tensor:
diff --git a/model_executor/models/interfaces_base.py b/model_executor/models/interfaces_base.py
old mode 100644
new mode 100755
index 22c9287..4d08388
--- a/model_executor/models/interfaces_base.py
+++ b/model_executor/models/interfaces_base.py
@@ -128,6 +128,11 @@ def is_text_generation_model(
     model: Union[Type[object], object],
 ) -> Union[TypeIs[Type[VllmModelForTextGeneration]],
            TypeIs[VllmModelForTextGeneration]]:
+    
+    print(f'====> Go here: {model = }')
+    print(f'====> {is_vllm_model(model) = }')
+    print(f'====> { isinstance(model, VllmModelForTextGeneration) = }')
+    
     if not is_vllm_model(model):
         return False
 
diff --git a/model_executor/models/llama.py b/model_executor/models/llama.py
old mode 100644
new mode 100755
index 3b42fdd..95f8f43
--- a/model_executor/models/llama.py
+++ b/model_executor/models/llama.py
@@ -71,7 +71,11 @@ def get_compressed_tensors_cache_scale(name: str) -> Optional[str]:
     # If no matches, return None
     return None
 
-
+# import torch
+# torch.set_printoptions(
+#     linewidth=1000,
+#     sci_mode=False
+# )
 class LlamaMLP(nn.Module):
 
     def __init__(
@@ -544,8 +548,13 @@ class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
         intermediate_tensors: Optional[IntermediateTensors] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
     ) -> Union[torch.Tensor, IntermediateTensors]:
+
         model_output = self.model(input_ids, positions, intermediate_tensors,
                                   inputs_embeds)
+        # print("-" * 50)
+        # print(f"Forward: {model_output.dtype = }, {model_output.shape = }, {input_ids = }, {positions = }")
+        # print(f"{model_output}")
+        # print("-" * 50)
         return model_output
 
     def compute_logits(
@@ -555,11 +564,19 @@ class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
     ) -> Optional[torch.Tensor]:
         logits = self.logits_processor(self.lm_head, hidden_states,
                                        sampling_metadata)
+        # print("-" * 50)
+        # print(f"Compute logits: {logits.dtype = }, {logits.shape = }")
+        # print(f"{logits}")
+        # print("-" * 50)
         return logits
 
     def sample(self, logits: torch.Tensor,
                sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:
         next_tokens = self.sampler(logits, sampling_metadata)
+        # print("-" * 50)
+        # print(f"Sample:")
+        # print(f"{next_tokens}")
+        # print("-" * 50)
         return next_tokens
 
     def load_weights(self, weights: Iterable[Tuple[str,
diff --git a/model_executor/models/registry.py b/model_executor/models/registry.py
old mode 100644
new mode 100755
index 7664c82..cb8560d
--- a/model_executor/models/registry.py
+++ b/model_executor/models/registry.py
@@ -301,11 +301,17 @@ class _LazyRegisteredModel(_BaseRegisteredModel):
 
     # Performed in another process to avoid initializing CUDA
     def inspect_model_cls(self) -> _ModelInfo:
+        self.load_model_cls()
         return _run_in_subprocess(
             lambda: _ModelInfo.from_model_cls(self.load_model_cls()))
 
     def load_model_cls(self) -> Type[nn.Module]:
         mod = importlib.import_module(self.module_name)
+        model = getattr(mod, self.class_name)
+        
+        # print(f"{is_text_generation_model(model) = }")
+        
+        # print(f'1. {mod = }, {self.class_name = }, {getattr(mod, self.class_name) = }')
         return getattr(mod, self.class_name)
 
 
@@ -330,6 +336,7 @@ def _try_inspect_model_cls(
     model: _BaseRegisteredModel,
 ) -> Optional[_ModelInfo]:
     try:
+        # print(f'===> 0. {model_arch = }, {model = }')
         return model.inspect_model_cls()
     except Exception:
         logger.exception("Error in inspecting model architecture '%s'",
@@ -409,7 +416,10 @@ class _ModelRegistry:
 
     def _try_inspect_model_cls(self, model_arch: str) -> Optional[_ModelInfo]:
         if model_arch not in self.models:
+            # print(f'{model_arch = } not in {self.models.keys() = }')
             return None
+        
+        # print(f'{model_arch = } in self.models')
 
         return _try_inspect_model_cls(model_arch, self.models[model_arch])
 
@@ -436,9 +446,14 @@ class _ModelRegistry:
         architectures: Union[str, List[str]],
     ) -> Tuple[_ModelInfo, str]:
         architectures = self._normalize_archs(architectures)
-
+        # print(f'-' * 100)
+        # print(f'{architectures = }')
+        # print(f'-' * 100)
         for arch in architectures:
             model_info = self._try_inspect_model_cls(arch)
+            # print(f'-' * 100)
+            # print(f'{model_info = }')
+            # print(f'-' * 100)
             if model_info is not None:
                 return (model_info, arch)
 
@@ -462,6 +477,9 @@ class _ModelRegistry:
         architectures: Union[str, List[str]],
     ) -> bool:
         model_cls, _ = self.inspect_model_cls(architectures)
+        # print(f'-' * 50)
+        # print(f'{model_cls = }')
+        # print(f'-' * 50)
         return model_cls.is_text_generation_model
 
     def is_pooling_model(
diff --git a/v1/attention/backends/triton_attn.py b/v1/attention/backends/triton_attn.py
old mode 100644
new mode 100755
index f11f2b6..ccc21ad
--- a/v1/attention/backends/triton_attn.py
+++ b/v1/attention/backends/triton_attn.py
@@ -70,6 +70,7 @@ class TritonAttentionImpl(AttentionImpl):
         blocksparse_params: Optional[dict[str, Any]] = None,
         logits_soft_cap: Optional[float] = None,
         attn_type: AttentionType = AttentionType.DECODER,
+        sinks: Optional[torch.Tensor] = None,
     ) -> None:
         if blocksparse_params is not None:
             raise ValueError(
@@ -101,6 +102,13 @@ class TritonAttentionImpl(AttentionImpl):
                                       "encoder/decoder cross-attention "
                                       "are not implemented for "
                                       "TritonAttentionImpl")
+            
+        self.sinks = sinks
+        if sinks is not None:
+            assert sinks.shape[0] == num_heads, (
+                "Sinks must have the same number of heads as the number of "
+                f"heads in the layer. Sinks shape: {sinks.shape}, "
+                f"num_heads: {num_heads}.")
 
     def forward(
         self,
@@ -173,6 +181,8 @@ class TritonAttentionImpl(AttentionImpl):
             v_scale=layer._v_scale,
             alibi_slopes=self.alibi_slopes,
             sliding_window=self.sliding_window[0],
-            sm_scale=self.scale)
+            sm_scale=self.scale,
+            sinks=self.sinks,
+        )
 
         return output
diff --git a/v1/worker/gpu_model_runner.py b/v1/worker/gpu_model_runner.py
old mode 100644
new mode 100755
index a85009f..9ef1979
--- a/v1/worker/gpu_model_runner.py
+++ b/v1/worker/gpu_model_runner.py
@@ -42,6 +42,8 @@ from vllm.v1.utils import bind_kv_cache
 from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
 from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
 
+from vllm.debug import FILE_LOG
+
 if TYPE_CHECKING:
     import xgrammar as xgr
 
@@ -1033,6 +1035,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # Run the decoder.
         # Use persistent buffers for CUDA graphs.
         with set_forward_context(attn_metadata, self.vllm_config):
+            # with open(FILE_LOG, "a", encoding="utf-8") as f:
+            #     f.write("=" * 100 + "\n" + ">>>>>>>>>>> EXECUTE MODEL >>>>>>>>>>\n")
             hidden_states = self.model(
                 input_ids=input_ids,
                 positions=positions,
@@ -1045,7 +1049,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
         hidden_states = hidden_states[:num_scheduled_tokens]
         sample_hidden_states = hidden_states[logits_indices]
+        # with open(FILE_LOG, "a", encoding="utf-8") as f:
+        #     f.write(f"Compute logits:: {hidden_states.shape = }, {sample_hidden_states.shape = }, {logits_indices = }, {hidden_states.dtype = }, {sample_hidden_states.dtype = }\n")
+            
         logits = self.model.compute_logits(sample_hidden_states, None)
+        # with open(FILE_LOG, "a", encoding="utf-8") as f:
+        #     f.write("-" * 100 + "\n")
 
         # Apply structured output bitmasks if present
         if scheduler_output.grammar_bitmask is not None:
@@ -1054,10 +1063,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # Sample the next token and get logprobs if needed.
         sampling_metadata = self.input_batch.sampling_metadata
         if spec_decode_metadata is None:
+            # with open(FILE_LOG, "a", encoding="utf-8") as f:
+            #     f.write(">>>>>>>>>>> SAMPLING >>>>>>>>>>\n")
             sampler_output = self.model.sample(
                 logits=logits,
                 sampling_metadata=sampling_metadata,
             )
+                # f.write(f"{sampler_output.sampled_token_ids = }\n")
+                
+
         else:
             # When indexing with a tensor (bonus_logits_indices), PyTorch
             # creates a new tensor with separate storage from the original
@@ -1082,7 +1096,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 sampling_metadata,
             )
             sampler_output.sampled_token_ids = output_token_ids
-
+            # with open(FILE_LOG, "a", encoding="utf-8") as f:
+            #     f.write("============================= GOHERER???????????????????????????\n")
         # TODO(woosuk): The following loop can be slow since it iterates over
         # the requests one by one. Optimize.
         for i, generator in self.input_batch.generators.items():
@@ -1593,12 +1608,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         block_size = self.vllm_config.cache_config.block_size
         use_mla = self.vllm_config.model_config.use_mla
         kv_cache_spec: dict[str, KVCacheSpec] = {}
+        
+        print('-' * 50)
+        print(f"{forward_ctx = }")
+        
         for layer_name, attn_module in forward_ctx.items():
-            if isinstance(attn_module, FusedMoE):
+            # if isinstance(attn_module, FusedMoE):
+            #     continue
+            if "FusedMoE" in attn_module.__class__.__name__:
                 continue
 
             # TODO: Support other attention modules, e.g., sliding window,
             # cross-attention
+            print(f"{attn_module = }")
+            
             assert isinstance(attn_module, Attention)
             if attn_module.attn_type == AttentionType.DECODER:
                 kv_cache_spec[layer_name] = FullAttentionSpec(
diff --git a/vllm/_version.py b/vllm/_version.py
new file mode 100755
index 0000000..a4cfd42
--- /dev/null
+++ b/vllm/_version.py
@@ -0,0 +1,21 @@
+# file generated by setuptools-scm
+# don't change, don't track in version control
+
+__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]
+
+TYPE_CHECKING = False
+if TYPE_CHECKING:
+    from typing import Tuple
+    from typing import Union
+
+    VERSION_TUPLE = Tuple[Union[int, str], ...]
+else:
+    VERSION_TUPLE = object
+
+version: str
+__version__: str
+__version_tuple__: VERSION_TUPLE
+version_tuple: VERSION_TUPLE
+
+__version__ = version = '0.10.2.dev1+g0979e7c75.d20251207'
+__version_tuple__ = version_tuple = (0, 10, 2, 'dev1', 'g0979e7c75.d20251207')
diff --git a/vllm/benchmarks/lib/__init__.py b/vllm/benchmarks/lib/__init__.py
new file mode 100755
index 0000000..005e87a
--- /dev/null
+++ b/vllm/benchmarks/lib/__init__.py
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""Benchmark library utilities."""
diff --git a/vllm/benchmarks/lib/endpoint_request_func.py b/vllm/benchmarks/lib/endpoint_request_func.py
new file mode 100755
index 0000000..47bc288
--- /dev/null
+++ b/vllm/benchmarks/lib/endpoint_request_func.py
@@ -0,0 +1,402 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""The request function for API endpoints."""
+
+import io
+import json
+import os
+import sys
+import time
+import traceback
+from dataclasses import dataclass, field
+from typing import Optional
+
+import aiohttp
+from tqdm.asyncio import tqdm
+
+AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
+
+
+@dataclass
+class RequestFuncInput:
+    """The input for the request function."""
+    prompt: str
+    api_url: str
+    prompt_len: int
+    output_len: int
+    model: str
+    model_name: Optional[str] = None
+    logprobs: Optional[int] = None
+    extra_body: Optional[dict] = None
+    multi_modal_content: Optional[dict | list[dict]] = None
+    ignore_eos: bool = False
+    language: Optional[str] = None
+
+
+@dataclass
+class RequestFuncOutput:
+    """The output of the request function including metrics."""
+    generated_text: str = ""
+    success: bool = False
+    latency: float = 0.0
+    output_tokens: int = 0
+    ttft: float = 0.0  # Time to first token
+    itl: list[float] = field(
+        default_factory=list)  # list of inter-token latencies
+    tpot: float = 0.0  # avg next-token latencies
+    prompt_len: int = 0
+    error: str = ""
+
+
+async def async_request_openai_completions(
+    request_func_input: RequestFuncInput,
+    session: aiohttp.ClientSession,
+    pbar: Optional[tqdm] = None,
+) -> RequestFuncOutput:
+    """The async request function for the OpenAI Completions API.
+
+    Args:
+        request_func_input: The input for the request function.
+        pbar: The progress bar to display the progress.
+
+    Returns:
+        The output of the request function.
+    """
+    api_url = request_func_input.api_url
+    assert api_url.endswith(
+        ("completions", "profile")
+    ), "OpenAI Completions API URL must end with 'completions' or 'profile'."
+
+    payload = {
+        "model": request_func_input.model_name \
+            if request_func_input.model_name else request_func_input.model,
+        "prompt": request_func_input.prompt,
+        "temperature": 0.0,
+        "repetition_penalty": 1.0,
+        "max_tokens": request_func_input.output_len,
+        "logprobs": request_func_input.logprobs,
+        "stream": True,
+        "stream_options": {
+            "include_usage": True,
+        },
+    }
+    if request_func_input.ignore_eos:
+        payload["ignore_eos"] = request_func_input.ignore_eos
+    if request_func_input.extra_body:
+        payload.update(request_func_input.extra_body)
+    headers = {
+        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}"
+    }
+
+    output = RequestFuncOutput()
+    output.prompt_len = request_func_input.prompt_len
+
+    generated_text = ""
+    st = time.perf_counter()
+    most_recent_timestamp = st
+    try:
+        async with session.post(url=api_url, json=payload,
+                                headers=headers) as response:
+            if response.status == 200:
+                first_chunk_received = False
+                async for chunk_bytes in response.content:
+                    chunk_bytes = chunk_bytes.strip()
+                    if not chunk_bytes:
+                        continue
+                    chunk_bytes = chunk_bytes.decode("utf-8")
+                    # NOTE: SSE comments (often used as pings) start with
+                    # a colon. These are not JSON data payload and should
+                    # be skipped.
+                    if chunk_bytes.startswith(":"):
+                        continue
+
+                    chunk = chunk_bytes.removeprefix("data: ")
+
+                    if chunk != "[DONE]":
+                        data = json.loads(chunk)
+
+                        # NOTE: Some completion API might have a last
+                        # usage summary response without a token so we
+                        # want to check a token was generated
+                        if choices := data.get("choices"):
+                            # Note that text could be empty here
+                            # e.g. for special tokens
+                            text = choices[0].get("text")
+                            timestamp = time.perf_counter()
+                            # First token
+                            if not first_chunk_received:
+                                first_chunk_received = True
+                                ttft = time.perf_counter() - st
+                                output.ttft = ttft
+
+                            # Decoding phase
+                            else:
+                                output.itl.append(timestamp -
+                                                    most_recent_timestamp)
+
+                            most_recent_timestamp = timestamp
+                            generated_text += text or ""
+                        elif usage := data.get("usage"):
+                            output.output_tokens = usage.get(
+                                "completion_tokens")
+                if first_chunk_received:
+                    output.success = True
+                else:
+                    output.success = False
+                    output.error = (
+                        "Never received a valid chunk to calculate TTFT."
+                        "This response will be marked as failed!")
+                output.generated_text = generated_text
+                output.latency = most_recent_timestamp - st
+            else:
+                output.error = response.reason or ""
+                output.success = False
+    except Exception:
+        output.success = False
+        exc_info = sys.exc_info()
+        output.error = "".join(traceback.format_exception(*exc_info))
+
+    if pbar:
+        pbar.update(1)
+    return output
+
+
+async def async_request_openai_chat_completions(
+    request_func_input: RequestFuncInput,
+    session: aiohttp.ClientSession,
+    pbar: Optional[tqdm] = None,
+) -> RequestFuncOutput:
+    api_url = request_func_input.api_url
+    assert api_url.endswith(("chat/completions", "profile")), (
+        "OpenAI Chat Completions API URL must end with 'chat/completions'.")
+
+    content = [{"type": "text", "text": request_func_input.prompt}]
+    if request_func_input.multi_modal_content:
+        mm_content = request_func_input.multi_modal_content
+        if isinstance(mm_content, list):
+            content.extend(mm_content)
+        elif isinstance(mm_content, dict):
+            content.append(mm_content)
+        else:
+            raise TypeError(
+                "multi_modal_content must be a dict or list[dict] "
+                "for openai-chat"
+            )
+    payload = {
+        "model":
+        request_func_input.model_name
+        if request_func_input.model_name else request_func_input.model,
+        "messages": [
+            {
+                "role": "user",
+                "content": content
+            },
+        ],
+        "temperature":
+        0.0,
+        "max_completion_tokens":
+        request_func_input.output_len,
+        "stream":
+        True,
+        "stream_options": {
+            "include_usage": True,
+        },
+    }
+    if request_func_input.ignore_eos:
+        payload["ignore_eos"] = request_func_input.ignore_eos
+    if request_func_input.extra_body:
+        payload.update(request_func_input.extra_body)
+    headers = {
+        "Content-Type": "application/json",
+        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+    }
+
+    output = RequestFuncOutput()
+    output.prompt_len = request_func_input.prompt_len
+
+    generated_text = ""
+    ttft = 0.0
+    st = time.perf_counter()
+    most_recent_timestamp = st
+    try:
+        async with session.post(url=api_url, json=payload,
+                                headers=headers) as response:
+            if response.status == 200:
+                async for chunk_bytes in response.content:
+                    chunk_bytes = chunk_bytes.strip()
+                    if not chunk_bytes:
+                        continue
+                    chunk_bytes = chunk_bytes.decode("utf-8")
+                    # NOTE: SSE comments (often used as pings) start with
+                    # a colon. These are not JSON data payload and should
+                    # be skipped.
+                    if chunk_bytes.startswith(":"):
+                        continue
+
+                    chunk = chunk_bytes.removeprefix("data: ")
+
+                    if chunk != "[DONE]":
+                        timestamp = time.perf_counter()
+                        data = json.loads(chunk)
+
+                        if choices := data.get("choices"):
+                            content = choices[0]["delta"].get("content")
+                            # First token
+                            if ttft == 0.0:
+                                ttft = timestamp - st
+                                output.ttft = ttft
+
+                            # Decoding phase
+                            else:
+                                output.itl.append(timestamp -
+                                                    most_recent_timestamp)
+
+                            generated_text += content or ""
+                        elif usage := data.get("usage"):
+                            output.output_tokens = usage.get(
+                                "completion_tokens")
+
+                        most_recent_timestamp = timestamp
+
+                output.generated_text = generated_text
+                output.success = True
+                output.latency = most_recent_timestamp - st
+            else:
+                output.error = response.reason or ""
+                output.success = False
+    except Exception:
+        output.success = False
+        exc_info = sys.exc_info()
+        output.error = "".join(traceback.format_exception(*exc_info))
+
+    if pbar:
+        pbar.update(1)
+    return output
+
+
+async def async_request_openai_audio(
+    request_func_input: RequestFuncInput,
+    session: aiohttp.ClientSession,
+    pbar: Optional[tqdm] = None,
+) -> RequestFuncOutput:
+    # Lazy import without PlaceholderModule to avoid vllm dep.
+    import soundfile
+
+    api_url = request_func_input.api_url
+    assert api_url.endswith(("transcriptions", "translations")), (
+        "OpenAI Chat Completions API URL must end with 'transcriptions' ")
+    "or `translations`."
+
+    content = [{"type": "text", "text": request_func_input.prompt}]
+    payload = {
+        "model":
+        request_func_input.model_name
+        if request_func_input.model_name else request_func_input.model,
+        "temperature":
+        0.0,
+        "max_completion_tokens":
+        request_func_input.output_len,
+        "stream":
+        True,
+        "language":
+        "en",
+        # Flattened due to multipart/form-data
+        "stream_include_usage":
+        True,
+        "stream_continuous_usage_stats":
+        True,
+    }
+    if request_func_input.extra_body:
+        payload.update(request_func_input.extra_body)
+    headers = {
+        "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+    }
+
+    # Send audio file
+    def to_bytes(y, sr):
+        buffer = io.BytesIO()
+        soundfile.write(buffer, y, sr, format="WAV")
+        buffer.seek(0)
+        return buffer
+
+    mm_audio = request_func_input.multi_modal_content
+    if not isinstance(mm_audio, dict) or "audio" not in mm_audio:
+        raise TypeError("multi_modal_content must be a dict containing 'audio'")
+    with to_bytes(*mm_audio["audio"]) as f:
+        form = aiohttp.FormData()
+        form.add_field("file", f, content_type="audio/wav")
+        for key, value in payload.items():
+            form.add_field(key, str(value))
+
+        output = RequestFuncOutput()
+        output.prompt_len = request_func_input.prompt_len
+
+        generated_text = ""
+        ttft = 0.0
+        st = time.perf_counter()
+        most_recent_timestamp = st
+        try:
+            async with session.post(url=api_url,
+                                    data=form,
+                                    headers=headers) as response:
+                if response.status == 200:
+                    async for chunk_bytes in response.content:
+                        chunk_bytes = chunk_bytes.strip()
+                        if not chunk_bytes:
+                            continue
+
+                        chunk = chunk_bytes.decode("utf-8").removeprefix(
+                            "data: ")
+                        if chunk != "[DONE]":
+                            timestamp = time.perf_counter()
+                            data = json.loads(chunk)
+
+                            if choices := data.get("choices"):
+                                content = choices[0]["delta"].get(
+                                    "content")
+                                # First token
+                                if ttft == 0.0:
+                                    ttft = timestamp - st
+                                    output.ttft = ttft
+
+                                # Decoding phase
+                                else:
+                                    output.itl.append(
+                                        timestamp - most_recent_timestamp)
+
+                                generated_text += content or ""
+                            elif usage := data.get("usage"):
+                                output.output_tokens = usage.get(
+                                    "completion_tokens")
+
+                            most_recent_timestamp = timestamp
+
+                    output.generated_text = generated_text
+                    output.success = True
+                    output.latency = most_recent_timestamp - st
+                else:
+                    output.error = response.reason or ""
+                    output.success = False
+        except Exception:
+            output.success = False
+            exc_info = sys.exc_info()
+            output.error = "".join(traceback.format_exception(*exc_info))
+
+    if pbar:
+        pbar.update(1)
+    return output
+
+
+# TODO: Add more request functions for different API protocols.
+ASYNC_REQUEST_FUNCS = {
+    "vllm": async_request_openai_completions,
+    "openai": async_request_openai_completions,
+    "openai-chat": async_request_openai_chat_completions,
+    "openai-audio": async_request_openai_audio,
+}
+
+OPENAI_COMPATIBLE_BACKENDS = [
+    k for k, v in ASYNC_REQUEST_FUNCS.items()
+    if v in (async_request_openai_completions,
+             async_request_openai_chat_completions)
+]
diff --git a/vllm/benchmarks/lib/ready_checker.py b/vllm/benchmarks/lib/ready_checker.py
new file mode 100755
index 0000000..7e83615
--- /dev/null
+++ b/vllm/benchmarks/lib/ready_checker.py
@@ -0,0 +1,72 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""Utilities for checking endpoint readiness."""
+
+import asyncio
+import time
+
+import aiohttp
+from tqdm.asyncio import tqdm
+
+from .endpoint_request_func import RequestFuncInput, RequestFuncOutput
+
+
+async def wait_for_endpoint(
+    request_func,
+    test_input: RequestFuncInput,
+    session: aiohttp.ClientSession,
+    timeout_seconds: int = 600,
+    retry_interval: int = 5,
+) -> RequestFuncOutput:
+    """
+    Wait for an endpoint to become available before starting benchmarks.
+    
+    Args:
+        request_func: The async request function to call
+        test_input: The RequestFuncInput to test with
+        timeout_seconds: Maximum time to wait in seconds (default: 10 minutes)
+        retry_interval: Time between retries in seconds (default: 5 seconds)
+        
+    Returns:
+        RequestFuncOutput: The successful response
+        
+    Raises:
+        ValueError: If the endpoint doesn't become available within the timeout
+    """
+    deadline = time.perf_counter() + timeout_seconds
+    output = RequestFuncOutput(success=False)
+    print(f"Waiting for endpoint to become up in {timeout_seconds} seconds")
+    
+    with tqdm(
+        total=timeout_seconds, 
+        bar_format="{desc} |{bar}| {elapsed} elapsed, {remaining} remaining",
+        unit="s",
+    ) as pbar:
+
+        while True:            
+            # update progress bar
+            remaining = deadline - time.perf_counter()
+            elapsed = timeout_seconds - remaining
+            update_amount = min(elapsed - pbar.n, timeout_seconds - pbar.n)
+            pbar.update(update_amount)
+            pbar.refresh()
+            if remaining <= 0:
+                pbar.close()
+                break
+
+            # ping the endpoint using request_func
+            try:
+                output = await request_func(
+                    request_func_input=test_input, session=session)
+                if output.success:
+                    pbar.close()
+                    return output
+            except aiohttp.ClientConnectorError:
+                pass
+            
+            # retry after a delay
+            sleep_duration = min(retry_interval, remaining)
+            if sleep_duration > 0:
+                await asyncio.sleep(sleep_duration)
+    
+    return output
diff --git a/vllm/benchmarks/lib/utils.py b/vllm/benchmarks/lib/utils.py
new file mode 100755
index 0000000..5f95fdc
--- /dev/null
+++ b/vllm/benchmarks/lib/utils.py
@@ -0,0 +1,75 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+import argparse
+import json
+import math
+import os
+from typing import Any
+
+
+def convert_to_pytorch_benchmark_format(args: argparse.Namespace,
+                                        metrics: dict[str, list],
+                                        extra_info: dict[str, Any]) -> list:
+    """
+    Save the benchmark results in the format used by PyTorch OSS benchmark with
+    on metric per record
+    https://github.com/pytorch/pytorch/wiki/How-to-integrate-with-PyTorch-OSS-benchmark-database
+    """
+    records = []
+    if not os.environ.get("SAVE_TO_PYTORCH_BENCHMARK_FORMAT", False):
+        return records
+
+    for name, benchmark_values in metrics.items():
+        record = {
+            "benchmark": {
+                "name": "vLLM benchmark",
+                "extra_info": {
+                    "args": vars(args),
+                },
+            },
+            "model": {
+                "name": args.model,
+            },
+            "metric": {
+                "name": name,
+                "benchmark_values": benchmark_values,
+                "extra_info": extra_info,
+            },
+        }
+
+        tp = record["benchmark"]["extra_info"]["args"].get(
+            "tensor_parallel_size")
+        # Save tensor_parallel_size parameter if it's part of the metadata
+        if not tp and "tensor_parallel_size" in extra_info:
+            record["benchmark"]["extra_info"]["args"][
+                "tensor_parallel_size"] = extra_info["tensor_parallel_size"]
+
+        records.append(record)
+
+    return records
+
+
+class InfEncoder(json.JSONEncoder):
+
+    def clear_inf(self, o: Any):
+        if isinstance(o, dict):
+            return {k: self.clear_inf(v) for k, v in o.items()}
+        elif isinstance(o, list):
+            return [self.clear_inf(v) for v in o]
+        elif isinstance(o, float) and math.isinf(o):
+            return "inf"
+        return o
+
+    def iterencode(self, o: Any, *args, **kwargs) -> Any:
+        return super().iterencode(self.clear_inf(o), *args, **kwargs)
+
+
+def write_to_json(filename: str, records: list) -> None:
+    with open(filename, "w") as f:
+        json.dump(
+            records,
+            f,
+            cls=InfEncoder,
+            default=lambda o: f"<{type(o).__name__} is not JSON serializable>",
+        )
