diff --git a/attention/ops/chunked_prefill_paged_decode.py b/attention/ops/chunked_prefill_paged_decode.py
old mode 100644
new mode 100755
index cbf863a..ba3643f
--- a/attention/ops/chunked_prefill_paged_decode.py
+++ b/attention/ops/chunked_prefill_paged_decode.py
@@ -24,6 +24,7 @@ def kernel_paged_attention_2d(
         query_ptr,  # [num_tokens, num_query_heads, head_size]
         key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]
         value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]
+        sink_ptr,  # [num_query_heads]
         block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]
         seq_lens_ptr,  # [num_seqs]
         alibi_slopes_ptr,  # [num_query_heads]
@@ -55,6 +56,7 @@ def kernel_paged_attention_2d(
         stride_v_cache_3: tl.constexpr,  # int
         filter_by_query_len: tl.constexpr,  # bool
         query_start_len_ptr,  # [num_seqs+1]
+        USE_SINKS: tl.constexpr,  # bool
 ):
     seq_idx = tl.program_id(0)
     kv_head_idx = tl.program_id(1)
@@ -91,8 +93,23 @@ def kernel_paged_attention_2d(
 
     block_table_offset = seq_idx * block_table_stride
 
-    M = tl.full([num_queries_per_kv_padded], float("-inf"), dtype=tl.float32)
+    ########################################
+    # M = tl.full([num_queries_per_kv_padded], float("-inf"), dtype=tl.float32)
+    # L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)
+    
+    if not USE_SINKS:
+        M = tl.full([num_queries_per_kv_padded],
+                    float("-inf"),
+                    dtype=tl.float32)
+    else:
+        M = tl.load(
+            sink_ptr + query_head_idx,
+            mask=head_mask,
+            other=float("-inf"),
+        ).to(dtype=tl.float32)
     L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)
+    ########################################
+    
     acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED],
                    dtype=tl.float32)
 
@@ -218,6 +235,8 @@ def chunked_prefill_paged_decode(
     alibi_slopes=None,
     sliding_window=None,
     sm_scale=None,
+    # Optional tensor for sinks
+    sinks=None,
 ):
 
     if sm_scale is None:
@@ -247,6 +266,7 @@ def chunked_prefill_paged_decode(
             sliding_window=sliding_window,
             sm_scale=sm_scale,
             skip_decode=True,
+            sinks=sinks,
         )
 
     block_size = value_cache.shape[3]
@@ -283,6 +303,7 @@ def chunked_prefill_paged_decode(
         query_ptr=query,
         key_cache_ptr=key_cache,
         value_cache_ptr=value_cache,
+        sink_ptr=sinks,
         block_tables_ptr=block_table,
         seq_lens_ptr=seq_lens,
         alibi_slopes_ptr=alibi_slopes,
@@ -314,5 +335,6 @@ def chunked_prefill_paged_decode(
         stride_v_cache_3=value_cache.stride(3),
         filter_by_query_len=True,
         query_start_len_ptr=query_start_loc,
-        num_stages=1
+        num_stages=1,
+        USE_SINKS=sinks is not None,
     )
diff --git a/attention/ops/prefix_prefill.py b/attention/ops/prefix_prefill.py
old mode 100644
new mode 100755
index e85ec60..4316904
--- a/attention/ops/prefix_prefill.py
+++ b/attention/ops/prefix_prefill.py
@@ -25,6 +25,7 @@ if triton.__version__ >= "2.1.0":
         V,
         K_cache,
         V_cache,
+        sink_ptr,
         B_Loc,
         sm_scale,
         k_scale,
@@ -65,6 +66,7 @@ if triton.__version__ >= "2.1.0":
         BLOCK_N: tl.constexpr,
         SLIDING_WINDOW: tl.constexpr,
         SKIP_DECODE: tl.constexpr,
+        USE_SINKS: tl.constexpr,
     ):
 
         cur_batch = tl.program_id(0)
@@ -109,8 +111,23 @@ if triton.__version__ >= "2.1.0":
                     other=0.0)  # [M,D]
 
         # initialize pointer to m and l
-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")  # [M]
-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # [M]
+        # m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")  # [M]
+        # l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # [M]
+        
+        ########################################
+        # initialize pointer to m and l
+        if not USE_SINKS:
+            m_i = tl.full([BLOCK_M], float("-inf"), dtype=tl.float32)
+        else:
+            m_i = tl.load(
+                sink_ptr + tl.full([BLOCK_M], cur_head, dtype=tl.int64),
+                mask=(offs_m < cur_batch_query_len),
+                other=float("-inf"),
+            ).to(dtype=tl.float32)
+
+        l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)
+        ########################################
+        
         acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED],
                        dtype=tl.float32)  # [M,D]
 
@@ -731,7 +748,8 @@ if triton.__version__ >= "2.1.0":
                               alibi_slopes=None,
                               sliding_window=None,
                               sm_scale=None,
-                              skip_decode=False):
+                              skip_decode=False,
+                            sinks=None):
 
         q_dtype_is_f32 = q.dtype is torch.float32
         # need to reduce num. blocks when using fp32
@@ -785,6 +803,7 @@ if triton.__version__ >= "2.1.0":
             sliding_window = 0
 
         if alibi_slopes is not None:
+            assert sinks is None, "Sinks arg is not supported with alibi"
             _fwd_kernel_alibi[grid](
                 q,
                 k,
@@ -845,6 +864,7 @@ if triton.__version__ >= "2.1.0":
             v,
             k_cache,
             v_cache,
+            sinks,
             b_loc,
             sm_scale,
             k_scale,
@@ -889,5 +909,6 @@ if triton.__version__ >= "2.1.0":
             SKIP_DECODE=skip_decode,
             num_warps=NUM_WARPS,
             num_stages=1,
+            USE_SINKS=sinks is not None,
         )
         return
diff --git a/benchmarks/multi_turn/generate_multi_turn.json b/benchmarks/multi_turn/generate_multi_turn.json
new file mode 100755
index 0000000..274d03c
--- /dev/null
+++ b/benchmarks/multi_turn/generate_multi_turn.json
@@ -0,0 +1,35 @@
+{
+    "filetype": "generate_conversations",
+    "num_conversations": 24,
+    "text_files": ["pg1184.txt"],
+    "print_stats": false,
+    "prompt_input": {
+        "num_turns": {
+            "distribution": "uniform",
+            "min": 12,
+            "max": 18
+        },
+        "common_prefix_num_tokens": {
+            "distribution": "constant",
+            "value": 500
+        },
+        "prefix_num_tokens": {
+            "distribution": "lognormal",
+            "mean": 6,
+            "sigma": 4,
+            "max": 1500
+        },
+        "num_tokens": {
+            "distribution": "uniform",
+            "min": 120,
+            "max": 160
+        }
+    },
+    "prompt_output": {
+        "num_tokens": {
+            "distribution": "uniform",
+            "min": 80,
+            "max": 120
+        }
+    }
+}
\ No newline at end of file
diff --git a/benchmarks/structured_schemas/structured_schema_1.json b/benchmarks/structured_schemas/structured_schema_1.json
new file mode 100755
index 0000000..13bd6b6
--- /dev/null
+++ b/benchmarks/structured_schemas/structured_schema_1.json
@@ -0,0 +1,19 @@
+{
+    "type": "object",
+    "properties": {
+      "name": { "type": "string" },
+      "email": { "type": "string" },
+      "street": { "type": "string" },
+      "city": { "type": "string" },
+      "state": { "type": "string" },
+      "zip": { "type": "string" },
+      "phone": { "type": "string" },
+      "website": { "type": "string" },
+      "company": { "type": "string" },
+      "age": { "type": "integer" }
+    },
+    "required": [
+      "name",
+      "email"
+    ]
+}
diff --git a/config.py b/config.py
old mode 100644
new mode 100755
index a2e547c..898fc4a
--- a/config.py
+++ b/config.py
@@ -537,7 +537,6 @@ class ModelConfig:
             ("RewardModel", "reward"),
         ]
         _, arch = self.registry.inspect_model_cls(architectures)
-
         for suffix, pref_task in suffix_to_preferred_task:
             if arch.endswith(suffix) and pref_task in supported_tasks:
                 return pref_task
@@ -581,7 +580,7 @@ class ModelConfig:
                     architectures, supported_tasks)
                 if preferred_task is not None:
                     selected_task = preferred_task
-
+                    
                 logger.info(
                     "This model supports multiple tasks: %s. "
                     "Defaulting to '%s'.", supported_tasks, selected_task)
@@ -2542,7 +2541,7 @@ def _get_and_verify_dtype(
     # NOTE: getattr(config, "torch_dtype", torch.float32) is not correct
     # because config.torch_dtype can be None.
     config_dtype = getattr(config, "torch_dtype", None)
-
+    
     # Fallbacks for multi-modal models if the root config
     # does not define torch_dtype
     if config_dtype is None and hasattr(config, "text_config"):
diff --git a/debug.py b/debug.py
new file mode 100755
index 0000000..ea14538
--- /dev/null
+++ b/debug.py
@@ -0,0 +1,68 @@
+import torch
+import re
+
+FILE_LOG = "/home/tester/phucnguyen/hygon-test/dev/vllm_plugin_vllmm_0.8.2/logs/log.log" 
+import os
+if os.path.exists(FILE_LOG):
+    os.remove(FILE_LOG)
+    print(f"Deleted old log file: {FILE_LOG}")
+
+def print_debug_dash(file_path, dash: int = 100):
+    with open(file_path, "a", encoding="utf-8") as f:
+        if dash > 0:
+            f.write("-" * dash + "\n")
+
+def print_debug_text(file_path, text: str):
+    with open(file_path, "a", encoding="utf-8") as f:
+        if text != "":
+            f.write(text + "\n")
+
+COUNT = 0
+def print_debug(a: torch.Tensor, file_path: str, name: str, print_shape=False, dash=0, layer=-1, force_print_weight:bool=False,force_more_print:int=-1):
+    # return
+    global COUNT
+        
+    P = 256
+    if force_more_print == -1:
+        MAX_SLICE = 16
+    else:
+        MAX_SLICE = force_more_print
+        
+    def print_dash(f, dash=0):
+        if dash > 0:
+            f.write("-" * dash + "\n")
+
+    if a.shape[0] in [1, 10] or force_print_weight:
+        if layer==0: COUNT += 1
+        if COUNT>=3: return
+
+
+        if a.ndim > 3:
+            raise ValueError(f"Tensor '{name}' has {a.ndim} dims. Max 3 supported.")
+
+        a_cpu = a.detach().cpu()
+        a_print = a_cpu
+
+        if any(dim > P for dim in a_cpu.shape):
+            slices = tuple(slice(0, MAX_SLICE) if dim > P else slice(None) for dim in a_cpu.shape)
+            a_print = a_cpu[slices]
+
+        try:
+            torch.set_printoptions(threshold=float("inf"), linewidth=20000, sci_mode=False, precision=4)
+            tensor_str = str(a_print)
+            tensor_str = re.sub(r"^tensor\(", "", tensor_str)
+            tensor_str = re.sub(r",\s*(dtype|device)=.*?\)$", "", tensor_str, flags=re.DOTALL)
+            tensor_str = re.sub(r"\)$", "", tensor_str)
+        except Exception as e:
+            raise RuntimeError("Tensor print failed") from e
+
+        with open(file_path, "a", encoding="utf-8") as f:
+            f.write(
+                f"name={name!r:<25} | "
+                f"dtype={str(a.dtype):<15} | "
+                f"shape={str(tuple(a.shape)):<20} | "
+                f"{('val = ' + str(a_cpu.item()) if a_cpu.numel() == 1 else '')}\n"
+            )
+            if not print_shape:
+                f.write(f"{tensor_str}\n")
+            print_dash(f, dash)
\ No newline at end of file
diff --git a/external/matmul.py b/external/matmul.py
new file mode 100755
index 0000000..1eeeea5
--- /dev/null
+++ b/external/matmul.py
@@ -0,0 +1,94 @@
+import torch
+import triton
+import triton.language as tl
+
+@triton.jit
+def matmul_kernel(
+    a_ptr, b_ptr, c_ptr,
+    M, N, K,
+    stride_am, stride_ak,
+    stride_bn, stride_bk,
+    stride_cm, stride_cn,
+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
+):
+    # Lấy PID (Program ID) của các khối
+    pid = tl.program_id(axis=0)
+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
+    pid_m = pid // num_pid_n
+    pid_n = pid % num_pid_n
+
+    # Tạo con trỏ cho khối A và B
+    # A shape [M, K], B shape [N, K] (Do B chưa transpose trong bộ nhớ)
+    
+    # Offsets cho A: hàng pid_m, chạy dọc K
+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
+    offs_k = tl.arange(0, BLOCK_SIZE_K)
+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
+
+    # Offsets cho B: hàng pid_n, chạy dọc K (vì ta cần B^T nên ta đọc B theo hàng của nó tương ứng cột của kết quả)
+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
+    b_ptrs = b_ptr + (offs_bn[None, :] * stride_bn + offs_k[:, None] * stride_bk)
+
+    # Biến tích lũy
+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
+
+    # Loop qua chiều K
+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
+        
+        # Thực hiện phép nhân: a [BLOCK_M, BLOCK_K] x b [BLOCK_K, BLOCK_N]
+        # Lưu ý: b ở đây đã được load sao cho chiều K là chiều dọc (nhờ cách set offsets ở trên)
+        accumulator += tl.dot(a, b)
+        
+        # Advance pointers
+        a_ptrs += BLOCK_SIZE_K * stride_ak
+        b_ptrs += BLOCK_SIZE_K * stride_bk
+
+    # Chuyển về đúng dtype của C (ví dụ float16/bfloat16)
+    c = accumulator.to(tl.float16) # Hoặc bfloat16 tùy input
+
+    # Ghi kết quả C
+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
+    tl.store(c_ptrs, c, mask=c_mask)
+
+def triton_matmul(A, B, bias=None):
+    # A: MxK, B: NxK -> C: MxN
+    M, K = A.shape
+    N, _ = B.shape
+    C = torch.empty((M, N), device=A.device, dtype=A.dtype)
+    
+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)
+    
+    matmul_kernel[grid](
+        A, B, C,
+        M, N, K,
+        A.stride(0), A.stride(1),
+        B.stride(0), B.stride(1),
+        C.stride(0), C.stride(1),
+        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=32 # Tuning tùy GPU
+    )
+    return C
+
+def naive_matmul_bias_3_loops(A, B, bias=None):
+    M, K = A.shape
+    N, _ = B.shape
+    
+    C = torch.zeros((M, N), dtype=A.dtype, device=A.device)
+    
+    for i in range(M):
+        for j in range(N):
+            total = 0.0
+            for k in range(K):
+                total += A[i, k] * B[j, k]
+            
+            if bias is not None:
+                total += bias[j]
+                
+            C[i, j] = total
+            
+    return C
\ No newline at end of file
diff --git a/model_executor/custom_op.py b/model_executor/custom_op.py
old mode 100644
new mode 100755
index dfd052f..b403ce9
--- a/model_executor/custom_op.py
+++ b/model_executor/custom_op.py
@@ -79,10 +79,10 @@ class CustomOp(nn.Module):
         else:
             compilation_config.disabled_custom_ops.update(
                 [self.__class__.name])
-
+            
         if not enabled:
             return self.forward_native
-
+        
         if current_platform.is_rocm():
             return self.forward_hip
         elif current_platform.is_cpu():
diff --git a/model_executor/layers/layernorm.py b/model_executor/layers/layernorm.py
old mode 100644
new mode 100755
index 6e3d480..213226a
--- a/model_executor/layers/layernorm.py
+++ b/model_executor/layers/layernorm.py
@@ -9,7 +9,6 @@ import vllm.envs as envs
 from vllm.model_executor.custom_op import CustomOp
 from vllm.platforms import current_platform
 
-
 def is_rocm_aiter_rmsnorm_enabled() -> bool:
     return current_platform.is_rocm() \
         and envs.VLLM_ROCM_USE_AITER_RMSNORM \
@@ -41,6 +40,8 @@ def fused_add_rms_norm(
         x: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor,
         variance_epsilon: float) -> Tuple[torch.Tensor, torch.Tensor]:
     from vllm import _custom_ops as ops
+    
+    envs.VLLM_USE_OPT_OP=False
     if envs.VLLM_USE_OPT_OP:
         ops.fused_add_rms_norm_opt(
             x,
@@ -115,6 +116,7 @@ class RMSNorm(CustomOp):
         self.variance_epsilon = eps
         self.variance_size_override = (None if var_hidden_size == hidden_size
                                        else var_hidden_size)
+            
         self.has_weight = has_weight
 
         self.weight = torch.ones(hidden_size)
@@ -164,6 +166,8 @@ class RMSNorm(CustomOp):
         x: torch.Tensor,
         residual: Optional[torch.Tensor] = None,
     ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+        return self.forward_native(x, residual)
+    
         if self.variance_size_override is not None:
             return self.forward_native(x, residual)
 
@@ -171,8 +175,9 @@ class RMSNorm(CustomOp):
         norm_func = dispatch_cuda_rmsnorm_func(add_residual)
 
         if add_residual:
-            return norm_func(x, residual, self.weight.data,
+            out = norm_func(x, residual, self.weight.data,
                              self.variance_epsilon)
+            return out
         else:
             return norm_func(x, self.weight.data, self.variance_epsilon)
 
diff --git a/model_executor/layers/logits_processor.py b/model_executor/layers/logits_processor.py
old mode 100644
new mode 100755
index 4a35972..0a248f0
--- a/model_executor/layers/logits_processor.py
+++ b/model_executor/layers/logits_processor.py
@@ -20,7 +20,6 @@ if envs.VLLM_LOGITS_PROCESSOR_THREADS is not None:
     _logits_processor_threadpool = ThreadPoolExecutor(
         envs.VLLM_LOGITS_PROCESSOR_THREADS)
 
-
 class LogitsProcessor(nn.Module):
     """Process logits and apply logits processors from sampling metadata.
 
diff --git a/model_executor/layers/vocab_parallel_embedding.py b/model_executor/layers/vocab_parallel_embedding.py
old mode 100644
new mode 100755
index d102bcf..3f28862
--- a/model_executor/layers/vocab_parallel_embedding.py
+++ b/model_executor/layers/vocab_parallel_embedding.py
@@ -18,6 +18,7 @@ from vllm.platforms import current_platform
 
 DEFAULT_VOCAB_PADDING_SIZE = 64
 
+from vllm.external.matmul import triton_matmul, naive_matmul_bias_3_loops
 
 class UnquantizedEmbeddingMethod(QuantizeMethodBase):
     """Unquantized method for embeddings."""
@@ -51,7 +52,10 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
             else:
                 return torch.matmul(x, layer.weight)
         else:
-            return F.linear(x, layer.weight, bias)
+            # out = F.linear(x, layer.weight, bias)
+            # out = naive_matmul_bias_3_loops(x, layer.weight, bias)
+            out = triton_matmul(x, layer.weight, bias)
+            return out
 
     def embedding(self, layer: torch.nn.Module,
                   input_: torch.Tensor) -> torch.Tensor:
@@ -82,7 +86,6 @@ def vocab_range_from_global_vocab_size(global_vocab_size: int,
                                                      rank,
                                                      offset=offset)
 
-
 @dataclass
 class VocabParallelEmbeddingShardIndices:
     """Indices for a shard of a vocab parallel embedding."""
diff --git a/model_executor/models/llama.py b/model_executor/models/llama.py
old mode 100644
new mode 100755
index 3b42fdd..a9fb96f
--- a/model_executor/models/llama.py
+++ b/model_executor/models/llama.py
@@ -71,7 +71,6 @@ def get_compressed_tensors_cache_scale(name: str) -> Optional[str]:
     # If no matches, return None
     return None
 
-
 class LlamaMLP(nn.Module):
 
     def __init__(
@@ -544,6 +543,7 @@ class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
         intermediate_tensors: Optional[IntermediateTensors] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
     ) -> Union[torch.Tensor, IntermediateTensors]:
+
         model_output = self.model(input_ids, positions, intermediate_tensors,
                                   inputs_embeds)
         return model_output
diff --git a/model_executor/models/registry.py b/model_executor/models/registry.py
old mode 100644
new mode 100755
index 7664c82..df469b7
--- a/model_executor/models/registry.py
+++ b/model_executor/models/registry.py
@@ -410,7 +410,7 @@ class _ModelRegistry:
     def _try_inspect_model_cls(self, model_arch: str) -> Optional[_ModelInfo]:
         if model_arch not in self.models:
             return None
-
+        
         return _try_inspect_model_cls(model_arch, self.models[model_arch])
 
     def _normalize_archs(
@@ -436,7 +436,6 @@ class _ModelRegistry:
         architectures: Union[str, List[str]],
     ) -> Tuple[_ModelInfo, str]:
         architectures = self._normalize_archs(architectures)
-
         for arch in architectures:
             model_info = self._try_inspect_model_cls(arch)
             if model_info is not None:
diff --git a/v1/attention/backends/triton_attn.py b/v1/attention/backends/triton_attn.py
old mode 100644
new mode 100755
index f11f2b6..ccc21ad
--- a/v1/attention/backends/triton_attn.py
+++ b/v1/attention/backends/triton_attn.py
@@ -70,6 +70,7 @@ class TritonAttentionImpl(AttentionImpl):
         blocksparse_params: Optional[dict[str, Any]] = None,
         logits_soft_cap: Optional[float] = None,
         attn_type: AttentionType = AttentionType.DECODER,
+        sinks: Optional[torch.Tensor] = None,
     ) -> None:
         if blocksparse_params is not None:
             raise ValueError(
@@ -101,6 +102,13 @@ class TritonAttentionImpl(AttentionImpl):
                                       "encoder/decoder cross-attention "
                                       "are not implemented for "
                                       "TritonAttentionImpl")
+            
+        self.sinks = sinks
+        if sinks is not None:
+            assert sinks.shape[0] == num_heads, (
+                "Sinks must have the same number of heads as the number of "
+                f"heads in the layer. Sinks shape: {sinks.shape}, "
+                f"num_heads: {num_heads}.")
 
     def forward(
         self,
@@ -173,6 +181,8 @@ class TritonAttentionImpl(AttentionImpl):
             v_scale=layer._v_scale,
             alibi_slopes=self.alibi_slopes,
             sliding_window=self.sliding_window[0],
-            sm_scale=self.scale)
+            sm_scale=self.scale,
+            sinks=self.sinks,
+        )
 
         return output
diff --git a/v1/worker/gpu_model_runner.py b/v1/worker/gpu_model_runner.py
old mode 100644
new mode 100755
index a85009f..d301d1a
--- a/v1/worker/gpu_model_runner.py
+++ b/v1/worker/gpu_model_runner.py
@@ -1058,6 +1058,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 logits=logits,
                 sampling_metadata=sampling_metadata,
             )
+                
+
         else:
             # When indexing with a tensor (bonus_logits_indices), PyTorch
             # creates a new tensor with separate storage from the original
@@ -1082,7 +1084,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 sampling_metadata,
             )
             sampler_output.sampled_token_ids = output_token_ids
-
         # TODO(woosuk): The following loop can be slow since it iterates over
         # the requests one by one. Optimize.
         for i, generator in self.input_batch.generators.items():
@@ -1593,12 +1594,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         block_size = self.vllm_config.cache_config.block_size
         use_mla = self.vllm_config.model_config.use_mla
         kv_cache_spec: dict[str, KVCacheSpec] = {}
+        
         for layer_name, attn_module in forward_ctx.items():
             if isinstance(attn_module, FusedMoE):
                 continue
+            if "FusedMoE" in attn_module.__class__.__name__:
+                continue
 
             # TODO: Support other attention modules, e.g., sliding window,
             # cross-attention
+            
             assert isinstance(attn_module, Attention)
             if attn_module.attn_type == AttentionType.DECODER:
                 kv_cache_spec[layer_name] = FullAttentionSpec(
